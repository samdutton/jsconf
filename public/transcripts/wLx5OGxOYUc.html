<section>
<p><span data-start="1.64" data-end="20.11">Thank you.</span> <span data-start="20.11" data-end="23.8">So hello everyone, my name is Raul.</span> <span data-start="23.8" data-end="28.21">I'm excited to talk to you today.</span> <span data-start="28.21" data-end="34.55">It's my first JSConf 2014 and it is really blowing my mind.</span> </p>
<p><span data-start="34.55" data-end="40.37">So, before getting to topic, let me introduce myself, I come from Spain, where I work as</span> <span data-start="40.37" data-end="48.13">a freelance developer, and I'm also studying masters degree in computer technologies focus</span> <span data-start="48.13" data-end="51.139">on data compression, crip crip and steganography.</span> <span data-start="51.139" data-end="53.269">\h‑‑ crypt technology autography.</span> <span data-start="53.269" data-end="61.53">Let's start with data compression, first of all, I'm not an expert on data compression,</span> <span data-start="61.53" data-end="67.93">I am learning about it, and I will like to share with you what I know really.</span> <span data-start="67.93" data-end="73.39">So, for me, data compression is an amazing topic.</span> <span data-start="73.39" data-end="78.35">When I say amazing, it is really amazing, I promise.</span> <span data-start="78.35" data-end="84.74">It can be seen like magic, okay, data compression algorithm take some data and reduce it's size</span> <span data-start="84.74" data-end="86.76">without losing investigation.</span> </p>
<p><span data-start="86.76" data-end="94.159">In some cases like logic compression it may be interesting to lose some information that</span> <span data-start="94.159" data-end="99.5">means once the compressed the result will not be identical.</span> <span data-start="99.5" data-end="107.689">For example in images, sounds, movies, they are more suitable to of use logic compression.</span> <span data-start="107.689" data-end="116.48">On the other hand text is usually compressed using a lossless method like him HTML file\h‑‑</span> <span data-start="116.48" data-end="124.92">it's not magic, we have tools to determine how far we can go in come pressing data without</span> <span data-start="124.92" data-end="132.17">loosing information or quantify much information is in a given message.</span> <span data-start="132.17" data-end="138.2">And this is with information theory introduce by Claud etiane nonin 1948 in mathematical</span> <span data-start="138.2" data-end="143.33">theory of communication\h‑‑ his famous paper.</span> <span data-start="143.33" data-end="151.41">So, how can we determine how much information is contained in a message? Okay.</span> <span data-start="151.41" data-end="156.19">That answer is Entropy.</span> <span data-start="156.19" data-end="162.01">The Entropy, you can forget the formula for now, the Entropy is the</span> <span data-start="162.01" data-end="167.09">A A amount of information contained in each message.</span> <span data-start="167.09" data-end="172.47">In binary for example, we can concern Entropy the number of bits to represent the message</span> <span data-start="172.47" data-end="177.06">without losing information.</span> </p>
<p><span data-start="177.06" data-end="183.58">The concept of Entropy is quite abstract, and it's not always easy to understand.</span> <span data-start="183.58" data-end="191.16">But, how a message can contain more information than other? Okay, it doesn't make any sense.</span> <span data-start="191.16" data-end="197.69">Imagine for example these two places, Berlin and the Desert of Arizona.</span> <span data-start="197.69" data-end="204.58">We would say it is raining in Berlin it contains less information than if we say it is raining</span> <span data-start="204.58" data-end="206.58">in the Desert of Arizona.</span> <span data-start="206.58" data-end="210.409">That is because it is less common.</span> <span data-start="210.409" data-end="218.129">In Berlin they have 225 days\h‑‑ rainy days per year, 62\hpercent.</span> </p>
<p><span data-start="218.129" data-end="223.769">While in the Desert of Arizona only 17 days, which is six percent.</span> <span data-start="223.769" data-end="232.629">So information theory Entropy is closely related to probabilities, as you can see in the formula.</span> <span data-start="232.629" data-end="241.269">And while this is mostly theoretical our brains are already designed to come press data, we</span> <span data-start="241.269" data-end="244.05">do it all the time.</span> </p>
<p><span data-start="244.05" data-end="249.97">For example, we use multiconstruct to express complex information like feelings with just</span> <span data-start="249.97" data-end="255.64">a couple of characters we can communicate that we're happy, we're sad, we are mocking</span> <span data-start="255.64" data-end="256.64">you.</span> <span data-start="256.64" data-end="262.77">We also have acronyms for common sentences, we no longer say laugh out loud we just say</span> </p>
<p><span data-start="262.77" data-end="266.56">LOL or ASAP for as soon as possible.</span> <span data-start="266.56" data-end="270.95">Savoring time while speaking and saving space when storing it.</span> <span data-start="270.95" data-end="277.94">Even in real life we can share so much information by facial eggs presentations, okay, like the</span> <span data-start="277.94" data-end="284.27">face is telling us if he's safe, happy and having a good time.</span> <span data-start="284.27" data-end="292.2">Of these two examples we could consider a\h‑‑ facial eggs presentation as a form of logic</span> <span data-start="292.2" data-end="293.97">compression.</span> <span data-start="293.97" data-end="296.67">Information is being missed here.</span> <span data-start="296.67" data-end="304.55">In the other hand acronyms, if they are unique and well defined, okay, we can decode it uniquely</span> <span data-start="304.55" data-end="308">would be a form of lossless compression.</span> <span data-start="308" data-end="313.94">But, even before the information theory was developed, data compression was already a</span> <span data-start="313.94" data-end="317.97">key factor for assigning codes to communicate.</span> <span data-start="317.97" data-end="324.88">Take the Morse code for example, to save time and Bandwidth Mors code assigned shorter code</span> <span data-start="324.88" data-end="326.89">to more frequent character ferreters.</span> <span data-start="326.89" data-end="331.65">For example in the English language, the Morse code character\h‑‑ the most used character</span> <span data-start="331.65" data-end="340.44">is the letter E, so in Morse it is the shortest code, it is just a dot.</span> </p>
<p><span data-start="340.44" data-end="351.16">So, what about data compression in web sites? Okay, this is the normal flow, okay, the browser</span> <span data-start="351.16" data-end="359.53">sends AEC request to a server to get the file for example index.HTML letting the server</span> <span data-start="359.53" data-end="365.55">know that accept the zip content.</span> <span data-start="365.55" data-end="375.16">The sever then reads the file, come presses on the fly, usually, and sends it to the browser,</span> <span data-start="375.16" data-end="382.64">okay, unless we con fight your the server to\h‑‑ content, it does it on the fly.</span> <span data-start="382.64" data-end="391.8">Finally, the browser deCOM presses the file also on the fly, again and gives the reader</span> <span data-start="391.8" data-end="392.8">content.</span> <span data-start="392.8" data-end="398.77">We will see later having to compress and the compress on the fly requires to have a compressional</span> <span data-start="398.77" data-end="410.97">algorithm that ise is fast to come press and fast to decompress in both direction.</span> <span data-start="410.97" data-end="416.09">Let's see how GZIP works internally.</span> <span data-start="416.09" data-end="429.04">GZIP is a file formate, it ise it is a file format based and a compression algorithm,</span> <span data-start="429.04" data-end="435.21">the actual compression takes place in the detailed\h‑‑\h‑‑ it was designed by</span> <span data-start="435.21" data-end="444.15">this man, Phil Katz for the PK zip archiving tool and is used for HTTP, PNG and PDF files.</span> <span data-start="444.15" data-end="445.15">Okay.</span> <span data-start="445.15" data-end="456.65">Unfortunately the history of Phil Katz, he was a genius, but he died when he was only</span> <span data-start="456.65" data-end="461.95">37 due to complications related to chronic alcoholism.</span> </p>
<p><span data-start="461.95" data-end="470.33">So, deplait is a combination of two algorithms, LZ 77 and Hufpmann coding.</span> <span data-start="470.33" data-end="473.32">Let's see how they work.</span> <span data-start="473.32" data-end="481.22">LZ 77 is a losse lossless data compression, we get the original data once come pressed.</span> <span data-start="481.22" data-end="489.13">Which is a compression by replacing repeated occurrence of data with references, okay.</span> </p>
<p><span data-start="489.13" data-end="497.66">So, in this example, of we see how the algorithm tries to find repeated occurrences and for</span> <span data-start="497.66" data-end="504.56">example files that space file, space file spice, is spice appears twice.</span> <span data-start="504.56" data-end="510.24">So, replaces the second occurrence with a reference.</span> <span data-start="510.24" data-end="516.12">The first number is the distance, okay, the jump, the number of positions you need to</span> <span data-start="516.12" data-end="522.09">go back to go to the other stream.</span> <span data-start="522.09" data-end="525.39">And the second number is the length, the length of the match.</span> </p>
<p><span data-start="525.39" data-end="532">So the decoder can uniquely decode that information.</span> <span data-start="532" data-end="539.27">To find matches, the algorithm keeps track of recently read data, of 232 kilobyte, this</span> <span data-start="539.27" data-end="542.08">is called the sliding window.</span> <span data-start="542.08" data-end="547.97">We refer sometimes LZ 77 as a sliding Al gosh rhythm.</span> <span data-start="547.97" data-end="554.27">Once come pressed we have three times of data in the compressed stream.</span> <span data-start="554.27" data-end="561.32">We have literals, which would be the characters that have not been replaced by references,</span> <span data-start="561.32" data-end="568.96">we have lengths, the length of the match, and distances, the jump.</span> <span data-start="568.96" data-end="581.021">Okay, and in the second step, the algorithm tries to use\h‑‑ bits for the zero, one</span> <span data-start="581.021" data-end="586.02">approach would be to use the necessary bits to represent the symbols.</span> <span data-start="586.02" data-end="594.74">For example here we have the string hello word, we will use ASCII, it uses 8 bits per</span> <span data-start="594.74" data-end="601.74">character, we would need 88 bits to represent that information, that message.</span> </p>
<p><span data-start="601.74" data-end="610.06">As we are only using the characters that appear in the table, they can be represented only</span> <span data-start="610.06" data-end="619.71">with 3 bits, so we can make a transformation and use only 3 bits to represent the message.</span> <span data-start="619.71" data-end="625.78">So, we will of only 33 bits to represent that information.</span> <span data-start="625.78" data-end="629.37">But we can do it better.</span> <span data-start="629.37" data-end="634.98">Instead of fix length codes we can use variable length codes.</span> </p>
<p><span data-start="634.98" data-end="642.64">So, if we\h‑‑ if we calculate the frequency, the number of appearances of the characters,</span> <span data-start="642.64" data-end="649.5">we can try to think shorter goals to more frequent symbols like the Morse code does.</span> <span data-start="649.5" data-end="654.63">So, in this example L is used three times.</span> <span data-start="654.63" data-end="660.29">The letter O is just twice, and the others only once.</span> <span data-start="660.29" data-end="669.43">So, we could do this and only use 19 bits instead of 33 to represent the message.</span> <span data-start="669.43" data-end="673.87">Okay, but there is a problem.</span> <span data-start="673.87" data-end="676.89">The come pressed stream is ambiguous.</span> <span data-start="676.89" data-end="687.03">These four bits could be, for example, HE, which is correct, A H which is 00 and E which</span> <span data-start="687.03" data-end="696.13">is 01, but it could also be LHO, L 0, H 00 and of O 1.</span> </p>
<p><span data-start="696.13" data-end="704.43">Also the O and many others, so we will need to\h‑‑ our next thing is to separate that</span> <span data-start="704.43" data-end="707.78">code using compression ratio.</span> <span data-start="707.78" data-end="718.09">And here is when huffman coding comes, it generates variable length codes, shorter codes</span> <span data-start="718.09" data-end="724.8">to more frequent characters, but they have the prefixed property, the prefixed property</span> <span data-start="724.8" data-end="729.35">says that any of the codes is a prefix of another.</span> <span data-start="729.35" data-end="736.99">So, the output can be decodeble it is not ambiguous.</span> <span data-start="736.99" data-end="745.4">So using huffman coding it can be represented with 32 bits, one bit less than you see in</span> <span data-start="745.4" data-end="747.48">the previous code.</span> </p>
<p><span data-start="747.48" data-end="751.34">And for larger messages gains could be even better.</span> <span data-start="751.34" data-end="764.46">So, Huffman code is used using the compressed output we got from LC 77.</span> <span data-start="764.46" data-end="770.32">One for literals and length and the second for distances.</span> <span data-start="770.32" data-end="780.61">So if we go out now to the GZIP file, not the algorithm, the format file, it come presses</span> <span data-start="780.61" data-end="785.29">an input data in blocks, okay, each block is compressed separately.</span> <span data-start="785.29" data-end="789.58">And there are three modes of compression.</span> <span data-start="789.58" data-end="794.06">In mode one there is no compression at all.</span> <span data-start="794.06" data-end="799.62">It is just useful for data that is already come pressed or data that is totally random</span> <span data-start="799.62" data-end="802.98">that cannot be come pressed.</span> <span data-start="802.98" data-end="812.61">Mode two uses some already generated Huffman codes using some statistical analysis which</span> <span data-start="812.61" data-end="821.13">fits good enough for most data, and mode three generate those code tables taking into account</span> <span data-start="821.13" data-end="824.75">the data of the block.</span> <span data-start="824.75" data-end="826.51">The mode one is the fastest.</span> <span data-start="826.51" data-end="833.11">And mode three is the slowest, but usually gets better compression ratios.</span> <span data-start="833.11" data-end="840.8">Of of security, you're familiar with\h‑‑ you probably have use in the the past to use</span> <span data-start="840.8" data-end="848.69">to split big files into several chunks the size of floppy disks, the non‑compression</span> <span data-start="848.69" data-end="851.45">mode to make these chunks.</span> </p>
<p><span data-start="851.45" data-end="860.339">Okay, so, we just saw how this works you may have noticed this there are different ways</span> <span data-start="860.339" data-end="867.29">to generate the GZIP compliant file, just splitting the file into several blocks with</span> <span data-start="867.29" data-end="873.4">no compression, the no compression mode we get GZIP compliant file, it is really fast</span> <span data-start="873.4" data-end="876.48">but there is no compression at all.</span> <span data-start="876.48" data-end="886.29">And there are a lot of\h‑‑ able to create the zip files, each of them use their own</span> <span data-start="886.29" data-end="889.75">optimizations, so the result will be different.</span> <span data-start="889.75" data-end="897.93">Also, there are compression modes, for example the high compression mode spends more time</span> <span data-start="897.93" data-end="904.04">in LC 77 trying to find a longer match, okay.</span> <span data-start="904.04" data-end="914.13">In general 7 zip gets better compression ratio than GNU GZIP, for example Zopfli follows</span> <span data-start="914.13" data-end="922.589">a different approach\h‑‑ this is from Google, what it does is spend much more time,</span> <span data-start="922.589" data-end="929.161">when I say much more time, it is much more time, around a hundred times slower to get</span> <span data-start="929.161" data-end="931.12">better compression ratios.</span> <span data-start="931.12" data-end="933.83">Okay, around five percent.</span> <span data-start="933.83" data-end="941.47">This is really useful if we want to come press, for example our JavaScript files or CSS files</span> <span data-start="941.47" data-end="945.16">before deploying, okay, in the deployment process.</span> <span data-start="945.16" data-end="952.13">And then serve those the zip files instead of come pressing on the fly.</span> </p>
<p><span data-start="952.13" data-end="959.93">As a general rule, more time, using more time, you get better compression ratios, so you</span> <span data-start="959.93" data-end="966.56">need the find a trade off between doing it off line and up load the come pressed files</span> <span data-start="966.56" data-end="972.68">or doing it on the fly.</span> <span data-start="972.68" data-end="979.19">Why GZIP, it is the best compression method? The answer is no.</span> <span data-start="979.19" data-end="986.77">There are many compression algorithms with better compression ratios than GZIP, so why</span> <span data-start="986.77" data-end="995.79">are we still using GZIP? Most of the time in computer science, there are trade offs.</span> <span data-start="995.79" data-end="1007.34">GZIP provides good enough compression ratio between 2.5 and 3 for text and it is fast,</span> <span data-start="1007.34" data-end="1012.79">it is fast to come press data and it is fast to deCOM press it.</span> </p>
<p><span data-start="1012.79" data-end="1019.17">As we usually can figure out come press responses on the fly, this is something to take into</span> <span data-start="1019.17" data-end="1022.51">account.</span> <span data-start="1022.51" data-end="1028.079">In addition, even in the worse case, for example if the data is already compressed, expands</span> <span data-start="1028.079" data-end="1036.91">the data just a little bit by only five bites by 32‑kilo bytes.</span> <span data-start="1036.91" data-end="1042.64">We try to compress everything it is important to be sure that damages will be relatively</span> <span data-start="1042.64" data-end="1043.8">low.</span> <span data-start="1043.8" data-end="1051.65">Also, the memory needed by\h‑‑ the decoder is independent of the size of the data.</span> <span data-start="1051.65" data-end="1060.36">And finally there are free implementation that avoids\h‑‑ it is also difficult to</span> <span data-start="1060.36" data-end="1064.179">newer compression methods to be widely used.</span> <span data-start="1064.179" data-end="1074.67">For example the Chromium team tried a few years ago for G.Zip 2 it provides better compression</span> <span data-start="1074.67" data-end="1078.38">ratio and the results are fast.</span> <span data-start="1078.38" data-end="1088.64">The problem was that even though the ACTP were correct, and the contents were compressed</span> <span data-start="1088.64" data-end="1093.551">be B zip 2 the approximateee some of the intermediary proxies didn't understand it.</span> <span data-start="1093.551" data-end="1101.98">\h‑‑ so it was corrupting if data.</span> </p>
<p><span data-start="1101.98" data-end="1112.38">So, basically, in the short term, at least, we are stuck in the GZIP.</span> <span data-start="1112.38" data-end="1120.21">What can we do to try to get better compression ratios? So, we can preprocess the data to</span> <span data-start="1120.21" data-end="1121.77">try to optimize matches.</span> <span data-start="1121.77" data-end="1125.82">Okay, take a look at this image.</span> <span data-start="1125.82" data-end="1133.35">This image was generated by a tool of\h‑‑ here mall showing what parts are compressed</span> <span data-start="1133.35" data-end="1141.31">file have been compressed better, those are in blue, and what parts have been expanded.</span> <span data-start="1141.31" data-end="1150.01">So, we can see the strings of characters in upper case or sometimes the first character</span> <span data-start="1150.01" data-end="1154">of capitalized words expand.</span> <span data-start="1154" data-end="1161.08">Of this is spectral behavior as they are less common and more difficult to find a match</span> <span data-start="1161.08" data-end="1163.39">to replace with a reference.</span> <span data-start="1163.39" data-end="1174.13">But preprocess this data to try to optimize the compression and the answer is, yes, sometimes.</span> <span data-start="1174.13" data-end="1182.12">The goal would be to have a functionality able to transform the data in a way that once</span> <span data-start="1182.12" data-end="1188.59">is the zip is smaller than the original data is zipped without losing information.</span> </p>
<p><span data-start="1188.59" data-end="1194.05">And, this depends highly on the type of data we are using.</span> <span data-start="1194.05" data-end="1203.27">For example there is a technique called transposing JSON, as you know JSON object is made of keycal</span> <span data-start="1203.27" data-end="1209.78">you pairs where the key portion is repeated for each instance.</span> <span data-start="1209.78" data-end="1216.81">The basic idea is to group together all the values for each property, so, in the example,</span> <span data-start="1216.81" data-end="1221.12">the name property will contain all the names and the country property will contain all</span> <span data-start="1221.12" data-end="1222.78">the countries.</span> <span data-start="1222.78" data-end="1229.85">Obviously, we will need to change our client‑side JavaScript to be able to read this.</span> </p>
<p><span data-start="1229.85" data-end="1237.89">But, doing this, we have two main benefits, the first one is that we reduce redene dun</span> <span data-start="1237.89" data-end="1244.63">Dennis, we are making the files smaller at the expense of spending more time or CPU cycles</span> <span data-start="1244.63" data-end="1248.56">in the client to interpret that information.</span> <span data-start="1248.56" data-end="1252.71">But, we are also grouped in similar data together.</span> <span data-start="1252.71" data-end="1264.17">If you remember when we talked about LC 77 it has a sliding window of 33‑kilo bytes.</span> <span data-start="1264.17" data-end="1271.36">We make sure that similar data, in this case names or countries will be closed, so it is</span> <span data-start="1271.36" data-end="1274.91">more likely to find matches between similar data.</span> <span data-start="1274.91" data-end="1280.66">It is important to note that not always we will better compression ratio with this technique,</span> <span data-start="1280.66" data-end="1284.85">so we must be careful.</span> <span data-start="1284.85" data-end="1290.96">For XML and HTML files I have been working on an automatic process in order to order</span> <span data-start="1290.96" data-end="1297.29">the attributes in a way to maximize LC 77 matches.</span> <span data-start="1297.29" data-end="1304.64">Here executing executing LC 77, only LC 77, not the detail completely, over the first</span> <span data-start="1304.64" data-end="1308.78">sample we get an improvement of the file size of 17\hpercent.</span> <span data-start="1308.78" data-end="1317.39">As attributes get more ordered, we see the percentage is bigger, so, in general it will</span> <span data-start="1317.39" data-end="1321.799">follow up to the other attributes, we'll get slightly better results.</span> </p>
<p><span data-start="1321.799" data-end="1333.35">The process is not as simple as also takes into account the value of the attribute, but,</span> <span data-start="1333.35" data-end="1341.76">just having a kind of standard to organize your attributes you will get better results.</span> <span data-start="1341.76" data-end="1348.72">You can see there's a link at the bottom with a small paper on it.</span> <span data-start="1348.72" data-end="1357.309">So, finally, if you have interest on data compression and GZIP, okay, some recommended</span> <span data-start="1357.309" data-end="1363.79">material, of compression is a series of videos of data compression, okay.</span> <span data-start="1363.79" data-end="1373.41">They are done by Colt McAnlis, which makes it easy to understand and funny.</span> <span data-start="1373.41" data-end="1379.66">So, want to thank him for reviewing this presentation.</span> <span data-start="1379.66" data-end="1381.64">This is my favorite book on the topic.</span> </p>
<p><span data-start="1381.64" data-end="1384.89">It is data compression, the complete reference.</span> <span data-start="1384.89" data-end="1392.03">I am not sure if it's the best idea for a beginner to start with this, but if you're</span> <span data-start="1392.03" data-end="1400.76">curious to know how a given algorithm\h‑‑ the answer is here, the book is quite expensive</span> <span data-start="1400.76" data-end="1403.57">so ..</span> <span data-start="1403.57" data-end="1408.67">so if you're not going to read it, don't buy it.</span> <span data-start="1408.67" data-end="1416.88">And if you like, reading papers I recommend to read this one from Jay cob Zif and abbra</span> <span data-start="1416.88" data-end="1422.18">ham Lempel where they talk about LC 77.</span> <span data-start="1422.18" data-end="1431.93">And finally the paper written by David Hutchman on the construction of minimum redundancy</span> <span data-start="1431.93" data-end="1439.17">codes, it's a very short paper around 12 pages.</span> <span data-start="1439.17" data-end="1443.54">But the impact on data compression and other fields has been huge.</span> </p>
</section>