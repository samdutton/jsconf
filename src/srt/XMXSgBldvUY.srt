1
00:00:00,000 --> 00:00:01,530

shut up or when I signed up for this

2
00:00:01,530 --> 00:00:04,710
talk I had no idea what was going to

3
00:00:04,710 --> 00:00:08,010
happen on black friday so thankfully

4
00:00:08,010 --> 00:00:13,349
this this talk is a bit more positive

5
00:00:13,349 --> 00:00:16,289
than it could have otherwise been this

6
00:00:16,289 --> 00:00:17,220
very much could have been a disaster

7
00:00:17,220 --> 00:00:19,650
porn segment but that's not the way it

8
00:00:19,650 --> 00:00:24,480
went thankfully so this is a second year

9
00:00:24,480 --> 00:00:28,609
that we've done a note BF sort of

10
00:00:28,609 --> 00:00:33,570
twitter campaign and basically we just

11
00:00:33,570 --> 00:00:36,030
documented what what happened in our

12
00:00:36,030 --> 00:00:39,660
system as we hit the most traffic that

13
00:00:39,660 --> 00:00:43,500
we see throughout the year and this year

14
00:00:43,500 --> 00:00:46,559
was fairly unique for myself because it

15
00:00:46,559 --> 00:00:49,140
was the first year that we were doing

16
00:00:49,140 --> 00:00:51,510
some fairly intensive cpu tasks within

17
00:00:51,510 --> 00:00:55,050
our note environment last year we were

18
00:00:55,050 --> 00:00:58,079
basically just concatenate ingela script

19
00:00:58,079 --> 00:01:00,750
or sorry concatenate HTML with a little

20
00:01:00,750 --> 00:01:02,879
bit of minimal JavaScript and pushing

21
00:01:02,879 --> 00:01:05,010
out to the client that lightweight super

22
00:01:05,010 --> 00:01:09,210
simple easy this year to try to improve

23
00:01:09,210 --> 00:01:11,250
our initial page load performance as

24
00:01:11,250 --> 00:01:14,369
well as our SEO we've updated a number

25
00:01:14,369 --> 00:01:17,850
of our pages to render the content on

26
00:01:17,850 --> 00:01:19,229
the server and then push it down to the

27
00:01:19,229 --> 00:01:22,170
client so it this provides a much faster

28
00:01:22,170 --> 00:01:25,560
page load I can show you some nice side

29
00:01:25,560 --> 00:01:27,750
by side videos that really drive home

30
00:01:27,750 --> 00:01:29,909
the point but I don't really have time

31
00:01:29,909 --> 00:01:33,860
for that today or in this presentation

32
00:01:33,860 --> 00:01:36,659
the the issue that this brings up is

33
00:01:36,659 --> 00:01:41,280
that it's a massively different of load

34
00:01:41,280 --> 00:01:44,130
on the servers its CPU versus i/o bound

35
00:01:44,130 --> 00:01:48,479
and notice awesome at a thai oh wait

36
00:01:48,479 --> 00:01:52,470
sorts of tasks but cpu-bound operations

37
00:01:52,470 --> 00:01:55,460
using having only one thread available

38
00:01:55,460 --> 00:01:59,130
can quickly smile spiral out of control

39
00:01:59,130 --> 00:02:01,649
if you don't have the proper

40
00:02:01,649 --> 00:02:06,659
configuration and safety mechanisms so

41
00:02:06,659 --> 00:02:10,349
this is kind of what I had thought that

42
00:02:10,349 --> 00:02:12,360
black friday would be and it certainly

43
00:02:12,360 --> 00:02:13,630
was in the

44
00:02:13,630 --> 00:02:15,160
the couple of months prior to black

45
00:02:15,160 --> 00:02:19,300
friday but we actually effectively

46
00:02:19,300 --> 00:02:22,330
repeated last year this is a graph of

47
00:02:22,330 --> 00:02:27,490
our I believe this is the RSS loads on

48
00:02:27,490 --> 00:02:30,430
the servers and they're basically flat

49
00:02:30,430 --> 00:02:33,610
there's the servers were at least on

50
00:02:33,610 --> 00:02:35,890
this particular metric not doing a whole

51
00:02:35,890 --> 00:02:40,030
lot this is obviously a bit contrived

52
00:02:40,030 --> 00:02:41,920
because the CPU is we're doing a lot of

53
00:02:41,920 --> 00:02:43,540
things as well but those graphs aren't

54
00:02:43,540 --> 00:02:45,580
nearly as interesting and don't drive

55
00:02:45,580 --> 00:02:50,320
the point home but to do all this it

56
00:02:50,320 --> 00:02:54,070
took a lot of work to to get our system

57
00:02:54,070 --> 00:02:55,750
into this state that we needed it to be

58
00:02:55,750 --> 00:02:59,160
when we first started deploying this

59
00:02:59,160 --> 00:03:03,310
everything seemed fine under our smaller

60
00:03:03,310 --> 00:03:05,350
production loads but we quickly found

61
00:03:05,350 --> 00:03:10,960
that I got our first stress test we

62
00:03:10,960 --> 00:03:12,640
couldn't even deliver half of the

63
00:03:12,640 --> 00:03:15,280
traffic that we did last year due to

64
00:03:15,280 --> 00:03:17,320
these changes and due to implementing

65
00:03:17,320 --> 00:03:22,090
them in some pathological ways which was

66
00:03:22,090 --> 00:03:24,280
not good needed to say there were some

67
00:03:24,280 --> 00:03:26,110
concerned people in the business group

68
00:03:26,110 --> 00:03:31,960
after those results but basically we

69
00:03:31,960 --> 00:03:35,530
used those stress tests as well as some

70
00:03:35,530 --> 00:03:37,840
pretty aggressive monitoring both during

71
00:03:37,840 --> 00:03:40,230
those tests and throughout the year to

72
00:03:40,230 --> 00:03:43,030
identify issues that we may have run

73
00:03:43,030 --> 00:03:46,540
into between memory leaks excess of CPU

74
00:03:46,540 --> 00:03:48,370
and those sorts of things and we flicked

75
00:03:48,370 --> 00:03:51,040
them fix them as quickly as we could and

76
00:03:51,040 --> 00:03:54,610
push the production and moved on to the

77
00:03:54,610 --> 00:03:58,870
next issue which this was very helpful

78
00:03:58,870 --> 00:04:05,950
we survived so I there's three big

79
00:04:05,950 --> 00:04:08,709
changes that we made that helped us out

80
00:04:08,709 --> 00:04:12,760
the most and in our opinion the first of

81
00:04:12,760 --> 00:04:18,580
those was the event loop effectively we

82
00:04:18,580 --> 00:04:20,470
were taking a system that was designed

83
00:04:20,470 --> 00:04:23,979
to generate HTML on a within a browser

84
00:04:23,979 --> 00:04:26,660
on the user's device

85
00:04:26,660 --> 00:04:30,800
and trying to make it work under a note

86
00:04:30,800 --> 00:04:33,440
environment that has very different

87
00:04:33,440 --> 00:04:35,750
goals on the user's device you may want

88
00:04:35,750 --> 00:04:38,660
to show partial behavior earlier or you

89
00:04:38,660 --> 00:04:41,390
may want to delay the actual you may

90
00:04:41,390 --> 00:04:44,150
want to run on the event loop longer in

91
00:04:44,150 --> 00:04:47,900
order to show everything at the proper

92
00:04:47,900 --> 00:04:50,300
time to the user and in some cases avoid

93
00:04:50,300 --> 00:04:55,580
browser bugs but this led to situations

94
00:04:55,580 --> 00:04:57,980
where you'd have 40 milliseconds to

95
00:04:57,980 --> 00:05:01,930
render a page and that's continuous so

96
00:05:01,930 --> 00:05:04,040
rendering a single page would be 40

97
00:05:04,040 --> 00:05:05,660
milliseconds sitting on the event loop

98
00:05:05,660 --> 00:05:10,190
and under under small loads that's not

99
00:05:10,190 --> 00:05:12,400
an issue but when you start throwing

100
00:05:12,400 --> 00:05:16,850
tons of crazed shoppers at the system it

101
00:05:16,850 --> 00:05:19,970
very quickly destabilize this a system

102
00:05:19,970 --> 00:05:22,370
and when you're in a situation that you

103
00:05:22,370 --> 00:05:25,790
have event loop contention on your node

104
00:05:25,790 --> 00:05:28,220
server it you can take down the entire

105
00:05:28,220 --> 00:05:30,320
thing very easily it's not just that

106
00:05:30,320 --> 00:05:33,410
only those routes are destabilized it's

107
00:05:33,410 --> 00:05:35,870
everything so with that ties that's

108
00:05:35,870 --> 00:05:37,460
basically the behavior that we saw in

109
00:05:37,460 --> 00:05:41,660
our first stress test the the entire

110
00:05:41,660 --> 00:05:45,230
server went down not just the the pages

111
00:05:45,230 --> 00:05:48,140
that were in question here and that's

112
00:05:48,140 --> 00:05:57,860
very bad so additionally we since we had

113
00:05:57,860 --> 00:06:00,650
the client side rendering path that we

114
00:06:00,650 --> 00:06:02,120
had to use previous years and we knew

115
00:06:02,120 --> 00:06:05,540
that that worked we were able to utilize

116
00:06:05,540 --> 00:06:10,880
this to failover at the at the proper

117
00:06:10,880 --> 00:06:13,220
port failover if the system determined

118
00:06:13,220 --> 00:06:18,080
that it was likely to hit a case where

119
00:06:18,080 --> 00:06:20,060
there would be contention or just

120
00:06:20,060 --> 00:06:24,530
general stress we also designed this so

121
00:06:24,530 --> 00:06:27,020
that it would be automatic the point

122
00:06:27,020 --> 00:06:29,600
that you're starting to have or that the

123
00:06:29,600 --> 00:06:31,310
point that human could react to having

124
00:06:31,310 --> 00:06:34,070
been notified by a system that email or

125
00:06:34,070 --> 00:06:36,289
pager or whatever the case may be your

126
00:06:36,289 --> 00:06:38,570
systems already highly unstable at that

127
00:06:38,570 --> 00:06:39,249
point

128
00:06:39,249 --> 00:06:42,789
and you may even depend on exactly how

129
00:06:42,789 --> 00:06:45,069
its implemented if it's a wife config

130
00:06:45,069 --> 00:06:48,879
push you may not be able to even talk to

131
00:06:48,879 --> 00:06:53,049
the endpoints in question and restarting

132
00:06:53,049 --> 00:06:54,939
servers that are already under that sort

133
00:06:54,939 --> 00:06:57,279
of load can also lead to some very bad

134
00:06:57,279 --> 00:07:01,299
things so you absolutely want to make

135
00:07:01,299 --> 00:07:02,919
sure that if you are doing any sort of

136
00:07:02,919 --> 00:07:08,019
failover it happens automatically we

137
00:07:08,019 --> 00:07:11,459
also designed the system so that it was

138
00:07:11,459 --> 00:07:14,499
targeted at the public use case at the

139
00:07:14,499 --> 00:07:18,789
Googlebot at the basically at the point

140
00:07:18,789 --> 00:07:20,919
that we can serve only the content to

141
00:07:20,919 --> 00:07:24,429
the user that or sorry that the initial

142
00:07:24,429 --> 00:07:27,129
response is only the content that is

143
00:07:27,129 --> 00:07:30,819
long-lived and applies to everyone so we

144
00:07:30,819 --> 00:07:32,619
split out services that would either

145
00:07:32,619 --> 00:07:37,179
user specific or have data that needs to

146
00:07:37,179 --> 00:07:39,459
be shorter-lived due to whatever reason

147
00:07:39,459 --> 00:07:42,459
and then we pushed all this into a large

148
00:07:42,459 --> 00:07:44,439
number of caches throughout the system

149
00:07:44,439 --> 00:07:50,229
and found that your use case will vary

150
00:07:50,229 --> 00:07:53,019
but they're having offload in a system

151
00:07:53,019 --> 00:07:58,509
like this is quite important so we did

152
00:07:58,509 --> 00:08:01,179
all these but there was certainly a

153
00:08:01,179 --> 00:08:03,549
varied amount of impact based off of

154
00:08:03,549 --> 00:08:08,529
what the based off of each different

155
00:08:08,529 --> 00:08:12,369
action that we took so the event loop

156
00:08:12,369 --> 00:08:15,339
was very important so was the event loop

157
00:08:15,339 --> 00:08:21,099
and the event loop this change added a

158
00:08:21,099 --> 00:08:22,989
lot of complexity or to our code because

159
00:08:22,989 --> 00:08:26,079
we needed to support rendering

160
00:08:26,079 --> 00:08:27,599
synchronously on the client and

161
00:08:27,599 --> 00:08:31,119
asynchronously on the server but it had

162
00:08:31,119 --> 00:08:33,279
we not made these changes I don't think

163
00:08:33,279 --> 00:08:35,039
that we would have survived the holiday

164
00:08:35,039 --> 00:08:39,519
or we would have had to turn things off

165
00:08:39,519 --> 00:08:42,129
and toned it down gone back to the older

166
00:08:42,129 --> 00:08:44,350
system or other sorts of mitigation

167
00:08:44,350 --> 00:08:49,540
techniques but the failover and the

168
00:08:49,540 --> 00:08:52,269
caching actually didn't matter that much

169
00:08:52,269 --> 00:08:55,690
for

170
00:08:55,700 --> 00:08:58,380
of course that did not work on this

171
00:08:58,380 --> 00:09:05,430
resolution so this is a graph of all the

172
00:09:05,430 --> 00:09:09,420
server side does execution instances of

173
00:09:09,420 --> 00:09:15,510
over I believe that nature there we

174
00:09:15,510 --> 00:09:18,450
found that almost no failover occurred

175
00:09:18,450 --> 00:09:20,250
there were some errors down at the

176
00:09:20,250 --> 00:09:23,310
bottom maybe two percent at the peak I'm

177
00:09:23,310 --> 00:09:26,360
not sure you can't even see the

178
00:09:26,360 --> 00:09:29,760
instances on this graph of the of the

179
00:09:29,760 --> 00:09:32,310
failover or the load based veil of her

180
00:09:32,310 --> 00:09:36,060
cases they were there in the raw data

181
00:09:36,060 --> 00:09:41,279
but in at this scale it just it didn't

182
00:09:41,279 --> 00:09:44,279
really matter whether or not those

183
00:09:44,279 --> 00:09:48,089
triggers were false positives or whether

184
00:09:48,089 --> 00:09:50,580
or not they had they not happened the

185
00:09:50,580 --> 00:09:52,260
system would have then become unstable

186
00:09:52,260 --> 00:09:55,080
is unclear but at the volume that they

187
00:09:55,080 --> 00:09:57,480
did occur it's it seems like that did

188
00:09:57,480 --> 00:10:00,720
not help us as much dairy under our

189
00:10:00,720 --> 00:10:04,260
particular load there which kind of ties

190
00:10:04,260 --> 00:10:10,200
me in sorry about that this kind of ties

191
00:10:10,200 --> 00:10:13,470
me into the point that everyone should

192
00:10:13,470 --> 00:10:15,839
be aware on this so there's some here be

193
00:10:15,839 --> 00:10:19,589
dragons to just assuming that the things

194
00:10:19,589 --> 00:10:22,079
that we did for our use case will work

195
00:10:22,079 --> 00:10:24,810
for your system if you're consulting

196
00:10:24,810 --> 00:10:29,700
similar things but it's certainly

197
00:10:29,700 --> 00:10:31,470
helpful to know that this is the case

198
00:10:31,470 --> 00:10:33,540
but you absolutely have to do your own

199
00:10:33,540 --> 00:10:36,690
testing to make sure that whatever your

200
00:10:36,690 --> 00:10:38,820
cat your traffic patterns whatever your

201
00:10:38,820 --> 00:10:41,670
Black Friday is your super bowl you can

202
00:10:41,670 --> 00:10:45,959
survive those sorts of situations so

203
00:10:45,959 --> 00:10:49,920
i'll post these slides later some links

204
00:10:49,920 --> 00:10:52,680
to the the system that we used as well

205
00:10:52,680 --> 00:10:55,410
as the write up that i did on the start

206
00:10:55,410 --> 00:10:57,540
of our node BF event but thank you very

207
00:10:57,540 --> 00:10:59,390
much

