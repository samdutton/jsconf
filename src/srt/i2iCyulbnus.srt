1
00:00:00,380 --> 00:00:02,700

all right it's the last talk of the day

2
00:00:02,700 --> 00:00:04,529
you guys made it and you know the first

3
00:00:04,529 --> 00:00:06,420
thing I want to do is to thank the MCS

4
00:00:06,420 --> 00:00:08,010
and the organizers for having such a

5
00:00:08,010 --> 00:00:10,260
great conference so give them a round of

6
00:00:10,260 --> 00:00:16,920
applause all right now I'm gonna ask

7
00:00:16,920 --> 00:00:19,470
something of you everyone in this room

8
00:00:19,470 --> 00:00:21,390
I'd appreciate it if you'd take your

9
00:00:21,390 --> 00:00:23,820
phones out and hold them in your hands

10
00:00:23,820 --> 00:00:30,480
you can do it okay everybody pretty much

11
00:00:30,480 --> 00:00:33,180
okay now pass your phone to the person

12
00:00:33,180 --> 00:00:44,050
on your right

13
00:00:44,060 --> 00:00:48,410
okay look at that phone feel it

14
00:00:48,410 --> 00:00:52,650
recognize it but you almost can't do

15
00:00:52,650 --> 00:00:54,420
that right because you're so distracted

16
00:00:54,420 --> 00:00:57,510
thinking about your phone in someone

17
00:00:57,510 --> 00:01:01,320
else's hands oh it's awful it's just

18
00:01:01,320 --> 00:01:04,110
awful and the thing that I want to bring

19
00:01:04,110 --> 00:01:06,090
up here is that the reason that we feel

20
00:01:06,090 --> 00:01:08,190
this way is that our phones are now an

21
00:01:08,190 --> 00:01:12,630
extension of ourselves these phones are

22
00:01:12,630 --> 00:01:15,119
become who we are in our connection to

23
00:01:15,119 --> 00:01:15,960
that data

24
00:01:15,960 --> 00:01:17,850
Pete smart makes this point when he

25
00:01:17,850 --> 00:01:19,890
talks about designing for human-computer

26
00:01:19,890 --> 00:01:21,270
interaction which we're gonna talk a

27
00:01:21,270 --> 00:01:22,890
little about a little bit today when he

28
00:01:22,890 --> 00:01:24,990
talks about our relationship with

29
00:01:24,990 --> 00:01:27,960
technology but we're gonna talk about it

30
00:01:27,960 --> 00:01:29,550
with a different stint we're not gonna

31
00:01:29,550 --> 00:01:31,440
talk about the device itself but how we

32
00:01:31,440 --> 00:01:33,690
relate to these technological advances

33
00:01:33,690 --> 00:01:35,460
so you can you can pass the phones back

34
00:01:35,460 --> 00:01:42,270
you know all right so there are a lot of

35
00:01:42,270 --> 00:01:44,099
different takes about living our lives

36
00:01:44,099 --> 00:01:46,229
increasingly online and if you're in

37
00:01:46,229 --> 00:01:48,330
this room chances are as we started to

38
00:01:48,330 --> 00:01:50,399
adopt things digitally you got excited

39
00:01:50,399 --> 00:01:52,020
we were thinking about all the things

40
00:01:52,020 --> 00:01:53,849
that we could build all the things that

41
00:01:53,849 --> 00:01:56,160
we can make and curiosity for how we

42
00:01:56,160 --> 00:02:00,569
could represent ourselves online so

43
00:02:00,569 --> 00:02:03,149
first we have geo cities which is like a

44
00:02:03,149 --> 00:02:06,030
bastion of Internet explosion I mean I

45
00:02:06,030 --> 00:02:07,950
can't make something that's good nobody

46
00:02:07,950 --> 00:02:10,259
can like that that's just it we reached

47
00:02:10,259 --> 00:02:12,540
the pinnacle right at the beginning then

48
00:02:12,540 --> 00:02:13,980
we in myspace which a lot of people

49
00:02:13,980 --> 00:02:16,050
customized everyone was friends with Tom

50
00:02:16,050 --> 00:02:17,550
and actually in putting this talk

51
00:02:17,550 --> 00:02:20,069
together I revisited mine which was a

52
00:02:20,069 --> 00:02:22,950
time capsule two terrible bangs weird

53
00:02:22,950 --> 00:02:25,380
old art pictures and weird pictures of

54
00:02:25,380 --> 00:02:27,630
me in a bathroom I don't know it was a

55
00:02:27,630 --> 00:02:30,690
face some people might have started

56
00:02:30,690 --> 00:02:33,959
developing with Dragon Ball Z and some

57
00:02:33,959 --> 00:02:35,310
people started because they wanted to be

58
00:02:35,310 --> 00:02:37,650
leet hackers so good at programming that

59
00:02:37,650 --> 00:02:41,410
their fingers were literally on fire

60
00:02:41,420 --> 00:02:44,129
whatever our interests were the Internet

61
00:02:44,129 --> 00:02:46,470
allowed us to explore and share our

62
00:02:46,470 --> 00:02:48,990
lives and express ourselves in creative

63
00:02:48,990 --> 00:02:52,000
bizarre interesting

64
00:02:52,000 --> 00:02:54,250
and unique ways that only a collectivity

65
00:02:54,250 --> 00:02:59,110
of humans could possibly imagine but the

66
00:02:59,110 --> 00:03:02,980
only constant is change and soon we had

67
00:03:02,980 --> 00:03:05,380
so many ways of being represented online

68
00:03:05,380 --> 00:03:07,330
and so many people were online that we

69
00:03:07,330 --> 00:03:09,520
needed to automate these tasks we needed

70
00:03:09,520 --> 00:03:11,530
something that could analyze build

71
00:03:11,530 --> 00:03:14,380
manage and sort for us and as things

72
00:03:14,380 --> 00:03:16,480
become more complicated what we realize

73
00:03:16,480 --> 00:03:19,210
is all of our lives become summations of

74
00:03:19,210 --> 00:03:21,970
these digital models we become pieces of

75
00:03:21,970 --> 00:03:24,310
someone else's algorithms which means we

76
00:03:24,310 --> 00:03:28,150
could be summarized in a single way so

77
00:03:28,150 --> 00:03:30,400
people are starting to use search

78
00:03:30,400 --> 00:03:32,350
engines rather than libraries or

79
00:03:32,350 --> 00:03:34,120
teachers to make sense of the world

80
00:03:34,120 --> 00:03:36,160
we're inhabiting these inaccuracies

81
00:03:36,160 --> 00:03:38,230
start to be reflected in what we learn

82
00:03:38,230 --> 00:03:40,090
what we believe in they become our

83
00:03:40,090 --> 00:03:43,510
societies and our education and tech can

84
00:03:43,510 --> 00:03:45,160
be super magical in this way right

85
00:03:45,160 --> 00:03:47,709
that's such a cool thing that we can all

86
00:03:47,709 --> 00:03:50,230
be here together for the same reason in

87
00:03:50,230 --> 00:03:53,680
Iceland but just like any other software

88
00:03:53,680 --> 00:03:56,290
we write the more magic it contains the

89
00:03:56,290 --> 00:03:59,739
more it can fail I'm Sara Dresner known

90
00:03:59,739 --> 00:04:01,720
on Twitter as Sara IDO and I work for

91
00:04:01,720 --> 00:04:06,880
Microsoft yeah and today's talk is

92
00:04:06,880 --> 00:04:15,100
called live and machine learning so in

93
00:04:15,100 --> 00:04:16,359
this talk we're gonna explore the

94
00:04:16,359 --> 00:04:17,440
paradigm shift that machine learning

95
00:04:17,440 --> 00:04:19,299
brings and we're gonna do that through

96
00:04:19,299 --> 00:04:21,160
talking about how much you need learnt

97
00:04:21,160 --> 00:04:22,510
machine learning works at a really high

98
00:04:22,510 --> 00:04:24,370
level we don't have a ton of time to go

99
00:04:24,370 --> 00:04:26,470
into it but we're also going to talk

100
00:04:26,470 --> 00:04:29,110
about challenges to democracy through

101
00:04:29,110 --> 00:04:31,210
image machine learning we're also gonna

102
00:04:31,210 --> 00:04:32,650
talk about machine learning for good

103
00:04:32,650 --> 00:04:34,810
machine learning does a lot of really

104
00:04:34,810 --> 00:04:36,040
cool things and we're gonna talk about

105
00:04:36,040 --> 00:04:38,560
those two and then we're also gonna talk

106
00:04:38,560 --> 00:04:40,810
about some of the practical tech that we

107
00:04:40,810 --> 00:04:43,180
all know so not necessarily machine

108
00:04:43,180 --> 00:04:44,440
learning but how we can make a

109
00:04:44,440 --> 00:04:45,940
difference with some of the things that

110
00:04:45,940 --> 00:04:49,180
we know today so let's talk at a very

111
00:04:49,180 --> 00:04:50,410
high level about what machine learning

112
00:04:50,410 --> 00:04:52,090
is and we'll kick it off with a quote by

113
00:04:52,090 --> 00:04:54,580
the fella who coined the term machine

114
00:04:54,580 --> 00:04:56,500
learning gives computers the ability to

115
00:04:56,500 --> 00:04:57,910
learn without being explicitly

116
00:04:57,910 --> 00:04:59,550
programmed

117
00:04:59,550 --> 00:05:02,229
okay so we're programmers we're familiar

118
00:05:02,229 --> 00:05:05,260
with if-then statements so what if these

119
00:05:05,260 --> 00:05:05,870
come

120
00:05:05,870 --> 00:05:07,400
Editions for the if-then statements

121
00:05:07,400 --> 00:05:12,080
become complex I mean really complex if

122
00:05:12,080 --> 00:05:14,270
you're building a program to understand

123
00:05:14,270 --> 00:05:16,850
that this is a pug and this is a kitten

124
00:05:16,850 --> 00:05:18,980
you don't necessarily want to program an

125
00:05:18,980 --> 00:05:21,590
if-statement for every single pixel here

126
00:05:21,590 --> 00:05:24,020
or the possibility for every pixel for

127
00:05:24,020 --> 00:05:26,000
every single pug photo I mean that's not

128
00:05:26,000 --> 00:05:28,910
how our brains work right so how do you

129
00:05:28,910 --> 00:05:31,040
program it well you want to teach it a

130
00:05:31,040 --> 00:05:34,040
certain pug leanness you want your

131
00:05:34,040 --> 00:05:35,930
teacher to let you you want your model

132
00:05:35,930 --> 00:05:39,560
to learn the essence of the Pug so in

133
00:05:39,560 --> 00:05:41,240
truth there are a few ways of doing this

134
00:05:41,240 --> 00:05:42,980
sometimes like the example above we can

135
00:05:42,980 --> 00:05:45,020
tag a set of data and your model will

136
00:05:45,020 --> 00:05:46,850
use this supervised data set to

137
00:05:46,850 --> 00:05:48,740
distinguish between these classes and

138
00:05:48,740 --> 00:05:50,690
this will help it learn better this is

139
00:05:50,690 --> 00:05:53,330
called supervised machine learning but

140
00:05:53,330 --> 00:05:55,490
there are other times where we don't

141
00:05:55,490 --> 00:05:57,350
really know the answer so if I asked a

142
00:05:57,350 --> 00:05:59,000
computer what those pugs would look like

143
00:05:59,000 --> 00:06:00,230
if they were bred together with

144
00:06:00,230 --> 00:06:02,810
stegosauruses well that's not really a

145
00:06:02,810 --> 00:06:06,110
thing so there's nothing we can check

146
00:06:06,110 --> 00:06:07,850
against and that's when we'll use an

147
00:06:07,850 --> 00:06:11,150
unsupervised process but let's dive into

148
00:06:11,150 --> 00:06:12,680
supervised process because it's the

149
00:06:12,680 --> 00:06:14,300
easiest to explain and the time we have

150
00:06:14,300 --> 00:06:16,670
and it's also pretty common and in truth

151
00:06:16,670 --> 00:06:18,500
even within supervised machine learning

152
00:06:18,500 --> 00:06:20,750
there are thousands of algorithms that

153
00:06:20,750 --> 00:06:22,510
you could use and among those

154
00:06:22,510 --> 00:06:24,500
convolutional neural networks are

155
00:06:24,500 --> 00:06:27,560
probably the most popular but genetic is

156
00:06:27,560 --> 00:06:29,750
the easiest to understand so we're gonna

157
00:06:29,750 --> 00:06:32,060
use that to kind of discuss things on a

158
00:06:32,060 --> 00:06:35,120
high level today so let's explore a

159
00:06:35,120 --> 00:06:37,670
simple toy model in order to make a pug

160
00:06:37,670 --> 00:06:40,280
kitten sorter what we do is build an

161
00:06:40,280 --> 00:06:42,620
algorithm that builds algorithms and

162
00:06:42,620 --> 00:06:45,200
we'll build many of these and they'll do

163
00:06:45,200 --> 00:06:48,050
their best to sort these and do their

164
00:06:48,050 --> 00:06:51,140
best sorting and at first it will be bad

165
00:06:51,140 --> 00:06:52,850
at it they won't just be bad at it

166
00:06:52,850 --> 00:06:55,940
they'll be terrible but we have another

167
00:06:55,940 --> 00:06:57,890
function that will check this and

168
00:06:57,890 --> 00:07:00,080
they'll check it against that tag data

169
00:07:00,080 --> 00:07:02,960
and for every kitten in pug scenario so

170
00:07:02,960 --> 00:07:05,900
the ones that get it wrong are thrown

171
00:07:05,900 --> 00:07:07,700
away and the ones that get it right

172
00:07:07,700 --> 00:07:10,460
circle back to the Builder and then the

173
00:07:10,460 --> 00:07:12,950
Builder starts using those to create new

174
00:07:12,950 --> 00:07:15,920
builders now this might seem hacky and

175
00:07:15,920 --> 00:07:18,500
imperfect and that's because it is if we

176
00:07:18,500 --> 00:07:19,550
did this one

177
00:07:19,550 --> 00:07:21,800
twice or three times it would fail but

178
00:07:21,800 --> 00:07:25,520
we do it thousands of times and we don't

179
00:07:25,520 --> 00:07:27,229
do it for a small amount of conditions

180
00:07:27,229 --> 00:07:29,479
either we build up all of the gradients

181
00:07:29,479 --> 00:07:31,070
that make up an eye and then we match it

182
00:07:31,070 --> 00:07:33,020
up against all of the other eyes that we

183
00:07:33,020 --> 00:07:35,270
know of and then we match that against

184
00:07:35,270 --> 00:07:38,330
the likelihood of snouts and in fact our

185
00:07:38,330 --> 00:07:40,789
own classification system genus kingdom

186
00:07:40,789 --> 00:07:43,520
phylum is a really human comprehensible

187
00:07:43,520 --> 00:07:45,830
version composed of this kind of set of

188
00:07:45,830 --> 00:07:49,280
rules so the part I want you to remember

189
00:07:49,280 --> 00:07:52,039
is this moving forward the point where

190
00:07:52,039 --> 00:07:55,280
we check you can see how that's a very

191
00:07:55,280 --> 00:07:57,650
important part of this we have to check

192
00:07:57,650 --> 00:07:59,510
if we want to get the data right and we

193
00:07:59,510 --> 00:08:02,110
must do so and keep on checking and

194
00:08:02,110 --> 00:08:05,030
another really important part here is if

195
00:08:05,030 --> 00:08:08,419
I only gave these pug pictures that it

196
00:08:08,419 --> 00:08:09,860
was fed on I mean that would be correct

197
00:08:09,860 --> 00:08:13,910
right these are all pugs but what would

198
00:08:13,910 --> 00:08:15,560
what might happen is it might fail when

199
00:08:15,560 --> 00:08:18,080
it's trying to find our one true pug so

200
00:08:18,080 --> 00:08:20,479
the type of data that we train on is

201
00:08:20,479 --> 00:08:22,940
really vital to whether or not we have

202
00:08:22,940 --> 00:08:25,970
an accurate model okay

203
00:08:25,970 --> 00:08:27,949
so let's explore some ways that machine

204
00:08:27,949 --> 00:08:29,539
learning has failed but I'm gonna call

205
00:08:29,539 --> 00:08:31,460
this section the road to hell is paved

206
00:08:31,460 --> 00:08:33,650
with good intentions because none of

207
00:08:33,650 --> 00:08:35,750
what I'm showing you here today is made

208
00:08:35,750 --> 00:08:38,270
with the idea of corrupting society

209
00:08:38,270 --> 00:08:39,860
quite the opposite all of these are

210
00:08:39,860 --> 00:08:42,620
really and well intentioned examples but

211
00:08:42,620 --> 00:08:44,089
their failure is really good for us to

212
00:08:44,089 --> 00:08:45,890
explore so that we don't fail in the

213
00:08:45,890 --> 00:08:48,920
future so let's say you need to patrol a

214
00:08:48,920 --> 00:08:50,900
city but you don't have enough patrol

215
00:08:50,900 --> 00:08:53,600
units to completely cover the city so it

216
00:08:53,600 --> 00:08:55,520
might make sense to figure out where all

217
00:08:55,520 --> 00:08:57,890
the crime is happening and just have

218
00:08:57,890 --> 00:08:59,839
some patrol units that focus on the

219
00:08:59,839 --> 00:09:02,150
areas of most activity but the problem

220
00:09:02,150 --> 00:09:05,000
is not all crime is equal to get enough

221
00:09:05,000 --> 00:09:07,820
data you have to include nuisance crimes

222
00:09:07,820 --> 00:09:09,470
in which no one's hurt like homelessness

223
00:09:09,470 --> 00:09:11,750
or panhandling with the violent crimes

224
00:09:11,750 --> 00:09:14,209
like rape and murder but rape and murder

225
00:09:14,209 --> 00:09:16,160
don't tend to be clustered in this way

226
00:09:16,160 --> 00:09:18,620
it takes a lot more data to build an

227
00:09:18,620 --> 00:09:20,900
area which is why the nuisance data is

228
00:09:20,900 --> 00:09:23,810
included so on another issue is that

229
00:09:23,810 --> 00:09:26,540
those nuisance data areas those tend to

230
00:09:26,540 --> 00:09:28,820
be the poorer areas so what ends up

231
00:09:28,820 --> 00:09:31,160
happening is you get more patrol areas

232
00:09:31,160 --> 00:09:33,079
in the poorer areas and then we have

233
00:09:33,079 --> 00:09:33,260
more

234
00:09:33,260 --> 00:09:35,990
data on those patrol areas and we get

235
00:09:35,990 --> 00:09:38,120
into this weird feedback loop where

236
00:09:38,120 --> 00:09:40,160
we're just patrolling those and then the

237
00:09:40,160 --> 00:09:41,900
rapes and murders that happen in the

238
00:09:41,900 --> 00:09:44,570
affluent areas start to not be reported

239
00:09:44,570 --> 00:09:46,580
and this is what saw the software pred

240
00:09:46,580 --> 00:09:47,900
pull does that we use in the United

241
00:09:47,900 --> 00:09:50,420
States and it's an example of sample

242
00:09:50,420 --> 00:09:54,530
bias and machine learning so another

243
00:09:54,530 --> 00:09:56,150
issue with this that the US Department

244
00:09:56,150 --> 00:09:57,740
of Education has found is that once

245
00:09:57,740 --> 00:09:58,580
you're a criminal

246
00:09:58,580 --> 00:10:01,130
sorry once you're a criminal you're

247
00:10:01,130 --> 00:10:03,260
pretty likely to become a criminal again

248
00:10:03,260 --> 00:10:05,630
a lot of people keep reoffending and

249
00:10:05,630 --> 00:10:08,000
what they found was the reason why

250
00:10:08,000 --> 00:10:10,160
people do this is that they report that

251
00:10:10,160 --> 00:10:11,660
they find it really hard to fit back

252
00:10:11,660 --> 00:10:13,790
into society once that once they're in

253
00:10:13,790 --> 00:10:16,190
jail another thing is that employment

254
00:10:16,190 --> 00:10:17,900
becomes more difficult because you have

255
00:10:17,900 --> 00:10:20,150
to disclose that you were once in jail

256
00:10:20,150 --> 00:10:21,860
on your employment forms and then people

257
00:10:21,860 --> 00:10:24,710
are less likely to employ you but here's

258
00:10:24,710 --> 00:10:27,980
an important part employment tends to be

259
00:10:27,980 --> 00:10:29,750
the number one indicator of whether

260
00:10:29,750 --> 00:10:32,020
you're gonna end up back in prison and

261
00:10:32,020 --> 00:10:34,880
even the analysis of who ends up in the

262
00:10:34,880 --> 00:10:37,190
system uses machine learning so when

263
00:10:37,190 --> 00:10:39,290
most defendants are booked in jail in

264
00:10:39,290 --> 00:10:41,210
jail they respond to a coppiced

265
00:10:41,210 --> 00:10:42,620
questionnaire and this is fed into a

266
00:10:42,620 --> 00:10:45,710
system that defines the risk level for

267
00:10:45,710 --> 00:10:47,420
that's used in the length of their

268
00:10:47,420 --> 00:10:49,010
sentencing so whether you're in for a

269
00:10:49,010 --> 00:10:50,840
short amount of time or a long amount of

270
00:10:50,840 --> 00:10:53,420
time in an evaluation of more than

271
00:10:53,420 --> 00:10:56,390
10,000 of these people found that the

272
00:10:56,390 --> 00:10:58,550
risk scores showed a startling

273
00:10:58,550 --> 00:11:00,890
difference black defendants had scores

274
00:11:00,890 --> 00:11:03,020
pretty evenly matched across the board

275
00:11:03,020 --> 00:11:05,810
one meaning no risk 10 would have have a

276
00:11:05,810 --> 00:11:08,240
very long sentence whereas the white

277
00:11:08,240 --> 00:11:10,100
risk assessments look quite different

278
00:11:10,100 --> 00:11:12,920
many are assessed at one steadily degree

279
00:11:12,920 --> 00:11:16,280
decreasing - very few at ten now you

280
00:11:16,280 --> 00:11:17,930
might say well what's what if that's

281
00:11:17,930 --> 00:11:21,260
just valid data except upon analysis

282
00:11:21,260 --> 00:11:23,210
it's not really the study showed that

283
00:11:23,210 --> 00:11:24,950
blacks were twice as likely to be

284
00:11:24,950 --> 00:11:27,890
labeled higher risk and not reoffended

285
00:11:27,890 --> 00:11:29,960
white counterparts were labeled not as

286
00:11:29,960 --> 00:11:32,600
risky as they were so if we look at this

287
00:11:32,600 --> 00:11:34,640
in human terms we see James reveille

288
00:11:34,640 --> 00:11:36,800
with low risk at three Robert cannon

289
00:11:36,800 --> 00:11:38,600
with medium risk at six

290
00:11:38,600 --> 00:11:41,090
James reveille had a domestic violence

291
00:11:41,090 --> 00:11:43,370
aggravated assault grand theft petty

292
00:11:43,370 --> 00:11:45,350
theft drug trafficking and he

293
00:11:45,350 --> 00:11:46,880
subsequently did

294
00:11:46,880 --> 00:11:49,540
and Robert cannon had one petty theft

295
00:11:49,540 --> 00:11:53,210
there are more - Gregory Lugo low risk

296
00:11:53,210 --> 00:11:55,790
at one Mallory Williams medium risk at

297
00:11:55,790 --> 00:11:56,360
six

298
00:11:56,360 --> 00:11:59,600
Gregory Lugo had three duis one battery

299
00:11:59,600 --> 00:12:02,750
and subsequently reoffended Mallory

300
00:12:02,750 --> 00:12:04,700
Williams had two misdemeanors and never

301
00:12:04,700 --> 00:12:06,770
reoffended again and these are just a

302
00:12:06,770 --> 00:12:10,040
couple of them there's a lot these kind

303
00:12:10,040 --> 00:12:11,900
of algorithms don't just affect black

304
00:12:11,900 --> 00:12:14,120
people either Zilly is a construction

305
00:12:14,120 --> 00:12:16,040
worker who stole a piece of equipment

306
00:12:16,040 --> 00:12:17,900
and sold it for parts because he was low

307
00:12:17,900 --> 00:12:20,120
on cash since then he's been going to

308
00:12:20,120 --> 00:12:22,820
church he's been volunteering but he

309
00:12:22,820 --> 00:12:24,950
says that his risk score doesn't reflect

310
00:12:24,950 --> 00:12:26,930
any of the ways that he's bettering

311
00:12:26,930 --> 00:12:29,390
himself as a person therefore not really

312
00:12:29,390 --> 00:12:31,490
a good indicator of whether or not he's

313
00:12:31,490 --> 00:12:34,580
going to reoffending n so what makes

314
00:12:34,580 --> 00:12:36,290
coppice fail as a machine learning

315
00:12:36,290 --> 00:12:39,110
algorithm one thing is that it has no

316
00:12:39,110 --> 00:12:41,480
transparency you basically make the

317
00:12:41,480 --> 00:12:43,970
algorithm and then they are sending it

318
00:12:43,970 --> 00:12:46,490
out so basically the people who are

319
00:12:46,490 --> 00:12:48,800
using it don't know how it works

320
00:12:48,800 --> 00:12:51,290
another thing is because of this because

321
00:12:51,290 --> 00:12:52,820
of that relationship it doesn't adapt

322
00:12:52,820 --> 00:12:54,710
over time remember we talked about how

323
00:12:54,710 --> 00:12:57,550
important that step of adapting is and

324
00:12:57,550 --> 00:12:59,870
related to these things people use it

325
00:12:59,870 --> 00:13:02,660
without any questions another question

326
00:13:02,660 --> 00:13:05,120
that might be asked at this time is

327
00:13:05,120 --> 00:13:07,220
whether this is a good place for machine

328
00:13:07,220 --> 00:13:08,570
learning algorithms machine learning

329
00:13:08,570 --> 00:13:10,700
algorithms are super powerful and really

330
00:13:10,700 --> 00:13:12,800
wonderful but sometimes this should be

331
00:13:12,800 --> 00:13:15,080
in the hands of a judge and here's the

332
00:13:15,080 --> 00:13:16,370
main thing about machine learning

333
00:13:16,370 --> 00:13:18,950
algorithms we try to be predictive of

334
00:13:18,950 --> 00:13:20,750
the future but machine learning

335
00:13:20,750 --> 00:13:23,210
algorithms can really only evaluate the

336
00:13:23,210 --> 00:13:27,050
past so Brennan one of the co-creators

337
00:13:27,050 --> 00:13:29,510
of this technology didn't actually build

338
00:13:29,510 --> 00:13:31,280
the technology with the idea of the use

339
00:13:31,280 --> 00:13:33,170
for sentencing but that's the strange

340
00:13:33,170 --> 00:13:35,060
thing about some of this tech once you

341
00:13:35,060 --> 00:13:36,980
put it out in the world adoption can be

342
00:13:36,980 --> 00:13:38,960
beyond us anyone who maintains an

343
00:13:38,960 --> 00:13:41,270
open-source project will say that

344
00:13:41,270 --> 00:13:42,560
sometimes you put something out in the

345
00:13:42,560 --> 00:13:44,570
world and the way people use it is

346
00:13:44,570 --> 00:13:46,250
totally different than the way that you

347
00:13:46,250 --> 00:13:47,900
intended or the way that you had

348
00:13:47,900 --> 00:13:50,600
foreseen this is why it's critical for

349
00:13:50,600 --> 00:13:52,610
these types of technologies to be

350
00:13:52,610 --> 00:13:54,890
transparent in order to not be abused

351
00:13:54,890 --> 00:13:57,830
they can't be magical people will say

352
00:13:57,830 --> 00:14:00,000
algorithms can't be biased

353
00:14:00,000 --> 00:14:03,420
but algorithms are written by people so

354
00:14:03,420 --> 00:14:04,920
this doesn't always happen to criminal

355
00:14:04,920 --> 00:14:06,870
only happen to criminals it happens to

356
00:14:06,870 --> 00:14:07,920
us too

357
00:14:07,920 --> 00:14:09,270
machine learning is now incorporated

358
00:14:09,270 --> 00:14:10,980
into a lot of the tools that we use

359
00:14:10,980 --> 00:14:13,530
every day and a lot of our Internet

360
00:14:13,530 --> 00:14:15,810
experience is built off of machine

361
00:14:15,810 --> 00:14:18,240
learning algorithms so axiom a data

362
00:14:18,240 --> 00:14:21,930
brokerage had an average of 1,500 data

363
00:14:21,930 --> 00:14:24,390
points for 500 million consumers

364
00:14:24,390 --> 00:14:28,350
including the entire adult population of

365
00:14:28,350 --> 00:14:32,370
the United States that was in 2012 and

366
00:14:32,370 --> 00:14:37,170
that's just one company so using a

367
00:14:37,170 --> 00:14:39,060
Chrome extension called what Facebook

368
00:14:39,060 --> 00:14:40,920
thinks you like I could find some of the

369
00:14:40,920 --> 00:14:43,200
ways that the systems had tagged me so

370
00:14:43,200 --> 00:14:44,670
some of these are pretty obvious if you

371
00:14:44,670 --> 00:14:46,020
follow me on Twitter you probably know

372
00:14:46,020 --> 00:14:47,370
some of these things like software

373
00:14:47,370 --> 00:14:48,960
developer I'm into cloud computing I

374
00:14:48,960 --> 00:14:54,240
like dogs like wine to strange and like

375
00:14:54,240 --> 00:14:57,210
kind of false so mermaid I don't know

376
00:14:57,210 --> 00:14:59,430
trenchcoats I've never read a James

377
00:14:59,430 --> 00:15:00,480
Patterson book

378
00:15:00,480 --> 00:15:02,310
I don't knit and it even got my

379
00:15:02,310 --> 00:15:06,420
political proclivities wrong so the

380
00:15:06,420 --> 00:15:09,990
issue with this is Facebook uses these

381
00:15:09,990 --> 00:15:12,720
types of tags to drive directed ads and

382
00:15:12,720 --> 00:15:14,970
you can see you that you can use these

383
00:15:14,970 --> 00:15:17,190
ads to target on different demographics

384
00:15:17,190 --> 00:15:19,800
including race and a couple years ago

385
00:15:19,800 --> 00:15:22,140
people could show you different houses

386
00:15:22,140 --> 00:15:23,850
that you could buy depending on these

387
00:15:23,850 --> 00:15:26,670
demographics depending on race this is

388
00:15:26,670 --> 00:15:28,620
illegal under the Fair Housing Act of

389
00:15:28,620 --> 00:15:31,500
1968 but here's the thing

390
00:15:31,500 --> 00:15:33,750
once that was pointed out to Facebook

391
00:15:33,750 --> 00:15:37,170
they fixed it and this is a major theme

392
00:15:37,170 --> 00:15:39,900
here it's not to make software that

393
00:15:39,900 --> 00:15:42,420
never fails that's impossible every

394
00:15:42,420 --> 00:15:44,430
machine learning algorithm can have a

395
00:15:44,430 --> 00:15:47,790
failure but once it does fail you have

396
00:15:47,790 --> 00:15:49,170
to course-correct

397
00:15:49,170 --> 00:15:51,660
and change things and another problem

398
00:15:51,660 --> 00:15:53,010
with this that you might have already

399
00:15:53,010 --> 00:15:55,080
thought about is that you don't tell

400
00:15:55,080 --> 00:15:57,450
Facebook your race their targeting based

401
00:15:57,450 --> 00:16:00,450
on ethnic affinity which is used by the

402
00:16:00,450 --> 00:16:02,190
same algorithm that thinks I'm into

403
00:16:02,190 --> 00:16:04,980
mermaids so I'm not coming down on

404
00:16:04,980 --> 00:16:07,110
Facebook in particular this is a problem

405
00:16:07,110 --> 00:16:09,000
with all machine learning algorithms

406
00:16:09,000 --> 00:16:13,140
used by every company so this the issue

407
00:16:13,140 --> 00:16:13,800
with this

408
00:16:13,800 --> 00:16:15,390
and what you might have guessed is that

409
00:16:15,390 --> 00:16:16,680
everything that you're seeing is

410
00:16:16,680 --> 00:16:19,529
filtered and that filtration system can

411
00:16:19,529 --> 00:16:22,170
be fallible at best and illegal at worse

412
00:16:22,170 --> 00:16:24,540
it might not mean houses it can also

413
00:16:24,540 --> 00:16:26,490
mean that the things and the studies

414
00:16:26,490 --> 00:16:28,230
that you're seeing and educational

415
00:16:28,230 --> 00:16:30,240
material that you're seeing is based on

416
00:16:30,240 --> 00:16:32,399
what you already believe which like that

417
00:16:32,399 --> 00:16:34,290
patrolling system that we saw earlier

418
00:16:34,290 --> 00:16:37,829
becomes a self-reinforcing prophecy and

419
00:16:37,829 --> 00:16:39,870
I've also personally seen this I worked

420
00:16:39,870 --> 00:16:41,490
at one point I work for a company that

421
00:16:41,490 --> 00:16:42,839
was starting to use machine learning

422
00:16:42,839 --> 00:16:45,600
they proposed an idea to surface content

423
00:16:45,600 --> 00:16:48,450
based on demographics I had just

424
00:16:48,450 --> 00:16:50,640
happened to read up on that issue this

425
00:16:50,640 --> 00:16:53,040
that weekend mainly that it wasn't legal

426
00:16:53,040 --> 00:16:54,779
and I brought it up to people and they

427
00:16:54,779 --> 00:16:56,399
said oh yeah that you're right this

428
00:16:56,399 --> 00:16:58,680
could probably and introduce bias so we

429
00:16:58,680 --> 00:17:00,810
didn't ship that feature but here's an

430
00:17:00,810 --> 00:17:03,000
important part I was just a regular

431
00:17:03,000 --> 00:17:05,280
engineer on that project I wasn't a data

432
00:17:05,280 --> 00:17:07,290
scientist and I wasn't a PM I wasn't the

433
00:17:07,290 --> 00:17:09,270
person shaping that project I was just

434
00:17:09,270 --> 00:17:11,699
carrying things out but without raising

435
00:17:11,699 --> 00:17:13,439
my hand we might have shipped that

436
00:17:13,439 --> 00:17:16,640
feature to some really bad consequences

437
00:17:16,640 --> 00:17:19,319
it's not that digitizing the world is

438
00:17:19,319 --> 00:17:21,540
inherently bad but the more technology

439
00:17:21,540 --> 00:17:23,910
becomes embedded in all aspects of life

440
00:17:23,910 --> 00:17:26,819
it matters whether the technology is

441
00:17:26,819 --> 00:17:30,419
biased alienating or harmful and these

442
00:17:30,419 --> 00:17:31,919
tales are pretty dark but machine

443
00:17:31,919 --> 00:17:33,600
learning offers us a lot of amazing

444
00:17:33,600 --> 00:17:35,700
benefits as well as my brilliant

445
00:17:35,700 --> 00:17:38,010
co-worker Paige Bailey says it's like

446
00:17:38,010 --> 00:17:39,780
the risk that we take every time we get

447
00:17:39,780 --> 00:17:41,429
into cars or airplanes they've

448
00:17:41,429 --> 00:17:43,710
completely transformed every aspect of

449
00:17:43,710 --> 00:17:45,600
life and made things possible that

450
00:17:45,600 --> 00:17:47,250
wouldn't be possible before they're

451
00:17:47,250 --> 00:17:49,530
inherently dangerous and they can be

452
00:17:49,530 --> 00:17:51,929
seriously misused so think about it I

453
00:17:51,929 --> 00:17:53,429
mean I wouldn't be on this stage if it

454
00:17:53,429 --> 00:17:54,750
wasn't for airplanes and some of you

455
00:17:54,750 --> 00:17:56,760
wouldn't be here right all in the same

456
00:17:56,760 --> 00:17:58,200
room that's pretty amazing

457
00:17:58,200 --> 00:18:00,210
but people have used airplanes as

458
00:18:00,210 --> 00:18:02,250
weapons and so we have a certain respect

459
00:18:02,250 --> 00:18:05,010
for them okay now that I've bummed

460
00:18:05,010 --> 00:18:09,000
everyone out let's explore some of the

461
00:18:09,000 --> 00:18:10,679
wonderful applications for machine

462
00:18:10,679 --> 00:18:12,950
learning and what we can do with it

463
00:18:12,950 --> 00:18:15,480
okay as I mentioned I work for Microsoft

464
00:18:15,480 --> 00:18:17,250
and an azure we have a thing called

465
00:18:17,250 --> 00:18:19,440
cognitive services there's a specific

466
00:18:19,440 --> 00:18:22,740
API called the mote emotion API and what

467
00:18:22,740 --> 00:18:24,900
it allow us to do is analyze things like

468
00:18:24,900 --> 00:18:27,300
pictures and videos and see what emotion

469
00:18:27,300 --> 00:18:27,539
it

470
00:18:27,539 --> 00:18:29,759
Tech's from what the computer sees it

471
00:18:29,759 --> 00:18:31,710
can use to be used to make things like

472
00:18:31,710 --> 00:18:35,369
this which I built in three Jas which

473
00:18:35,369 --> 00:18:37,879
has no practical application except

474
00:18:37,879 --> 00:18:39,419
entertaining my four-year-old

475
00:18:39,419 --> 00:18:50,039
stepdaughter Wow yeah there's stuff up

476
00:18:50,039 --> 00:18:54,009
there huh

477
00:18:54,019 --> 00:18:58,559
okay which can also be pretty magical to

478
00:18:58,559 --> 00:19:00,479
tell you the truth but what if we put

479
00:19:00,479 --> 00:19:02,759
this to more useful use okay so I was

480
00:19:02,759 --> 00:19:04,229
mentoring a blind woman and she

481
00:19:04,229 --> 00:19:06,509
mentioned to me that she felt left out

482
00:19:06,509 --> 00:19:07,950
of a lot of conversations on Twitter

483
00:19:07,950 --> 00:19:09,570
because a lot of the photos didn't have

484
00:19:09,570 --> 00:19:12,720
alt text and this is true for her even

485
00:19:12,720 --> 00:19:15,330
on things like news sites so I made I

486
00:19:15,330 --> 00:19:17,340
used another cognitive services offering

487
00:19:17,340 --> 00:19:19,679
the computer vision API which will not

488
00:19:19,679 --> 00:19:22,619
only analyze photo but also its contents

489
00:19:22,619 --> 00:19:24,929
like words and just to show how some of

490
00:19:24,929 --> 00:19:27,239
this works we're gonna dive briefly into

491
00:19:27,239 --> 00:19:30,389
some of the code so we'll get the image

492
00:19:30,389 --> 00:19:31,950
from the user I'm using view jeaious

493
00:19:31,950 --> 00:19:33,720
here these are methods and we're gonna

494
00:19:33,720 --> 00:19:36,119
store it then you can see here we're

495
00:19:36,119 --> 00:19:38,279
using Axios for a simple call to the

496
00:19:38,279 --> 00:19:40,200
computer vision API and then we're

497
00:19:40,200 --> 00:19:41,279
you're gonna use address cognitive

498
00:19:41,279 --> 00:19:44,729
services to analyze that image and we'll

499
00:19:44,729 --> 00:19:46,979
do so in a couple of different ways one

500
00:19:46,979 --> 00:19:49,200
to see what the image contains and one

501
00:19:49,200 --> 00:19:51,419
to check for text within the image then

502
00:19:51,419 --> 00:19:53,429
we'll dynamically add the alt text to

503
00:19:53,429 --> 00:19:55,649
the image and now that this is built I'm

504
00:19:55,649 --> 00:19:57,840
building a Chrome extension so that

505
00:19:57,840 --> 00:19:59,519
blind people can use it on any website

506
00:19:59,519 --> 00:20:02,669
that they can visit visit so this

507
00:20:02,669 --> 00:20:04,289
application of machine learning can help

508
00:20:04,289 --> 00:20:06,119
bring more people into the conversation

509
00:20:06,119 --> 00:20:09,149
and broaden our reach it's not perfect

510
00:20:09,149 --> 00:20:10,409
but it's headed in a really good

511
00:20:10,409 --> 00:20:12,059
direction where all of a sudden people

512
00:20:12,059 --> 00:20:14,429
who are left out of the story can now be

513
00:20:14,429 --> 00:20:15,869
part of it and that's the amazing thing

514
00:20:15,869 --> 00:20:18,809
that by being inclusive you're actually

515
00:20:18,809 --> 00:20:20,999
meeting business goals it doesn't mean

516
00:20:20,999 --> 00:20:22,470
you're excluding the people in the

517
00:20:22,470 --> 00:20:24,629
center it means that you're reaching a

518
00:20:24,629 --> 00:20:27,720
larger audience there are all sorts of

519
00:20:27,720 --> 00:20:30,149
ways that other people can use machine

520
00:20:30,149 --> 00:20:32,549
learning in medicine too so one of these

521
00:20:32,549 --> 00:20:34,590
is Sophia genetics is training

522
00:20:34,590 --> 00:20:37,379
algorithms to diagnose diseases through

523
00:20:37,379 --> 00:20:40,470
DNA analysis another one that's kind of

524
00:20:40,470 --> 00:20:41,400
dear to my heart

525
00:20:41,400 --> 00:20:43,860
is that all for cure is a social network

526
00:20:43,860 --> 00:20:46,020
for cancer patients where patients can

527
00:20:46,020 --> 00:20:47,850
enter diagnosis and treatment so that

528
00:20:47,850 --> 00:20:50,040
can be analyzed this is really important

529
00:20:50,040 --> 00:20:53,160
because doctors can't just report every

530
00:20:53,160 --> 00:20:55,170
single patient that they have and they

531
00:20:55,170 --> 00:20:57,390
can't read every single study either so

532
00:20:57,390 --> 00:20:58,680
if you have all of these people

533
00:20:58,680 --> 00:21:00,870
reporting what treatments they were used

534
00:21:00,870 --> 00:21:02,880
whether or not they're surviving well

535
00:21:02,880 --> 00:21:04,440
then all of a sudden this machine

536
00:21:04,440 --> 00:21:06,390
learning can pick up all of that data

537
00:21:06,390 --> 00:21:08,610
and we we can become much smarter in

538
00:21:08,610 --> 00:21:11,910
finding cures for cancer health map

539
00:21:11,910 --> 00:21:13,830
algorithm spotted in an Ebola outbreak

540
00:21:13,830 --> 00:21:15,750
nine days before the World Health

541
00:21:15,750 --> 00:21:18,000
Organization and this is a really

542
00:21:18,000 --> 00:21:19,530
important one so I'm just gonna focus on

543
00:21:19,530 --> 00:21:22,410
for just a second nine days is an

544
00:21:22,410 --> 00:21:24,630
incredible amount of lead time for on an

545
00:21:24,630 --> 00:21:27,510
outbreak outbreaks grow exponentially so

546
00:21:27,510 --> 00:21:29,670
by using machine learning we adjusted

547
00:21:29,670 --> 00:21:31,740
the amount of people infected from here

548
00:21:31,740 --> 00:21:36,690
to here that's amazing so you might be

549
00:21:36,690 --> 00:21:38,760
saying to yourself this is all well and

550
00:21:38,760 --> 00:21:40,650
good but I'm not a data scientist and I

551
00:21:40,650 --> 00:21:42,450
don't work with machine learning but

552
00:21:42,450 --> 00:21:45,000
that's okay this subject that about how

553
00:21:45,000 --> 00:21:47,730
technology can shape our lives is really

554
00:21:47,730 --> 00:21:50,130
broad and it doesn't necessarily have to

555
00:21:50,130 --> 00:21:52,200
be machine learning so let's talk about

556
00:21:52,200 --> 00:21:53,640
some of the things that we as web

557
00:21:53,640 --> 00:21:56,160
developers can do now today with what we

558
00:21:56,160 --> 00:21:58,980
already know we work every day on

559
00:21:58,980 --> 00:22:00,600
building applications for so many

560
00:22:00,600 --> 00:22:02,790
different use cases so what if we take

561
00:22:02,790 --> 00:22:04,800
some of the more generic tools that we

562
00:22:04,800 --> 00:22:07,320
have and put them to tasks where we can

563
00:22:07,320 --> 00:22:10,380
maybe help save lives so there's a

564
00:22:10,380 --> 00:22:11,820
firehouse near where I live and they

565
00:22:11,820 --> 00:22:13,650
have some sufficiently complex systems

566
00:22:13,650 --> 00:22:15,030
to make sure that everything is online

567
00:22:15,030 --> 00:22:17,040
and functioning I went out to their

568
00:22:17,040 --> 00:22:18,690
station and I asked them a few questions

569
00:22:18,690 --> 00:22:21,210
about how everything works and to see if

570
00:22:21,210 --> 00:22:22,650
there was anything that I could do that

571
00:22:22,650 --> 00:22:24,930
I might help that might help and here's

572
00:22:24,930 --> 00:22:27,420
what I learned when the truck goes out

573
00:22:27,420 --> 00:22:29,370
it has to communicate with a dispatcher

574
00:22:29,370 --> 00:22:31,350
which will help facilitate if they need

575
00:22:31,350 --> 00:22:34,530
more water supplies or backup and the

576
00:22:34,530 --> 00:22:36,060
app is really simple kind of

577
00:22:36,060 --> 00:22:37,500
purposefully so because they want

578
00:22:37,500 --> 00:22:39,540
everybody able to be able to use it

579
00:22:39,540 --> 00:22:41,550
quickly at a glance and there aren't

580
00:22:41,550 --> 00:22:43,050
that many things that they might need so

581
00:22:43,050 --> 00:22:44,730
for their use case what they created is

582
00:22:44,730 --> 00:22:47,130
really perfect but I asked a few

583
00:22:47,130 --> 00:22:49,740
questions including what happens if

584
00:22:49,740 --> 00:22:51,750
they're all of a sudden offline what if

585
00:22:51,750 --> 00:22:54,270
they can can't communicate well right

586
00:22:54,270 --> 00:22:55,530
now there

587
00:22:55,530 --> 00:23:00,000
get it host fireman okay anyway terrible

588
00:23:00,000 --> 00:23:03,510
joke okay so here's our opportunity what

589
00:23:03,510 --> 00:23:04,800
I created was a really simplified

590
00:23:04,800 --> 00:23:06,750
version of their app preserving the

591
00:23:06,750 --> 00:23:09,030
basic layout but I made use of service

592
00:23:09,030 --> 00:23:11,070
workers and background sync to alert

593
00:23:11,070 --> 00:23:13,080
them when they're offline when they're

594
00:23:13,080 --> 00:23:15,570
offline cue the messages and when it's

595
00:23:15,570 --> 00:23:17,640
backup send them out even if the browser

596
00:23:17,640 --> 00:23:20,090
tab is closed even if something happens

597
00:23:20,090 --> 00:23:22,020
so here's what the application looks

598
00:23:22,020 --> 00:23:25,590
like if we click on water it register if

599
00:23:25,590 --> 00:23:27,240
the sink is registered and it sends it

600
00:23:27,240 --> 00:23:30,960
out but if we go off Wi-Fi then it'll

601
00:23:30,960 --> 00:23:33,929
tell them that they're off offline and

602
00:23:33,929 --> 00:23:35,910
then I could even close out that browser

603
00:23:35,910 --> 00:23:39,870
tab go back and then we can turn the

604
00:23:39,870 --> 00:23:44,640
Wi-Fi back on and then the sync is

605
00:23:44,640 --> 00:23:46,710
registered and what we requested before

606
00:23:46,710 --> 00:23:49,860
is then sent out so the first thing that

607
00:23:49,860 --> 00:23:51,240
we're going to do is alert them one that

608
00:23:51,240 --> 00:23:53,070
when they're offline we're gonna do that

609
00:23:53,070 --> 00:23:55,980
with navigator dot online and we're

610
00:23:55,980 --> 00:23:57,390
ready we're gonna register the service

611
00:23:57,390 --> 00:23:59,070
worker when a request is made the

612
00:23:59,070 --> 00:24:00,570
service worker isn't just a normal

613
00:24:00,570 --> 00:24:02,550
function if you can't just add it as a

614
00:24:02,550 --> 00:24:04,380
method in your app it has to be in a

615
00:24:04,380 --> 00:24:05,820
different JavaScript file and this

616
00:24:05,820 --> 00:24:07,260
details really important if you want to

617
00:24:07,260 --> 00:24:10,080
integrate it so one thing about this is

618
00:24:10,080 --> 00:24:12,059
that you're kind of throwing it over the

619
00:24:12,059 --> 00:24:14,190
wall it's almost like communicating with

620
00:24:14,190 --> 00:24:16,200
a server so you're throwing things at it

621
00:24:16,200 --> 00:24:18,390
and it's throwing things back so we're

622
00:24:18,390 --> 00:24:20,220
gonna pass in the type of event that was

623
00:24:20,220 --> 00:24:22,440
triggered so we can submit or queue what

624
00:24:22,440 --> 00:24:24,630
the user requested not shown here we'll

625
00:24:24,630 --> 00:24:27,690
do some error handling and then after

626
00:24:27,690 --> 00:24:29,460
we've activated and installed the

627
00:24:29,460 --> 00:24:30,870
Service Worker we're gonna strip out the

628
00:24:30,870 --> 00:24:32,880
type from the event tag and pass it into

629
00:24:32,880 --> 00:24:34,650
a function that will process a fetch

630
00:24:34,650 --> 00:24:37,440
request when it gets connectivity in

631
00:24:37,440 --> 00:24:39,750
that function will fetch our URL which

632
00:24:39,750 --> 00:24:41,760
is a placeholder server in this case and

633
00:24:41,760 --> 00:24:44,100
we'll check if the response has happened

634
00:24:44,100 --> 00:24:46,320
and once it has we'll send a message

635
00:24:46,320 --> 00:24:48,390
back to the client with post message so

636
00:24:48,390 --> 00:24:50,580
we'll send the we'll send the response

637
00:24:50,580 --> 00:24:51,840
and the type of resource that was

638
00:24:51,840 --> 00:24:54,870
requested and after that we can alert

639
00:24:54,870 --> 00:24:58,590
the user when it's finally sent but the

640
00:24:58,590 --> 00:25:00,420
most important part of the demo isn't

641
00:25:00,420 --> 00:25:01,980
the service workers it's where we

642
00:25:01,980 --> 00:25:04,440
started we started by asking questions I

643
00:25:04,440 --> 00:25:06,150
went to the firehouse and I asked them

644
00:25:06,150 --> 00:25:07,250
questions

645
00:25:07,250 --> 00:25:09,140
and there are a lot of times where I

646
00:25:09,140 --> 00:25:10,730
wasn't even sure if I could really help

647
00:25:10,730 --> 00:25:13,039
them but by asking questions about what

648
00:25:13,039 --> 00:25:15,530
happened in any kind of use case in any

649
00:25:15,530 --> 00:25:17,990
kind of scenario I can't I finally found

650
00:25:17,990 --> 00:25:19,340
something that they could use the help

651
00:25:19,340 --> 00:25:23,179
for we have to ask questions we have to

652
00:25:23,179 --> 00:25:24,830
ask questions in the beginning like the

653
00:25:24,830 --> 00:25:27,230
fire fire house demo and we have to ask

654
00:25:27,230 --> 00:25:29,150
questions like when it's deployed like

655
00:25:29,150 --> 00:25:31,370
the criminal scores we have to keep

656
00:25:31,370 --> 00:25:33,679
evaluating these systems and where

657
00:25:33,679 --> 00:25:35,179
they're going and make purposeful

658
00:25:35,179 --> 00:25:36,620
decisions to help people in

659
00:25:36,620 --> 00:25:39,890
course-correct when we don't I see a lot

660
00:25:39,890 --> 00:25:41,600
of tech that ask questions about how we

661
00:25:41,600 --> 00:25:42,559
can make our lives more comfortable

662
00:25:42,559 --> 00:25:45,380
which I appreciate I make apps on the

663
00:25:45,380 --> 00:25:47,390
weekends this is a cocktail finder I

664
00:25:47,390 --> 00:25:51,159
built that just finds me cocktails okay

665
00:25:51,159 --> 00:25:53,630
but it seems to me that a

666
00:25:53,630 --> 00:25:55,880
disproportionate amount of projects

667
00:25:55,880 --> 00:25:58,490
focus inward rather than outward focus

668
00:25:58,490 --> 00:26:00,350
on serving people with money instead of

669
00:26:00,350 --> 00:26:02,720
asking people if they need help there

670
00:26:02,720 --> 00:26:04,610
are a lot of opportunities out there to

671
00:26:04,610 --> 00:26:06,110
help and you might already have the

672
00:26:06,110 --> 00:26:08,450
skills to do so it just takes a little

673
00:26:08,450 --> 00:26:10,460
bit of imagination and exploration and

674
00:26:10,460 --> 00:26:13,669
it can really pay off so what I want you

675
00:26:13,669 --> 00:26:16,070
to think about going forward is this but

676
00:26:16,070 --> 00:26:17,539
I truly believe that you're some of the

677
00:26:17,539 --> 00:26:20,299
brightest minds of our generation as you

678
00:26:20,299 --> 00:26:21,950
move forward building applications for

679
00:26:21,950 --> 00:26:24,500
the webs the web you're building the

680
00:26:24,500 --> 00:26:26,480
pillars and blocks that our society

681
00:26:26,480 --> 00:26:28,820
rests on you're crafting the shape of

682
00:26:28,820 --> 00:26:30,230
our digital lives

683
00:26:30,230 --> 00:26:32,450
this could mean building new features

684
00:26:32,450 --> 00:26:33,740
this could mean advocating for

685
00:26:33,740 --> 00:26:35,000
accessibility and this could be as

686
00:26:35,000 --> 00:26:36,350
simple as raising your hand when you

687
00:26:36,350 --> 00:26:38,510
think something isn't right it's not

688
00:26:38,510 --> 00:26:40,669
just your job to create these things and

689
00:26:40,669 --> 00:26:42,860
do as you're told it's also your job to

690
00:26:42,860 --> 00:26:45,260
ask are we building things correctly are

691
00:26:45,260 --> 00:26:47,659
we building the right things and are we

692
00:26:47,659 --> 00:26:49,730
building them well for all of the

693
00:26:49,730 --> 00:26:56,760
digital lives that we touch thank you

