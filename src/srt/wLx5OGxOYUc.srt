1
00:00:01,640 --> 00:00:20,110
Thank you.

2
00:00:20,110 --> 00:00:23,800
So hello everyone, my name is Raul.

3
00:00:23,800 --> 00:00:28,210
I'm excited to talk to you today.

4
00:00:28,210 --> 00:00:34,550
It's my first JSConf 2014 and it is really
blowing my mind.

5
00:00:34,550 --> 00:00:40,370
So, before getting to topic, let me introduce
myself, I come from Spain, where I work as

6
00:00:40,370 --> 00:00:48,130
a freelance developer, and I'm also studying
masters degree in computer technologies focus

7
00:00:48,130 --> 00:00:51,139
on data compression, crip crip and steganography.

8
00:00:51,139 --> 00:00:53,269
\h‑‑ crypt technology autography.

9
00:00:53,269 --> 00:01:01,530
Let's start with data compression, first of
all, I'm not an expert on data compression,

10
00:01:01,530 --> 00:01:07,930
I am learning about it, and I will like to
share with you what I know really.

11
00:01:07,930 --> 00:01:13,390
So, for me, data compression is an amazing
topic.

12
00:01:13,390 --> 00:01:18,350
When I say amazing, it is really amazing,
I promise.

13
00:01:18,350 --> 00:01:24,740
It can be seen like magic, okay, data compression
algorithm take some data and reduce it's size

14
00:01:24,740 --> 00:01:26,760
without losing investigation.

15
00:01:26,760 --> 00:01:34,159
In some cases like logic compression it may
be interesting to lose some information that

16
00:01:34,159 --> 00:01:39,500
means once the compressed the result will
not be identical.

17
00:01:39,500 --> 00:01:47,689
For example in images, sounds, movies, they
are more suitable to of use logic compression.

18
00:01:47,689 --> 00:01:56,480
On the other hand text is usually compressed
using a lossless method like him HTML file\h‑‑

19
00:01:56,480 --> 00:02:04,920
it's not magic, we have tools to determine
how far we can go in come pressing data without

20
00:02:04,920 --> 00:02:12,170
loosing information or quantify much information
is in a given message.

21
00:02:12,170 --> 00:02:18,200
And this is with information theory introduce
by Claud etiane nonin 1948 in mathematical

22
00:02:18,200 --> 00:02:23,330
theory of communication\h‑‑ his famous
paper.

23
00:02:23,330 --> 00:02:31,410
So, how can we determine how much information
is contained in a message? Okay.

24
00:02:31,410 --> 00:02:36,190
That answer is Entropy.

25
00:02:36,190 --> 00:02:42,010
The Entropy, you can forget the formula for
now, the Entropy is the

26
00:02:42,010 --> 00:02:47,090
A A amount of information contained in each
message.

27
00:02:47,090 --> 00:02:52,470
In binary for example, we can concern Entropy
the number of bits to represent the message

28
00:02:52,470 --> 00:02:57,060
without losing information.

29
00:02:57,060 --> 00:03:03,580
The concept of Entropy is quite abstract,
and it's not always easy to understand.

30
00:03:03,580 --> 00:03:11,160
But, how a message can contain more information
than other? Okay, it doesn't make any sense.

31
00:03:11,160 --> 00:03:17,690
Imagine for example these two places, Berlin
and the Desert of Arizona.

32
00:03:17,690 --> 00:03:24,580
We would say it is raining in Berlin it contains
less information than if we say it is raining

33
00:03:24,580 --> 00:03:26,580
in the Desert of Arizona.

34
00:03:26,580 --> 00:03:30,409
That is because it is less common.

35
00:03:30,409 --> 00:03:38,129
In Berlin they have 225 days\h‑‑ rainy
days per year, 62\hpercent.

36
00:03:38,129 --> 00:03:43,769
While in the Desert of Arizona only 17 days,
which is six percent.

37
00:03:43,769 --> 00:03:52,629
So information theory Entropy is closely related
to probabilities, as you can see in the formula.

38
00:03:52,629 --> 00:04:01,269
And while this is mostly theoretical our brains
are already designed to come press data, we

39
00:04:01,269 --> 00:04:04,050
do it all the time.

40
00:04:04,050 --> 00:04:09,970
For example, we use multiconstruct to express
complex information like feelings with just

41
00:04:09,970 --> 00:04:15,640
a couple of characters we can communicate
that we're happy, we're sad, we are mocking

42
00:04:15,640 --> 00:04:16,640
you.

43
00:04:16,640 --> 00:04:22,770
We also have acronyms for common sentences,
we no longer say laugh out loud we just say

44
00:04:22,770 --> 00:04:26,560
LOL or ASAP for as soon as possible.

45
00:04:26,560 --> 00:04:30,950
Savoring time while speaking and saving space
when storing it.

46
00:04:30,950 --> 00:04:37,940
Even in real life we can share so much information
by facial eggs presentations, okay, like the

47
00:04:37,940 --> 00:04:44,270
face is telling us if he's safe, happy and
having a good time.

48
00:04:44,270 --> 00:04:52,200
Of these two examples we could consider a\h‑‑
facial eggs presentation as a form of logic

49
00:04:52,200 --> 00:04:53,970
compression.

50
00:04:53,970 --> 00:04:56,670
Information is being missed here.

51
00:04:56,670 --> 00:05:04,550
In the other hand acronyms, if they are unique
and well defined, okay, we can decode it uniquely

52
00:05:04,550 --> 00:05:08,000
would be a form of lossless compression.

53
00:05:08,000 --> 00:05:13,940
But, even before the information theory was
developed, data compression was already a

54
00:05:13,940 --> 00:05:17,970
key factor for assigning codes to communicate.

55
00:05:17,970 --> 00:05:24,880
Take the Morse code for example, to save time
and Bandwidth Mors code assigned shorter code

56
00:05:24,880 --> 00:05:26,890
to more frequent character ferreters.

57
00:05:26,890 --> 00:05:31,650
For example in the English language, the Morse
code character\h‑‑ the most used character

58
00:05:31,650 --> 00:05:40,440
is the letter E, so in Morse it is the shortest
code, it is just a dot.

59
00:05:40,440 --> 00:05:51,160
So, what about data compression in web sites?
Okay, this is the normal flow, okay, the browser

60
00:05:51,160 --> 00:05:59,530
sends AEC request to a server to get the file
for example index.HTML letting the server

61
00:05:59,530 --> 00:06:05,550
know that accept the zip content.

62
00:06:05,550 --> 00:06:15,160
The sever then reads the file, come presses
on the fly, usually, and sends it to the browser,

63
00:06:15,160 --> 00:06:22,640
okay, unless we con fight your the server
to\h‑‑ content, it does it on the fly.

64
00:06:22,640 --> 00:06:31,800
Finally, the browser deCOM presses the file
also on the fly, again and gives the reader

65
00:06:31,800 --> 00:06:32,800
content.

66
00:06:32,800 --> 00:06:38,770
We will see later having to compress and the
compress on the fly requires to have a compressional

67
00:06:38,770 --> 00:06:50,970
algorithm that ise is fast to come press and
fast to decompress in both direction.

68
00:06:50,970 --> 00:06:56,090
Let's see how GZIP works internally.

69
00:06:56,090 --> 00:07:09,040
GZIP is a file formate, it ise it is a file
format based and a compression algorithm,

70
00:07:09,040 --> 00:07:15,210
the actual compression takes place in the
detailed\h‑‑\h‑‑ it was designed by

71
00:07:15,210 --> 00:07:24,150
this man, Phil Katz for the PK zip archiving
tool and is used for HTTP, PNG and PDF files.

72
00:07:24,150 --> 00:07:25,150
Okay.

73
00:07:25,150 --> 00:07:36,650
Unfortunately the history of Phil Katz, he
was a genius, but he died when he was only

74
00:07:36,650 --> 00:07:41,950
37 due to complications related to chronic
alcoholism.

75
00:07:41,950 --> 00:07:50,330
So, deplait is a combination of two algorithms,
LZ 77 and Hufpmann coding.

76
00:07:50,330 --> 00:07:53,320
Let's see how they work.

77
00:07:53,320 --> 00:08:01,220
LZ 77 is a losse lossless data compression,
we get the original data once come pressed.

78
00:08:01,220 --> 00:08:09,130
Which is a compression by replacing repeated
occurrence of data with references, okay.

79
00:08:09,130 --> 00:08:17,660
So, in this example, of we see how the algorithm
tries to find repeated occurrences and for

80
00:08:17,660 --> 00:08:24,560
example files that space file, space file
spice, is spice appears twice.

81
00:08:24,560 --> 00:08:30,240
So, replaces the second occurrence with a
reference.

82
00:08:30,240 --> 00:08:36,120
The first number is the distance, okay, the
jump, the number of positions you need to

83
00:08:36,120 --> 00:08:42,090
go back to go to the other stream.

84
00:08:42,090 --> 00:08:45,390
And the second number is the length, the length
of the match.

85
00:08:45,390 --> 00:08:52,000
So the decoder can uniquely decode that information.

86
00:08:52,000 --> 00:08:59,270
To find matches, the algorithm keeps track
of recently read data, of 232 kilobyte, this

87
00:08:59,270 --> 00:09:02,080
is called the sliding window.

88
00:09:02,080 --> 00:09:07,970
We refer sometimes LZ 77 as a sliding Al gosh
rhythm.

89
00:09:07,970 --> 00:09:14,270
Once come pressed we have three times of data
in the compressed stream.

90
00:09:14,270 --> 00:09:21,320
We have literals, which would be the characters
that have not been replaced by references,

91
00:09:21,320 --> 00:09:28,960
we have lengths, the length of the match,
and distances, the jump.

92
00:09:28,960 --> 00:09:41,021
Okay, and in the second step, the algorithm
tries to use\h‑‑ bits for the zero, one

93
00:09:41,021 --> 00:09:46,020
approach would be to use the necessary bits
to represent the symbols.

94
00:09:46,020 --> 00:09:54,740
For example here we have the string hello
word, we will use ASCII, it uses 8 bits per

95
00:09:54,740 --> 00:10:01,740
character, we would need 88 bits to represent
that information, that message.

96
00:10:01,740 --> 00:10:10,060
As we are only using the characters that appear
in the table, they can be represented only

97
00:10:10,060 --> 00:10:19,710
with 3 bits, so we can make a transformation
and use only 3 bits to represent the message.

98
00:10:19,710 --> 00:10:25,780
So, we will of only 33 bits to represent that
information.

99
00:10:25,780 --> 00:10:29,370
But we can do it better.

100
00:10:29,370 --> 00:10:34,980
Instead of fix length codes we can use variable
length codes.

101
00:10:34,980 --> 00:10:42,640
So, if we\h‑‑ if we calculate the frequency,
the number of appearances of the characters,

102
00:10:42,640 --> 00:10:49,500
we can try to think shorter goals to more
frequent symbols like the Morse code does.

103
00:10:49,500 --> 00:10:54,630
So, in this example L is used three times.

104
00:10:54,630 --> 00:11:00,290
The letter O is just twice, and the others
only once.

105
00:11:00,290 --> 00:11:09,430
So, we could do this and only use 19 bits
instead of 33 to represent the message.

106
00:11:09,430 --> 00:11:13,870
Okay, but there is a problem.

107
00:11:13,870 --> 00:11:16,890
The come pressed stream is ambiguous.

108
00:11:16,890 --> 00:11:27,030
These four bits could be, for example, HE,
which is correct, A H which is 00 and E which

109
00:11:27,030 --> 00:11:36,130
is 01, but it could also be LHO, L 0, H 00
and of O 1.

110
00:11:36,130 --> 00:11:44,430
Also the O and many others, so we will need
to\h‑‑ our next thing is to separate that

111
00:11:44,430 --> 00:11:47,780
code using compression ratio.

112
00:11:47,780 --> 00:11:58,090
And here is when huffman coding comes, it
generates variable length codes, shorter codes

113
00:11:58,090 --> 00:12:04,800
to more frequent characters, but they have
the prefixed property, the prefixed property

114
00:12:04,800 --> 00:12:09,350
says that any of the codes is a prefix of
another.

115
00:12:09,350 --> 00:12:16,990
So, the output can be decodeble it is not
ambiguous.

116
00:12:16,990 --> 00:12:25,400
So using huffman coding it can be represented
with 32 bits, one bit less than you see in

117
00:12:25,400 --> 00:12:27,480
the previous code.

118
00:12:27,480 --> 00:12:31,340
And for larger messages gains could be even
better.

119
00:12:31,340 --> 00:12:44,460
So, Huffman code is used using the compressed
output we got from LC 77.

120
00:12:44,460 --> 00:12:50,320
One for literals and length and the second
for distances.

121
00:12:50,320 --> 00:13:00,610
So if we go out now to the GZIP file, not
the algorithm, the format file, it come presses

122
00:13:00,610 --> 00:13:05,290
an input data in blocks, okay, each block
is compressed separately.

123
00:13:05,290 --> 00:13:09,580
And there are three modes of compression.

124
00:13:09,580 --> 00:13:14,060
In mode one there is no compression at all.

125
00:13:14,060 --> 00:13:19,620
It is just useful for data that is already
come pressed or data that is totally random

126
00:13:19,620 --> 00:13:22,980
that cannot be come pressed.

127
00:13:22,980 --> 00:13:32,610
Mode two uses some already generated Huffman
codes using some statistical analysis which

128
00:13:32,610 --> 00:13:41,130
fits good enough for most data, and mode three
generate those code tables taking into account

129
00:13:41,130 --> 00:13:44,750
the data of the block.

130
00:13:44,750 --> 00:13:46,510
The mode one is the fastest.

131
00:13:46,510 --> 00:13:53,110
And mode three is the slowest, but usually
gets better compression ratios.

132
00:13:53,110 --> 00:14:00,800
Of of security, you're familiar with\h‑‑
you probably have use in the the past to use

133
00:14:00,800 --> 00:14:08,690
to split big files into several chunks the
size of floppy disks, the non‑compression

134
00:14:08,690 --> 00:14:11,450
mode to make these chunks.

135
00:14:11,450 --> 00:14:20,339
Okay, so, we just saw how this works you may
have noticed this there are different ways

136
00:14:20,339 --> 00:14:27,290
to generate the GZIP compliant file, just
splitting the file into several blocks with

137
00:14:27,290 --> 00:14:33,400
no compression, the no compression mode we
get GZIP compliant file, it is really fast

138
00:14:33,400 --> 00:14:36,480
but there is no compression at all.

139
00:14:36,480 --> 00:14:46,290
And there are a lot of\h‑‑ able to create
the zip files, each of them use their own

140
00:14:46,290 --> 00:14:49,750
optimizations, so the result will be different.

141
00:14:49,750 --> 00:14:57,930
Also, there are compression modes, for example
the high compression mode spends more time

142
00:14:57,930 --> 00:15:04,040
in LC 77 trying to find a longer match, okay.

143
00:15:04,040 --> 00:15:14,130
In general 7 zip gets better compression ratio
than GNU GZIP, for example Zopfli follows

144
00:15:14,130 --> 00:15:22,589
a different approach\h‑‑ this is from
Google, what it does is spend much more time,

145
00:15:22,589 --> 00:15:29,161
when I say much more time, it is much more
time, around a hundred times slower to get

146
00:15:29,161 --> 00:15:31,120
better compression ratios.

147
00:15:31,120 --> 00:15:33,830
Okay, around five percent.

148
00:15:33,830 --> 00:15:41,470
This is really useful if we want to come press,
for example our JavaScript files or CSS files

149
00:15:41,470 --> 00:15:45,160
before deploying, okay, in the deployment
process.

150
00:15:45,160 --> 00:15:52,130
And then serve those the zip files instead
of come pressing on the fly.

151
00:15:52,130 --> 00:15:59,930
As a general rule, more time, using more time,
you get better compression ratios, so you

152
00:15:59,930 --> 00:16:06,560
need the find a trade off between doing it
off line and up load the come pressed files

153
00:16:06,560 --> 00:16:12,680
or doing it on the fly.

154
00:16:12,680 --> 00:16:19,190
Why GZIP, it is the best compression method?
The answer is no.

155
00:16:19,190 --> 00:16:26,770
There are many compression algorithms with
better compression ratios than GZIP, so why

156
00:16:26,770 --> 00:16:35,790
are we still using GZIP? Most of the time
in computer science, there are trade offs.

157
00:16:35,790 --> 00:16:47,340
GZIP provides good enough compression ratio
between 2.5 and 3 for text and it is fast,

158
00:16:47,340 --> 00:16:52,790
it is fast to come press data and it is fast
to deCOM press it.

159
00:16:52,790 --> 00:16:59,170
As we usually can figure out come press responses
on the fly, this is something to take into

160
00:16:59,170 --> 00:17:02,510
account.

161
00:17:02,510 --> 00:17:08,079
In addition, even in the worse case, for example
if the data is already compressed, expands

162
00:17:08,079 --> 00:17:16,910
the data just a little bit by only five bites
by 32‑kilo bytes.

163
00:17:16,910 --> 00:17:22,640
We try to compress everything it is important
to be sure that damages will be relatively

164
00:17:22,640 --> 00:17:23,800
low.

165
00:17:23,800 --> 00:17:31,650
Also, the memory needed by\h‑‑ the decoder
is independent of the size of the data.

166
00:17:31,650 --> 00:17:40,360
And finally there are free implementation
that avoids\h‑‑ it is also difficult to

167
00:17:40,360 --> 00:17:44,179
newer compression methods to be widely used.

168
00:17:44,179 --> 00:17:54,670
For example the Chromium team tried a few
years ago for G.Zip 2 it provides better compression

169
00:17:54,670 --> 00:17:58,380
ratio and the results are fast.

170
00:17:58,380 --> 00:18:08,640
The problem was that even though the ACTP
were correct, and the contents were compressed

171
00:18:08,640 --> 00:18:13,551
be B zip 2 the approximateee some of the intermediary
proxies didn't understand it.

172
00:18:13,551 --> 00:18:21,980
\h‑‑ so it was corrupting if data.

173
00:18:21,980 --> 00:18:32,380
So, basically, in the short term, at least,
we are stuck in the GZIP.

174
00:18:32,380 --> 00:18:40,210
What can we do to try to get better compression
ratios? So, we can preprocess the data to

175
00:18:40,210 --> 00:18:41,770
try to optimize matches.

176
00:18:41,770 --> 00:18:45,820
Okay, take a look at this image.

177
00:18:45,820 --> 00:18:53,350
This image was generated by a tool of\h‑‑
here mall showing what parts are compressed

178
00:18:53,350 --> 00:19:01,310
file have been compressed better, those are
in blue, and what parts have been expanded.

179
00:19:01,310 --> 00:19:10,010
So, we can see the strings of characters in
upper case or sometimes the first character

180
00:19:10,010 --> 00:19:14,000
of capitalized words expand.

181
00:19:14,000 --> 00:19:21,080
Of this is spectral behavior as they are less
common and more difficult to find a match

182
00:19:21,080 --> 00:19:23,390
to replace with a reference.

183
00:19:23,390 --> 00:19:34,130
But preprocess this data to try to optimize
the compression and the answer is, yes, sometimes.

184
00:19:34,130 --> 00:19:42,120
The goal would be to have a functionality
able to transform the data in a way that once

185
00:19:42,120 --> 00:19:48,590
is the zip is smaller than the original data
is zipped without losing information.

186
00:19:48,590 --> 00:19:54,050
And, this depends highly on the type of data
we are using.

187
00:19:54,050 --> 00:20:03,270
For example there is a technique called transposing
JSON, as you know JSON object is made of keycal

188
00:20:03,270 --> 00:20:09,780
you pairs where the key portion is repeated
for each instance.

189
00:20:09,780 --> 00:20:16,810
The basic idea is to group together all the
values for each property, so, in the example,

190
00:20:16,810 --> 00:20:21,120
the name property will contain all the names
and the country property will contain all

191
00:20:21,120 --> 00:20:22,780
the countries.

192
00:20:22,780 --> 00:20:29,850
Obviously, we will need to change our client‑side
JavaScript to be able to read this.

193
00:20:29,850 --> 00:20:37,890
But, doing this, we have two main benefits,
the first one is that we reduce redene dun

194
00:20:37,890 --> 00:20:44,630
Dennis, we are making the files smaller at
the expense of spending more time or CPU cycles

195
00:20:44,630 --> 00:20:48,560
in the client to interpret that information.

196
00:20:48,560 --> 00:20:52,710
But, we are also grouped in similar data together.

197
00:20:52,710 --> 00:21:04,170
If you remember when we talked about LC 77
it has a sliding window of 33‑kilo bytes.

198
00:21:04,170 --> 00:21:11,360
We make sure that similar data, in this case
names or countries will be closed, so it is

199
00:21:11,360 --> 00:21:14,910
more likely to find matches between similar
data.

200
00:21:14,910 --> 00:21:20,660
It is important to note that not always we
will better compression ratio with this technique,

201
00:21:20,660 --> 00:21:24,850
so we must be careful.

202
00:21:24,850 --> 00:21:30,960
For XML and HTML files I have been working
on an automatic process in order to order

203
00:21:30,960 --> 00:21:37,290
the attributes in a way to maximize LC 77
matches.

204
00:21:37,290 --> 00:21:44,640
Here executing executing LC 77, only LC 77,
not the detail completely, over the first

205
00:21:44,640 --> 00:21:48,780
sample we get an improvement of the file size
of 17\hpercent.

206
00:21:48,780 --> 00:21:57,390
As attributes get more ordered, we see the
percentage is bigger, so, in general it will

207
00:21:57,390 --> 00:22:01,799
follow up to the other attributes, we'll get
slightly better results.

208
00:22:01,799 --> 00:22:13,350
The process is not as simple as also takes
into account the value of the attribute, but,

209
00:22:13,350 --> 00:22:21,760
just having a kind of standard to organize
your attributes you will get better results.

210
00:22:21,760 --> 00:22:28,720
You can see there's a link at the bottom with
a small paper on it.

211
00:22:28,720 --> 00:22:37,309
So, finally, if you have interest on data
compression and GZIP, okay, some recommended

212
00:22:37,309 --> 00:22:43,790
material, of compression is a series of videos
of data compression, okay.

213
00:22:43,790 --> 00:22:53,410
They are done by Colt McAnlis, which makes
it easy to understand and funny.

214
00:22:53,410 --> 00:22:59,660
So, want to thank him for reviewing this presentation.

215
00:22:59,660 --> 00:23:01,640
This is my favorite book on the topic.

216
00:23:01,640 --> 00:23:04,890
It is data compression, the complete reference.

217
00:23:04,890 --> 00:23:12,030
I am not sure if it's the best idea for a
beginner to start with this, but if you're

218
00:23:12,030 --> 00:23:20,760
curious to know how a given algorithm\h‑‑
the answer is here, the book is quite expensive

219
00:23:20,760 --> 00:23:23,570
so ...

220
00:23:23,570 --> 00:23:28,670
so if you're not going to read it, don't buy
it.

221
00:23:28,670 --> 00:23:36,880
And if you like, reading papers I recommend
to read this one from Jay cob Zif and abbra

222
00:23:36,880 --> 00:23:42,180
ham Lempel where they talk about LC 77.

223
00:23:42,180 --> 00:23:51,930
And finally the paper written by David Hutchman
on the construction of minimum redundancy

224
00:23:51,930 --> 00:23:59,170
codes, it's a very short paper around 12 pages.

225
00:23:59,170 --> 00:24:03,540
But the impact on data compression and other
fields has been huge.

