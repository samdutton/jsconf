1
00:00:00,760 --> 00:00:05,810

[Music]

2
00:00:05,810 --> 00:00:09,150
my name is Nikhil so we're here to

3
00:00:09,150 --> 00:00:10,800
talked about machine learning and

4
00:00:10,800 --> 00:00:13,830
JavaScript with tensorflow J's so if

5
00:00:13,830 --> 00:00:15,660
you're familiar at all with machine

6
00:00:15,660 --> 00:00:17,369
learning and data science you'll know

7
00:00:17,369 --> 00:00:19,380
that most of it happens in Python so

8
00:00:19,380 --> 00:00:21,330
this is a poll that was given by a

9
00:00:21,330 --> 00:00:23,250
popular data science blog called Katy

10
00:00:23,250 --> 00:00:25,680
nuggets and you can see that Python

11
00:00:25,680 --> 00:00:28,230
absolutely dominates and this is for a

12
00:00:28,230 --> 00:00:29,760
pretty good reason you know there has

13
00:00:29,760 --> 00:00:31,830
been many many years of tooling built in

14
00:00:31,830 --> 00:00:34,469
Python for data science from numpy to

15
00:00:34,469 --> 00:00:36,710
pandas to scikit-learn to tensorflow

16
00:00:36,710 --> 00:00:39,360
and this field is gonna continue to

17
00:00:39,360 --> 00:00:42,149
evolve in python and i don't have to

18
00:00:42,149 --> 00:00:43,550
convince the folks in this room

19
00:00:43,550 --> 00:00:45,600
javascript is a very very popular

20
00:00:45,600 --> 00:00:47,430
language this is obviously the October's

21
00:00:47,430 --> 00:00:50,160
survey by github and you can see that

22
00:00:50,160 --> 00:00:53,280
JavaScript absolutely dominates so we

23
00:00:53,280 --> 00:00:54,780
think that there's actually a lot of the

24
00:00:54,780 --> 00:00:56,160
JavaScript community and the folks in

25
00:00:56,160 --> 00:00:58,520
this room can bring to machine learning

26
00:00:58,520 --> 00:01:01,859
so who here has seen this this is

27
00:01:01,859 --> 00:01:05,070
tensorflow playground okay a few people

28
00:01:05,070 --> 00:01:06,840
but not a lot of people so I definitely

29
00:01:06,840 --> 00:01:09,540
recommend checking this link out this is

30
00:01:09,540 --> 00:01:11,610
an in-browser visualization of a neural

31
00:01:11,610 --> 00:01:13,560
network it was built by one of our

32
00:01:13,560 --> 00:01:15,840
colleagues at Google and the idea here

33
00:01:15,840 --> 00:01:18,000
is you can immediately change the number

34
00:01:18,000 --> 00:01:20,369
of layers neurons learning rate and so

35
00:01:20,369 --> 00:01:22,530
forth and immediately see how the neural

36
00:01:22,530 --> 00:01:25,320
network generalizes over a data set now

37
00:01:25,320 --> 00:01:27,210
this thing was a huge educational

38
00:01:27,210 --> 00:01:28,920
success you know it's being used in

39
00:01:28,920 --> 00:01:31,140
universities across the world now and in

40
00:01:31,140 --> 00:01:32,549
Google's own machine learning crash

41
00:01:32,549 --> 00:01:34,320
course so again I definitely recommend

42
00:01:34,320 --> 00:01:37,049
checking this out so we step back and we

43
00:01:37,049 --> 00:01:38,970
kind of asked ourselves you know why was

44
00:01:38,970 --> 00:01:40,979
this such a success like why do people

45
00:01:40,979 --> 00:01:42,390
care about this in browser machine

46
00:01:42,390 --> 00:01:44,280
learning thing at all and we kind of

47
00:01:44,280 --> 00:01:46,110
distilled it down into a few points so

48
00:01:46,110 --> 00:01:48,240
the obvious thing is that you just click

49
00:01:48,240 --> 00:01:50,640
a link and you get going if you've done

50
00:01:50,640 --> 00:01:52,979
any Python ml it's like really pain in

51
00:01:52,979 --> 00:01:55,380
the butt to get your drivers installed

52
00:01:55,380 --> 00:01:57,149
Python libraries installing that kind of

53
00:01:57,149 --> 00:01:59,820
thing it's super interactive you know

54
00:01:59,820 --> 00:02:00,840
you have buttons you have hyper

55
00:02:00,840 --> 00:02:02,100
parameters you can play you can

56
00:02:02,100 --> 00:02:04,049
immediately see how the changing the

57
00:02:04,049 --> 00:02:06,860
knobs affect the the the learning

58
00:02:06,860 --> 00:02:09,840
configuration of the model we didn't

59
00:02:09,840 --> 00:02:11,099
take advantage of this in the playground

60
00:02:11,099 --> 00:02:12,780
but you know in the JavaScript world in

61
00:02:12,780 --> 00:02:13,680
the browser you have

62
00:02:13,680 --> 00:02:15,420
cameras and microphones and standardized

63
00:02:15,420 --> 00:02:16,920
access to these things which you don't

64
00:02:16,920 --> 00:02:19,099
really get as much in the Python lab and

65
00:02:19,099 --> 00:02:21,450
super important to us is that you can

66
00:02:21,450 --> 00:02:23,129
actually make predictions locally and

67
00:02:23,129 --> 00:02:25,170
data can stay on the client so this is

68
00:02:25,170 --> 00:02:28,950
privacy-preserving so we took this and

69
00:02:28,950 --> 00:02:31,170
we launched a library called tensorflow

70
00:02:31,170 --> 00:02:34,920
JS we released it last year in March its

71
00:02:34,920 --> 00:02:38,040
GPU accelerated so we use WebGL to make

72
00:02:38,040 --> 00:02:39,480
everything fast so we actually do all

73
00:02:39,480 --> 00:02:41,790
the the linear algebra in fragment

74
00:02:41,790 --> 00:02:43,739
shaders and I'll talk a little bit about

75
00:02:43,739 --> 00:02:46,079
that in a second but one of the things

76
00:02:46,079 --> 00:02:47,430
that we do is we let you make

77
00:02:47,430 --> 00:02:48,659
predictions through machine learning

78
00:02:48,659 --> 00:02:50,549
models but we also let you train them

79
00:02:50,549 --> 00:02:52,500
directly in the browser or even in no

80
00:02:52,500 --> 00:02:54,180
Jess and we'll talk about about that as

81
00:02:54,180 --> 00:02:57,209
well so when designing the library you

82
00:02:57,209 --> 00:02:59,519
know we had a couple goals in mind one

83
00:02:59,519 --> 00:03:01,469
is that we wanted to empower a diverse

84
00:03:01,469 --> 00:03:03,060
group of Java developers like the folks

85
00:03:03,060 --> 00:03:04,439
in this room you know there's a lot of

86
00:03:04,439 --> 00:03:05,849
really awesome people in this community

87
00:03:05,849 --> 00:03:08,040
and we want to sort of marry these two

88
00:03:08,040 --> 00:03:10,530
worlds at the same time we wanted the

89
00:03:10,530 --> 00:03:12,329
folks who are experienced in the machine

90
00:03:12,329 --> 00:03:14,219
learning to be able to port their work

91
00:03:14,219 --> 00:03:16,769
to the web now these goals are kind of

92
00:03:16,769 --> 00:03:18,510
sometimes at conflict so we'll talk a

93
00:03:18,510 --> 00:03:22,010
little bit about how we resolve those

94
00:03:22,010 --> 00:03:23,970
okay so one of the principles that we

95
00:03:23,970 --> 00:03:25,620
had was we wanted the library to be

96
00:03:25,620 --> 00:03:28,199
super easy to use and we kind of leaned

97
00:03:28,199 --> 00:03:30,150
towards that over performance at the

98
00:03:30,150 --> 00:03:31,290
same time we didn't want to sacrifice

99
00:03:31,290 --> 00:03:34,409
any functionality for simplicity so just

100
00:03:34,409 --> 00:03:37,590
jumping into what that means we decided

101
00:03:37,590 --> 00:03:39,720
to go with this eager only approach I'm

102
00:03:39,720 --> 00:03:41,459
not going to go into what that means but

103
00:03:41,459 --> 00:03:42,930
it's a much simpler way of programming

104
00:03:42,930 --> 00:03:44,400
and actually most of the machine

105
00:03:44,400 --> 00:03:45,780
learning world is moving towards this

106
00:03:45,780 --> 00:03:48,239
eager approach versus a graph based

107
00:03:48,239 --> 00:03:49,379
approach where you actually stitch

108
00:03:49,379 --> 00:03:51,900
together a computation graph and execute

109
00:03:51,900 --> 00:03:53,819
it later and we've really wanted to be

110
00:03:53,819 --> 00:03:56,579
easy so we move towards eager we also

111
00:03:56,579 --> 00:03:59,459
provide a high level layers API which is

112
00:03:59,459 --> 00:04:01,379
a set of best practices in the machine

113
00:04:01,379 --> 00:04:02,849
learning community so you don't have to

114
00:04:02,849 --> 00:04:04,199
think about all the details of your

115
00:04:04,199 --> 00:04:05,489
linear algebra when you're constructing

116
00:04:05,489 --> 00:04:08,909
a model and we also provide a whole

117
00:04:08,909 --> 00:04:10,859
repository of pre trained models that

118
00:04:10,859 --> 00:04:12,540
require zero understanding of machine

119
00:04:12,540 --> 00:04:14,250
learning to get started and I'm going to

120
00:04:14,250 --> 00:04:16,169
show you a couple examples of those in a

121
00:04:16,169 --> 00:04:19,229
second and I want to highlight this we

122
00:04:19,229 --> 00:04:20,820
focus on performance when and where it

123
00:04:20,820 --> 00:04:22,529
matters obviously we want matrix

124
00:04:22,529 --> 00:04:24,990
multiplies to be fast what we did was we

125
00:04:24,990 --> 00:04:27,180
took individual models and we figured

126
00:04:27,180 --> 00:04:27,330
out

127
00:04:27,330 --> 00:04:28,919
how to make those individual models

128
00:04:28,919 --> 00:04:33,000
faster on a use case basis so as I said

129
00:04:33,000 --> 00:04:34,560
we don't want to actually make the

130
00:04:34,560 --> 00:04:37,050
library less functional so we support

131
00:04:37,050 --> 00:04:39,150
gradients this is fancy talk for

132
00:04:39,150 --> 00:04:42,120
sensitivity of each of the weights which

133
00:04:42,120 --> 00:04:44,009
means we can train through any operation

134
00:04:44,009 --> 00:04:45,870
that you use in the tensor flow chest

135
00:04:45,870 --> 00:04:48,000
library support a lot of the tensor flow

136
00:04:48,000 --> 00:04:50,250
ops 130 of them and for any of these

137
00:04:50,250 --> 00:04:52,020
models that we're about to show you you

138
00:04:52,020 --> 00:04:54,389
can actually dig down and get some of

139
00:04:54,389 --> 00:04:55,860
the machine learning constructs out of

140
00:04:55,860 --> 00:05:00,629
them if you want to okay so quickly

141
00:05:00,629 --> 00:05:02,490
jumping into what the technical stack

142
00:05:02,490 --> 00:05:04,740
looks like at the very top of the

143
00:05:04,740 --> 00:05:06,539
abstraction api's we have our models

144
00:05:06,539 --> 00:05:08,430
repo so this is set of pre trained

145
00:05:08,430 --> 00:05:09,599
models I'll show you a couple of those

146
00:05:09,599 --> 00:05:10,949
in a second require very little

147
00:05:10,949 --> 00:05:13,080
understanding of machine learning below

148
00:05:13,080 --> 00:05:15,599
that this is our layers API this is

149
00:05:15,599 --> 00:05:17,400
where you can construct a model you can

150
00:05:17,400 --> 00:05:19,409
train the model you can serialize the

151
00:05:19,409 --> 00:05:21,479
model for later and we'll show you some

152
00:05:21,479 --> 00:05:23,969
of that soon too and we also have below

153
00:05:23,969 --> 00:05:25,650
that our core API which is or just

154
00:05:25,650 --> 00:05:26,969
linear algebra kernels so these are

155
00:05:26,969 --> 00:05:29,819
matrix multiplies convolutions and their

156
00:05:29,819 --> 00:05:33,150
gradients which are derivatives so all

157
00:05:33,150 --> 00:05:35,370
of these api is you can sort of poke in

158
00:05:35,370 --> 00:05:38,099
at any of these abstraction layers all

159
00:05:38,099 --> 00:05:39,690
of these sit on top of WebGL in the

160
00:05:39,690 --> 00:05:42,270
browser we use fragment shaders to run

161
00:05:42,270 --> 00:05:44,789
all of our math in parallel and in node

162
00:05:44,789 --> 00:05:47,339
we actually bind with the N API to

163
00:05:47,339 --> 00:05:49,979
tensorflow C++ and what that means is if

164
00:05:49,979 --> 00:05:52,379
you use that same API for any of these

165
00:05:52,379 --> 00:05:54,389
things you immediately get the hardware

166
00:05:54,389 --> 00:05:56,250
acceleration that tensorflow has been

167
00:05:56,250 --> 00:05:58,830
working hard on for the CPU and for GPUs

168
00:05:58,830 --> 00:06:00,960
with CUDA and eventually we're gonna

169
00:06:00,960 --> 00:06:04,229
have TPU support it was also very

170
00:06:04,229 --> 00:06:06,539
important for us not to silo ourselves

171
00:06:06,539 --> 00:06:08,370
in the JavaScript world there is a whole

172
00:06:08,370 --> 00:06:10,289
wealth of models that are trained in the

173
00:06:10,289 --> 00:06:12,120
Python ecosystem that we want to take

174
00:06:12,120 --> 00:06:13,770
advantage of so we have converter tools

175
00:06:13,770 --> 00:06:15,810
that let you take a Karass model or a

176
00:06:15,810 --> 00:06:17,789
tensorflow Saves model and bring them

177
00:06:17,789 --> 00:06:20,930
back into the JavaScript world

178
00:06:20,930 --> 00:06:23,669
ok so let's quickly take a look at what

179
00:06:23,669 --> 00:06:26,819
the models repo looks like so if you

180
00:06:26,819 --> 00:06:29,159
check out this link these are all our

181
00:06:29,159 --> 00:06:31,889
pre trained models these are hosted on

182
00:06:31,889 --> 00:06:34,289
github and on NPM so we host all the

183
00:06:34,289 --> 00:06:35,759
weights and all the JavaScript for you

184
00:06:35,759 --> 00:06:37,979
and we have a wealth of models from

185
00:06:37,979 --> 00:06:40,060
object recognition to

186
00:06:40,060 --> 00:06:42,520
human pose detection - localization -

187
00:06:42,520 --> 00:06:45,160
segmentation - text classification and

188
00:06:45,160 --> 00:06:47,110
the list goes on just go check this link

189
00:06:47,110 --> 00:06:49,360
out but I want to show you one of the

190
00:06:49,360 --> 00:06:51,330
demos because it's fun

191
00:06:51,330 --> 00:07:01,110
so okay so this model is called pose net

192
00:07:01,110 --> 00:07:03,669
it's running completely in the browser

193
00:07:03,669 --> 00:07:05,380
nothing is being sent back to a server

194
00:07:05,380 --> 00:07:08,020
and the idea here is we take RGB images

195
00:07:08,020 --> 00:07:10,090
from webcam we pass it through this pose

196
00:07:10,090 --> 00:07:12,220
detection model that generates key

197
00:07:12,220 --> 00:07:14,350
points for each of you know sort of my

198
00:07:14,350 --> 00:07:16,960
body parts and then it returns an object

199
00:07:16,960 --> 00:07:18,040
that we can just render on the screen

200
00:07:18,040 --> 00:07:19,600
and obviously it works with two people

201
00:07:19,600 --> 00:07:21,310
so this is a lot of fun and we'll show

202
00:07:21,310 --> 00:07:23,530
you how to use one of these models in a

203
00:07:23,530 --> 00:07:27,100
minute the second model is very similar

204
00:07:27,100 --> 00:07:29,080
to pose net it's doing person

205
00:07:29,080 --> 00:07:31,360
segmentation so it's you know this

206
00:07:31,360 --> 00:07:33,100
backgrounds a little funny but basically

207
00:07:33,100 --> 00:07:36,490
what it does is it draws a mask of a one

208
00:07:36,490 --> 00:07:38,110
where it thinks is a human pose and a

209
00:07:38,110 --> 00:07:39,430
zero where it does where it thinks

210
00:07:39,430 --> 00:07:41,380
there's not so this is what this one's a

211
00:07:41,380 --> 00:07:42,639
lot of fun and I don't know if this is

212
00:07:42,639 --> 00:07:44,440
gonna show well here but one of the

213
00:07:44,440 --> 00:07:46,150
effects I really like is the portrait

214
00:07:46,150 --> 00:07:48,729
mode you can see this thing blurring so

215
00:07:48,729 --> 00:07:50,430
we have you know a software-based

216
00:07:50,430 --> 00:07:52,479
portrait mode that's running directly in

217
00:07:52,479 --> 00:07:55,630
the browser pretty fast okay so let's go

218
00:07:55,630 --> 00:07:56,950
back to the slides and I'm going to show

219
00:07:56,950 --> 00:07:58,539
you how that actually works and what the

220
00:07:58,539 --> 00:08:00,760
code looks like so that model is called

221
00:08:00,760 --> 00:08:03,310
body pics it's a pre trained person

222
00:08:03,310 --> 00:08:04,810
segmentation model that we've done a lot

223
00:08:04,810 --> 00:08:08,169
of work to make super fast pretty

224
00:08:08,169 --> 00:08:09,580
straightforward you import tensorflow

225
00:08:09,580 --> 00:08:11,919
jeaious and body pics to libraries we

226
00:08:11,919 --> 00:08:13,570
have them on NPM we host them on CDN for

227
00:08:13,570 --> 00:08:16,990
you we you know have a regular image tag

228
00:08:16,990 --> 00:08:19,900
that's it and this image is Frank now

229
00:08:19,900 --> 00:08:23,050
Frank is Knicks baby and he is doing a

230
00:08:23,050 --> 00:08:24,850
yoga pose for us so we're gonna try to

231
00:08:24,850 --> 00:08:26,820
figure out where Frank is in this image

232
00:08:26,820 --> 00:08:29,770
so first we just load the model we call

233
00:08:29,770 --> 00:08:31,539
a weight body picks out load and this is

234
00:08:31,539 --> 00:08:33,760
gonna download all of our weights these

235
00:08:33,760 --> 00:08:36,400
weights we host on our GCP buckets for

236
00:08:36,400 --> 00:08:37,510
you so you don't have to pay for any of

237
00:08:37,510 --> 00:08:40,450
that and then you just call one line of

238
00:08:40,450 --> 00:08:42,070
code SMA person segmentation on the

239
00:08:42,070 --> 00:08:44,110
image and you get a JSON object out and

240
00:08:44,110 --> 00:08:46,209
inside of that JSON object is a binary

241
00:08:46,209 --> 00:08:48,400
mask of where it thinks the kid is that

242
00:08:48,400 --> 00:08:49,810
is that simple you don't really have to

243
00:08:49,810 --> 00:08:52,480
understand the end all bits of this one

244
00:08:52,480 --> 00:08:53,610
of the other things is modeled

245
00:08:53,610 --> 00:08:56,610
you is parts as well so it'll tell you

246
00:08:56,610 --> 00:08:58,320
which pixels our face which pixels our

247
00:08:58,320 --> 00:09:00,600
arms and legs and so forth and we

248
00:09:00,600 --> 00:09:02,970
provide some fun utilities for drawing

249
00:09:02,970 --> 00:09:06,180
masks on top of those so you can imagine

250
00:09:06,180 --> 00:09:07,740
this being used for like a video game

251
00:09:07,740 --> 00:09:09,360
sprite you just jump around on screen

252
00:09:09,360 --> 00:09:11,130
and you immediately have a fun video

253
00:09:11,130 --> 00:09:15,300
game sprite okay so I don't have to

254
00:09:15,300 --> 00:09:16,649
explain this to people in the room but

255
00:09:16,649 --> 00:09:18,360
JavaScript runs in a ton of tunnel

256
00:09:18,360 --> 00:09:20,519
places and we're working hard to get

257
00:09:20,519 --> 00:09:22,079
tensorflow J's working in those places

258
00:09:22,079 --> 00:09:24,390
so we have the browser a node obviously

259
00:09:24,390 --> 00:09:26,730
but working on electron and react native

260
00:09:26,730 --> 00:09:28,350
and WeChat so we'll talk about those in

261
00:09:28,350 --> 00:09:30,120
a second but I want to show you some of

262
00:09:30,120 --> 00:09:31,950
the cool examples that we we like in

263
00:09:31,950 --> 00:09:34,529
these worlds so in the browser side

264
00:09:34,529 --> 00:09:37,019
hopefully the links are here we have a

265
00:09:37,019 --> 00:09:38,279
project called create ability this is

266
00:09:38,279 --> 00:09:41,279
one that's done by Google and it's a set

267
00:09:41,279 --> 00:09:43,260
of experiments around can we make

268
00:09:43,260 --> 00:09:46,170
interacting with music and art more

269
00:09:46,170 --> 00:09:48,240
accessible so we're using that pose net

270
00:09:48,240 --> 00:09:49,709
model that I showed you and we're

271
00:09:49,709 --> 00:09:51,420
actually able to play a synth with just

272
00:09:51,420 --> 00:09:53,100
our face this runs completely in the

273
00:09:53,100 --> 00:09:54,690
browser the link is there go try this

274
00:09:54,690 --> 00:09:56,660
after the talk please

275
00:09:56,660 --> 00:09:58,920
cool so then we also have a project

276
00:09:58,920 --> 00:10:02,100
called uber manifold not by us by uber

277
00:10:02,100 --> 00:10:05,820
and this project is a way to debug and

278
00:10:05,820 --> 00:10:07,470
understand machine learning models as

279
00:10:07,470 --> 00:10:08,880
their training and I actually use

280
00:10:08,880 --> 00:10:10,860
tensive ojs just for linear algebra so

281
00:10:10,860 --> 00:10:12,449
fast matrix multiplications in the

282
00:10:12,449 --> 00:10:15,750
browser Airbnb is also using tensorflow

283
00:10:15,750 --> 00:10:18,449
J's they ship a little model to the to

284
00:10:18,449 --> 00:10:20,339
the client so when you're about to

285
00:10:20,339 --> 00:10:22,410
upload a profile picture if they see a

286
00:10:22,410 --> 00:10:24,300
license or a government-issued passport

287
00:10:24,300 --> 00:10:26,459
in that in that photo they'll yell at

288
00:10:26,459 --> 00:10:27,779
you before they upload to their server

289
00:10:27,779 --> 00:10:29,490
so they don't have to own that PII on

290
00:10:29,490 --> 00:10:33,390
the backend on the on the desktop and

291
00:10:33,390 --> 00:10:34,709
node there's a project called clinic

292
00:10:34,709 --> 00:10:36,690
doctor and clinic doctor is a project

293
00:10:36,690 --> 00:10:39,449
that monitors your node application for

294
00:10:39,449 --> 00:10:41,910
CPU spikes and they use tensorflow dress

295
00:10:41,910 --> 00:10:43,920
actually to disambiguate garbage

296
00:10:43,920 --> 00:10:46,620
collection spikes from your CPU in your

297
00:10:46,620 --> 00:10:49,079
in your actual program one of my

298
00:10:49,079 --> 00:10:50,760
personal favorites is a project called

299
00:10:50,760 --> 00:10:52,949
magenta studio magenta is a team at

300
00:10:52,949 --> 00:10:54,690
Google that does generative music and

301
00:10:54,690 --> 00:10:56,459
art and they actually have an electron

302
00:10:56,459 --> 00:10:57,930
app that plugs directly into Ableton

303
00:10:57,930 --> 00:11:00,660
Live and it can generate MIDI notes on a

304
00:11:00,660 --> 00:11:03,120
track for you or it can generate a drum

305
00:11:03,120 --> 00:11:05,010
beat alongside maybe a guitar groove

306
00:11:05,010 --> 00:11:07,440
that you have so this is a ton of

307
00:11:07,440 --> 00:11:09,360
on augments and existing work fro our

308
00:11:09,360 --> 00:11:10,800
workflow and you know JavaScript is

309
00:11:10,800 --> 00:11:14,280
awesome so of course we do it there okay

310
00:11:14,280 --> 00:11:16,640
so this other platform called WeChat is

311
00:11:16,640 --> 00:11:18,780
massive in China if people don't know

312
00:11:18,780 --> 00:11:20,850
about it it's got a billion users lots

313
00:11:20,850 --> 00:11:22,760
of mini programs lots of developers and

314
00:11:22,760 --> 00:11:25,230
they all run a Java Script and we're

315
00:11:25,230 --> 00:11:26,760
working hard to get GPU acceleration

316
00:11:26,760 --> 00:11:29,430
stories working inside of that with that

317
00:11:29,430 --> 00:11:30,690
I'm gonna hand it off to Nick to talk

318
00:11:30,690 --> 00:11:32,030
about some other stuff

319
00:11:32,030 --> 00:11:37,280
thanks aquel I speak kind of highlighted

320
00:11:37,280 --> 00:11:40,380
JavaScript runs in a lot of places and

321
00:11:40,380 --> 00:11:42,120
we're starting to think of areas where

322
00:11:42,120 --> 00:11:44,430
we can keep expanding where you can run

323
00:11:44,430 --> 00:11:48,090
tensorflow j/s i want to step back and

324
00:11:48,090 --> 00:11:49,530
talk about our node bindings first

325
00:11:49,530 --> 00:11:52,560
before we dive into the next topic we

326
00:11:52,560 --> 00:11:56,610
launched these about a year ago and the

327
00:11:56,610 --> 00:11:58,380
library is great because it's super fast

328
00:11:58,380 --> 00:12:00,450
it uses that C library like Nickell

329
00:12:00,450 --> 00:12:03,660
mentioned and it's great for deploying

330
00:12:03,660 --> 00:12:05,880
on the servers or doing local workflows

331
00:12:05,880 --> 00:12:08,760
on your desktop or workstation but there

332
00:12:08,760 --> 00:12:10,170
are a few downsides this particular

333
00:12:10,170 --> 00:12:12,990
library we have one of them is the GPU

334
00:12:12,990 --> 00:12:15,030
acceleration requires nvidia cuda

335
00:12:15,030 --> 00:12:17,550
library it's a really fast library but

336
00:12:17,550 --> 00:12:20,550
it's very large and we at tensorflow

337
00:12:20,550 --> 00:12:22,590
don't currently support mac OS so

338
00:12:22,590 --> 00:12:25,230
there's no GPU acceleration on math and

339
00:12:25,230 --> 00:12:26,760
the other thing is the node package

340
00:12:26,760 --> 00:12:29,190
itself is a native module all built on

341
00:12:29,190 --> 00:12:32,100
an API and it links to the C library of

342
00:12:32,100 --> 00:12:33,690
tensorflow that can be really large

343
00:12:33,690 --> 00:12:35,730
depending on which library you're using

344
00:12:35,730 --> 00:12:38,580
CUDA can be around 250 megabytes or so

345
00:12:38,580 --> 00:12:40,800
just on Linux so it's a very large

346
00:12:40,800 --> 00:12:44,940
package to ship so we started to think

347
00:12:44,940 --> 00:12:46,320
is there something in between we can do

348
00:12:46,320 --> 00:12:49,680
on node and we started working really

349
00:12:49,680 --> 00:12:52,710
hard and launched earlier this year a

350
00:12:52,710 --> 00:12:55,230
new headless graphic stack for node and

351
00:12:55,230 --> 00:12:57,510
we launched it it's called the node - -

352
00:12:57,510 --> 00:12:59,550
yes package we worked hard with the

353
00:12:59,550 --> 00:13:02,070
chrome team here to build a headless

354
00:13:02,070 --> 00:13:04,410
graphic stack for that and we wanted to

355
00:13:04,410 --> 00:13:06,300
take that and accelerate our existing

356
00:13:06,300 --> 00:13:08,990
WebGL stack all headless and node and

357
00:13:08,990 --> 00:13:12,030
this library runs by angle and angle is

358
00:13:12,030 --> 00:13:13,800
the driver we ship in chrome today and

359
00:13:13,800 --> 00:13:16,410
it translates WebGL calls to your native

360
00:13:16,410 --> 00:13:18,750
system graphics stack so in Windows

361
00:13:18,750 --> 00:13:20,850
that's direct3d

362
00:13:20,850 --> 00:13:23,309
OpenGL on windows and in your native Mac

363
00:13:23,309 --> 00:13:26,279
OS graphic stack implementation so we

364
00:13:26,279 --> 00:13:27,569
think this is gonna be great for some

365
00:13:27,569 --> 00:13:30,149
desktop apps like electron mobile and

366
00:13:30,149 --> 00:13:33,839
embedded space and in devices plus this

367
00:13:33,839 --> 00:13:35,519
is going to bring GPU acceleration to

368
00:13:35,519 --> 00:13:38,279
Mac OS we're working hard to finish this

369
00:13:38,279 --> 00:13:40,139
up a couple things so we're hoping the

370
00:13:40,139 --> 00:13:41,789
launch here later in June or sometime

371
00:13:41,789 --> 00:13:46,470
this summer and I wanted to show a demo

372
00:13:46,470 --> 00:13:49,589
of this actually running we've built a

373
00:13:49,589 --> 00:13:53,639
really quick electron app so if I go

374
00:13:53,639 --> 00:13:56,939
ahead and just run my app this app uses

375
00:13:56,939 --> 00:13:58,169
mobile net which is one of our

376
00:13:58,169 --> 00:14:00,359
out-of-the-box models that does basic

377
00:14:00,359 --> 00:14:01,949
image classification so you can see an

378
00:14:01,949 --> 00:14:04,949
image and tell you what it is so as I

379
00:14:04,949 --> 00:14:10,079
pull up in my app here lo not the most

380
00:14:10,079 --> 00:14:13,470
exciting UI but we it shows the GL stack

381
00:14:13,470 --> 00:14:14,609
that's running you can see it's running

382
00:14:14,609 --> 00:14:17,669
angle with an open Geo for one core and

383
00:14:17,669 --> 00:14:20,339
the latest OpenGL ES stack through angle

384
00:14:20,339 --> 00:14:22,859
and when I click run demo what's

385
00:14:22,859 --> 00:14:24,149
happening it's going out it's fetching

386
00:14:24,149 --> 00:14:26,819
our model it's loading it and it

387
00:14:26,819 --> 00:14:28,529
predicted that that's a Labrador

388
00:14:28,529 --> 00:14:30,839
Retriever and we're running about 150

389
00:14:30,839 --> 00:14:33,539
milliseconds or I'm sorry running 150

390
00:14:33,539 --> 00:14:35,249
predictions on an image and our

391
00:14:35,249 --> 00:14:37,289
averaging about 23 milliseconds so

392
00:14:37,289 --> 00:14:39,209
that's that's very close to 30 frames a

393
00:14:39,209 --> 00:14:42,179
second in real time so we think this is

394
00:14:42,179 --> 00:14:43,919
going to be really great on the electron

395
00:14:43,919 --> 00:14:47,729
side it doesn't block your UI thread for

396
00:14:47,729 --> 00:14:49,589
doing all the displays your dispatching

397
00:14:49,589 --> 00:14:52,709
all these Kreml calls through the node

398
00:14:52,709 --> 00:14:55,079
process all with a headless GL and that

399
00:14:55,079 --> 00:14:56,669
package is like five to ten megabytes

400
00:14:56,669 --> 00:15:01,709
it's very small and I also wanted to

401
00:15:01,709 --> 00:15:04,799
show one other thing this is the latest

402
00:15:04,799 --> 00:15:08,220
type of IOT boards this is a Nvidia

403
00:15:08,220 --> 00:15:11,129
Jetson Nano and basically just has a big

404
00:15:11,129 --> 00:15:13,859
GPU stapled to the top of it and we were

405
00:15:13,859 --> 00:15:16,049
able to last week at this writing with

406
00:15:16,049 --> 00:15:18,149
this headless stack as well running that

407
00:15:18,149 --> 00:15:21,419
same model a note console dump is it

408
00:15:21,419 --> 00:15:22,559
that most exciting but we're doing

409
00:15:22,559 --> 00:15:25,139
around 76 milliseconds of inference time

410
00:15:25,139 --> 00:15:29,519
just with the very thin arm 64 build of

411
00:15:29,519 --> 00:15:32,440
our our node back in

412
00:15:32,440 --> 00:15:36,190
I also want to talk about another

413
00:15:36,190 --> 00:15:37,389
library we've been working really hard

414
00:15:37,389 --> 00:15:40,360
and it's in browser visualization for

415
00:15:40,360 --> 00:15:43,509
our tensorflow GS library so ounce this

416
00:15:43,509 --> 00:15:46,600
package it's called TF GS - biz and you

417
00:15:46,600 --> 00:15:48,069
can think of it as like the chrome dev

418
00:15:48,069 --> 00:15:50,379
tools for ML models we have this thing

419
00:15:50,379 --> 00:15:52,089
called the visor and it slides out and

420
00:15:52,089 --> 00:15:54,220
it's a canvas for painting a bunch of

421
00:15:54,220 --> 00:15:57,069
elements that the library provides we

422
00:15:57,069 --> 00:15:59,019
have a bunch of built-in charts such as

423
00:15:59,019 --> 00:16:03,459
loss in accuracy for ML training we also

424
00:16:03,459 --> 00:16:05,050
have what we call high level

425
00:16:05,050 --> 00:16:07,209
visualization methods this basically

426
00:16:07,209 --> 00:16:09,250
allows you to look at those complicated

427
00:16:09,250 --> 00:16:11,319
hops like convolutions which do a bunch

428
00:16:11,319 --> 00:16:13,509
of filters on your image while you're

429
00:16:13,509 --> 00:16:14,920
training and see what's happening in

430
00:16:14,920 --> 00:16:18,910
between each of those convolutions model

431
00:16:18,910 --> 00:16:21,189
evaluation utilities is another set of

432
00:16:21,189 --> 00:16:23,350
drawing libraries and that sort of shows

433
00:16:23,350 --> 00:16:26,199
you where your model is might be over

434
00:16:26,199 --> 00:16:28,750
biased in particular class and ways that

435
00:16:28,750 --> 00:16:31,060
you can sort of see how you might alter

436
00:16:31,060 --> 00:16:32,589
your dataset to make sure you have a

437
00:16:32,589 --> 00:16:37,660
very nicely trained model all right

438
00:16:37,660 --> 00:16:38,920
we've been talking about we've shown you

439
00:16:38,920 --> 00:16:40,420
a lot of stuff but we want to show you a

440
00:16:40,420 --> 00:16:42,519
lot of the things that Nick held myself

441
00:16:42,519 --> 00:16:43,779
and the team have been thinking about

442
00:16:43,779 --> 00:16:45,189
and where we're going forward with the

443
00:16:45,189 --> 00:16:46,769
project

444
00:16:46,769 --> 00:16:48,880
one thing is we're really excited about

445
00:16:48,880 --> 00:16:52,000
the current future with all the new

446
00:16:52,000 --> 00:16:54,360
specs that are coming down the browser

447
00:16:54,360 --> 00:16:57,430
especially with JavaScript on the

448
00:16:57,430 --> 00:16:59,019
website we have two new standards we've

449
00:16:59,019 --> 00:17:00,009
been looking at really hard the last

450
00:17:00,009 --> 00:17:04,089
couple months one of them is web GPU web

451
00:17:04,089 --> 00:17:06,309
GPU is the next-generation graphic stack

452
00:17:06,309 --> 00:17:07,990
that's coming to the browser we've been

453
00:17:07,990 --> 00:17:09,370
working really hard with the chrome team

454
00:17:09,370 --> 00:17:11,799
to try to get that implementation up and

455
00:17:11,799 --> 00:17:14,500
rolling another one we've been looking

456
00:17:14,500 --> 00:17:17,709
at is wasum now in the ml world we

457
00:17:17,709 --> 00:17:20,169
really need to do Sindhi to make wasum a

458
00:17:20,169 --> 00:17:22,740
really an effective accelerator for CPUs

459
00:17:22,740 --> 00:17:24,850
so we've been working really hard on

460
00:17:24,850 --> 00:17:26,530
that with again with the chrome team and

461
00:17:26,530 --> 00:17:27,909
we're hoping to have something for

462
00:17:27,909 --> 00:17:29,860
devices where the GPU isn't all that

463
00:17:29,860 --> 00:17:34,700
great we can fall back to Lawson cindy

464
00:17:34,710 --> 00:17:37,720
one of the great parts about the ml

465
00:17:37,720 --> 00:17:39,870
space is just amount of research and

466
00:17:39,870 --> 00:17:43,120
we're finding about every year arm all

467
00:17:43,120 --> 00:17:46,380
the same models have get faster from

468
00:17:46,380 --> 00:17:49,060
reductions in architecture or new

469
00:17:49,060 --> 00:17:51,880
hardware acceleration stories so every

470
00:17:51,880 --> 00:17:53,050
year of the models that we keep showing

471
00:17:53,050 --> 00:17:55,030
continue to get faster especially on

472
00:17:55,030 --> 00:17:59,470
edge and browser devices another great

473
00:17:59,470 --> 00:18:01,900
product we have at Google is Auto ml an

474
00:18:01,900 --> 00:18:04,690
animal solves the whole training part if

475
00:18:04,690 --> 00:18:05,710
you want to do an image classification

476
00:18:05,710 --> 00:18:07,870
problem you can give it a set of images

477
00:18:07,870 --> 00:18:10,750
and it uploads to the cloud and it

478
00:18:10,750 --> 00:18:11,980
automatically finds the right

479
00:18:11,980 --> 00:18:13,840
architecture for your model and then

480
00:18:13,840 --> 00:18:15,520
spits out the model that you can deploy

481
00:18:15,520 --> 00:18:17,860
on your device we're looking at some

482
00:18:17,860 --> 00:18:19,510
integration with that team as well to

483
00:18:19,510 --> 00:18:20,590
make it just a really seamless

484
00:18:20,590 --> 00:18:23,470
experience and the other thing that our

485
00:18:23,470 --> 00:18:24,750
team has been focusing on is just

486
00:18:24,750 --> 00:18:27,130
optimizing our existing backends

487
00:18:27,130 --> 00:18:29,710
so our WebGL implementation for example

488
00:18:29,710 --> 00:18:32,110
we worked on packing textures which is a

489
00:18:32,110 --> 00:18:34,750
fancy term of using less memory as as

490
00:18:34,750 --> 00:18:36,610
much as possible in our acceleration

491
00:18:36,610 --> 00:18:39,100
library and we found that speed spent a

492
00:18:39,100 --> 00:18:41,560
bunch of things including iOS up to 10

493
00:18:41,560 --> 00:18:42,790
times faster than what we were seeing

494
00:18:42,790 --> 00:18:47,560
before looking at the things were going

495
00:18:47,560 --> 00:18:49,720
to launch this summer visualization has

496
00:18:49,720 --> 00:18:51,430
already launched another package that we

497
00:18:51,430 --> 00:18:52,660
didn't really highlight it's the data

498
00:18:52,660 --> 00:18:54,850
library and the data library is there

499
00:18:54,850 --> 00:18:57,460
real easy to use package for getting

500
00:18:57,460 --> 00:19:00,840
stuff out of the browser microphone data

501
00:19:00,840 --> 00:19:02,800
webcam data you don't have to worry

502
00:19:02,800 --> 00:19:04,090
about converting to tensors so you can

503
00:19:04,090 --> 00:19:05,620
just sort of streamline these things

504
00:19:05,620 --> 00:19:09,250
into the braaap into your model on our

505
00:19:09,250 --> 00:19:11,110
platform side expanding where we run

506
00:19:11,110 --> 00:19:14,260
tensorflow GS as mentioned we chat the

507
00:19:14,260 --> 00:19:16,000
headless WebGL stuff and then we're

508
00:19:16,000 --> 00:19:17,620
really starting to dive into how we can

509
00:19:17,620 --> 00:19:21,090
provide a nice react native experience

510
00:19:21,090 --> 00:19:23,710
and then on our out of the box model

511
00:19:23,710 --> 00:19:25,120
fronts we're going to continue focusing

512
00:19:25,120 --> 00:19:27,340
on audio and text models as well as

513
00:19:27,340 --> 00:19:29,500
improving the accuracy and performance

514
00:19:29,500 --> 00:19:33,280
of our existing offerings what I don't

515
00:19:33,280 --> 00:19:34,480
want to thank you for attending our talk

516
00:19:34,480 --> 00:19:37,000
everything we've shown is we work purely

517
00:19:37,000 --> 00:19:39,370
an open source and all of our stuff is

518
00:19:39,370 --> 00:19:42,940
found on GS tensorflow org one of the

519
00:19:42,940 --> 00:19:44,710
other things we wanted to acknowledge is

520
00:19:44,710 --> 00:19:47,500
while Nicola and myself work at Google

521
00:19:47,500 --> 00:19:48,940
and we get to work on this project we've

522
00:19:48,940 --> 00:19:50,440
this project would not be where it's at

523
00:19:50,440 --> 00:19:52,930
without the large number of open source

524
00:19:52,930 --> 00:19:54,520
contributors we've had and we want to

525
00:19:54,520 --> 00:19:56,260
extend a thank you to them for all the

526
00:19:56,260 --> 00:19:58,840
hard work they've done and then one last

527
00:19:58,840 --> 00:19:59,770
plug

528
00:19:59,770 --> 00:20:01,810
we are actually hiring a developer

529
00:20:01,810 --> 00:20:03,910
advocate for our team and if anyone is

530
00:20:03,910 --> 00:20:06,820
interested please follow that link or

531
00:20:06,820 --> 00:20:09,300
come see us at the booth here at GSK

532
00:20:09,300 --> 00:20:11,500
that's all

533
00:20:11,500 --> 00:20:15,320
[Applause]

