1
00:00:09,000 --> 00:00:11,240
Hi, everyone.

2
00:00:11,240 --> 00:00:15,210
Thank you so much for coming here today, and
a big shout out to the organisers for putting

3
00:00:15,210 --> 00:00:16,210
this event together.

4
00:00:16,210 --> 00:00:21,760
I'm very honoured to get to be on this stage
and talk to you about some really serious

5
00:00:21,760 --> 00:00:23,280
stuff.

6
00:00:23,280 --> 00:00:29,000
In 2016, the most pressing topic of the year
for software engineers was empathy, and, in

7
00:00:29,000 --> 00:00:34,140
2017, the topic of the year was inclusion,
and, this year, the topic seems to be about

8
00:00:34,140 --> 00:00:39,839
ethics, which I'm really excited about, because
the discourse grows more and more concerning

9
00:00:39,839 --> 00:00:41,219
each day.

10
00:00:41,219 --> 00:00:46,729
Even though these issues feel like new wounds,
they have a long history steeped in bias,

11
00:00:46,729 --> 00:00:52,899
and systemic oppression, and we need to pay
attention to these things.

12
00:00:52,899 --> 00:01:01,210
Data and society research institute recently
hosted Author Algorithms of Oppression, talking

13
00:01:01,210 --> 00:01:04,210
about technological red lining.

14
00:01:04,210 --> 00:01:10,480
Noble reflected at the event on her upbringing
in Fresno, California.

15
00:01:10,480 --> 00:01:20,470
She recalls seeing the KKK in public, the
sun set curfew on black people, the black

16
00:01:20,470 --> 00:01:27,060
poor side of town was a food desert which
meant there were no grocery stores at all.

17
00:01:27,060 --> 00:01:36,910
With no buses serving the town, it meant there
was a huge burden placed on the poor and the

18
00:01:36,910 --> 00:01:40,670
black people of Fresno, California.

19
00:01:40,670 --> 00:01:44,830
Her talking about that history and having
grown newspaper that environment, and how

20
00:01:44,830 --> 00:01:50,520
it shaped her to be the person who she was
and pushed her to do the research that she

21
00:01:50,520 --> 00:01:55,500
did to publish the book Algorithms of Oppression,
reminded me about the forgotten New York City

22
00:01:55,500 --> 00:02:02,310
of neighbourhood of Manhattan Town.

23
00:02:02,310 --> 00:02:09,280
In 1905, the colour line was broken in Colombia
University when he began purchasing real estate

24
00:02:09,280 --> 00:02:19,980
on upper 99th Street and then between Columbus
Avenue and Central Park West.

25
00:02:19,980 --> 00:02:24,069
There were a lot of notable resident of Manhattan
town, including some of the most venerated

26
00:02:24,069 --> 00:02:28,879
artists and intellectuals of the Harlem Renaissance.

27
00:02:28,879 --> 00:02:40,959
One of my trivia facts was that Sadie Fagin,
the mother of Billie Holliday ran a cafe called

28
00:02:40,959 --> 00:02:49,420
Mom Holidays, where she got in an argument
with Billie wanting to borrow money, and that's

29
00:02:49,420 --> 00:02:54,340
when she came up with the title of the song
God Bless The Child.

30
00:02:54,340 --> 00:03:02,629
By 19 inform the 1955, those two blocks were
nothing but rubble.

31
00:03:02,629 --> 00:03:09,919
In 1949, President Harry Truman signed the
Housing Act which gave Robert Moses in his

32
00:03:09,919 --> 00:03:15,540
position as Chairman of the New York City
Selenium committee, the power to condemn building

33
00:03:15,540 --> 00:03:18,450
and steal land under the guise of urban renewal.

34
00:03:18,450 --> 00:03:23,769
He would grant the land to companies of his
choosing, often as a way to amass political

35
00:03:23,769 --> 00:03:26,189
favour.

36
00:03:26,189 --> 00:03:31,200
Manhattan Town was literally the very first
neighbourhood that Moses picked to do this

37
00:03:31,200 --> 00:03:32,530
to.

38
00:03:32,530 --> 00:03:38,469
Residents of Manhattan Town were evicted in
1951, the buildings were torn down, and most

39
00:03:38,469 --> 00:03:45,760
of it was just rubble until the 1960s when
the Park West Village Apartments were built.

40
00:03:45,760 --> 00:03:52,049
Moses did irrevocable social I don't economic
damage by forcing more than 300,000 families

41
00:03:52,049 --> 00:03:58,329
out of their home in the name of urban renewal.

42
00:03:58,329 --> 00:04:05,469
Here you can see on maps, the two blocks are
gone, and in their place are the Park West

43
00:04:05,469 --> 00:04:09,029
Apartments and a Whole Foods Markets.

44
00:04:09,029 --> 00:04:17,209
We talk about displacement of people through
gentrification.

45
00:04:17,209 --> 00:04:21,900
In April of this year, Saheed Vassell was
killed by the New York Police Department as

46
00:04:21,900 --> 00:04:24,800
a consequence of gentrification.

47
00:04:24,800 --> 00:04:31,360
And tech has been responsible for all the
gentrification of all of San Francisco, and

48
00:04:31,360 --> 00:04:40,630
made it easy for the culture neighbourhoods,
Little Havana in Miami, and much of Brooklyn

49
00:04:40,630 --> 00:04:48,640
and New York, to be digitally erased by prioritising
commonly search terms that gain prominence.

50
00:04:48,640 --> 00:04:54,430
When we use Google to search for things, we
teach those machines that that is the priority,

51
00:04:54,430 --> 00:04:58,620
and, since many times low-income people don't
have access, they don't have a need to Google

52
00:04:58,620 --> 00:05:04,500
for something, that means, when there is more
attention paid to a neighbourhood, when gentrification

53
00:05:04,500 --> 00:05:10,690
starts happening, the software we are writing
is backing that up.

54
00:05:10,690 --> 00:05:14,720
So, algorithms with the help of machine-learning
are not objective.

55
00:05:14,720 --> 00:05:18,980
They're instilled with the biases by the people
who write them, when measures are not taken

56
00:05:18,980 --> 00:05:22,090
to consider the consequences of disruption.

57
00:05:22,090 --> 00:05:25,780
This is why we need to apply more scrutiny
when it comes to whether or not the features

58
00:05:25,780 --> 00:05:28,960
we push as software are ethical.

59
00:05:28,960 --> 00:05:34,910
Satre's theory of existentialism dictates
that humans creates their own values and determine

60
00:05:34,910 --> 00:05:36,620
their meaning to their life.

61
00:05:36,620 --> 00:05:41,810
For Satre, once an oppressive situation is
regarded as such by those who feel oppressed,

62
00:05:41,810 --> 00:05:46,010
the situation becomes intolerable.

63
00:05:46,010 --> 00:05:51,040
We are then compelled to project our intentions
on to our present condition and transform

64
00:05:51,040 --> 00:05:52,630
them into action.

65
00:05:52,630 --> 00:05:58,810
And because ethical dilemmas disproportionately
affect under represented minorities, it is

66
00:05:58,810 --> 00:06:03,350
important to take the time to listen to us
instead of assuming what we need.

67
00:06:03,350 --> 00:06:09,920
Some of the most pressing dilemmas right now
are surrounding the issues of privacy, surveillance,

68
00:06:09,920 --> 00:06:12,310
data, and discrimination.

69
00:06:12,310 --> 00:06:17,940
Of course, we've all heard about the way that
Cambridge Analytica unethically used Facebook's

70
00:06:17,940 --> 00:06:22,370
data, but there are even more serious things
happening right now, especially in the United

71
00:06:22,370 --> 00:06:23,370
States.

72
00:06:23,370 --> 00:06:29,840
The Stop LED Spying Coalition recently released
a report about the harassment and mistreatment

73
00:06:29,840 --> 00:06:36,050
of low-income black people due to the use
of predictive policing software by Palantir

74
00:06:36,050 --> 00:06:42,500
that they sold to the LAPD and nearly sold
to the New York Police Department.

75
00:06:42,500 --> 00:06:49,340
Amazon is trying to get into the predictive
policing with product recognition.

76
00:06:49,340 --> 00:06:55,560
It has the power to recognise people in large
groups of people and crowded events and public

77
00:06:55,560 --> 00:07:01,780
places, and they include race as an identifier
which results in increased bias.

78
00:07:01,780 --> 00:07:04,130
None of these issues are new.

79
00:07:04,130 --> 00:07:13,860
In 2016, Pro Publica uncovered bias, in the
Compass software, correctional management

80
00:07:13,860 --> 00:07:22,900
profiling for alternative sanctions, and it
is used by courts to automatically determine

81
00:07:22,900 --> 00:07:29,060
if sentencing based on criminal recidivism
scores set by questions that have extreme

82
00:07:29,060 --> 00:07:34,550
bias, against low-income black people, so
how do we prevent this kind of discrimination?

83
00:07:34,550 --> 00:07:40,770
How do we prevent our software from ending
people's lives?

84
00:07:40,770 --> 00:07:46,790
We need to agree on what the ethical foundation
of our work is in this industry, in the workplace,

85
00:07:46,790 --> 00:07:48,260
and in the world.

86
00:07:48,260 --> 00:07:55,020
In 1985, James Moore detailed in his essay
What Is Computer Ethics, these four tenets.

87
00:07:55,020 --> 00:08:01,890
It's important to apply these to the workplace
and ask these questions when we talk about

88
00:08:01,890 --> 00:08:07,480
machine-learning, AI, and data collection,
just to name a few.

89
00:08:07,480 --> 00:08:13,900
According to Professor Margaret Ann Pierce,
she defined the need for the following three

90
00:08:13,900 --> 00:08:23,490
ethical influences: an individual personal
code, an informal code of ethics, and - I'm

91
00:08:23,490 --> 00:08:30,590
sorry - an informal code of ethics, and a
code of ethics in the workplace.

92
00:08:30,590 --> 00:08:35,820
Because, unlike our code, we can't say that
we were only taking orders.

93
00:08:35,820 --> 00:08:43,390
Or, as it was been called, rationalising in
the way of the little - I like to remember,

94
00:08:43,390 --> 00:08:50,500
when we talk about ethics, the root Greek
word is "ethos" which translates to "habit"

95
00:08:50,500 --> 00:08:57,150
or "custom", and that is the key to developing
an ethical framework, because we need to be

96
00:08:57,150 --> 00:08:58,150
accountable.

97
00:08:58,150 --> 00:09:03,050
We need to make it a habit; we need to make
it a custom.

98
00:09:03,050 --> 00:09:11,180
In the essay Moral Machines, they wrote about
how machine ethics were drive advancement

99
00:09:11,180 --> 00:09:17,320
in human ethics but forcing us to address
gaps in modern normative theories of ethics.

100
00:09:17,320 --> 00:09:18,320
This is our moral imperative.

101
00:09:18,320 --> 00:09:26,780
It is our job to consider the ways that our
actions are shaping the ethical world.

102
00:09:26,780 --> 00:09:30,810
Here are some organisations with their own
form al codes.

103
00:09:30,810 --> 00:09:34,430
Right how in the United States, the only CS
programmes that require ethics are the ones

104
00:09:34,430 --> 00:09:41,530
that are ABET accredited, but the Association
For Computer Machinery - ACM - is considered

105
00:09:41,530 --> 00:09:44,090
one of the most appropriate for web development.

106
00:09:44,090 --> 00:09:49,760
I urge you all to look up all six of these,
and pick one.

107
00:09:49,760 --> 00:09:51,279
Apply it to your work.

108
00:09:51,279 --> 00:09:56,190
Apply it to the decisions that you make.

109
00:09:56,190 --> 00:10:02,260
Practising ethical decision-making in the
workplace is about stretching your imagination.

110
00:10:02,260 --> 00:10:10,110
The EFF recently published a science fiction
story by Corey Doctrau to consider the sections

111
00:10:10,110 --> 00:10:17,390
of 12.01 of the DMCA and how, coming down
hard on copyright infringement, will change

112
00:10:17,390 --> 00:10:18,390
society.

113
00:10:18,390 --> 00:10:28,300
It is important not just to think about these
issues, it's important that we act on them.

114
00:10:28,300 --> 00:10:34,990
If you remember Robert Moses, the man who
basically pushed all of those people out of

115
00:10:34,990 --> 00:10:40,660
Manhattanville, his legacy was extremely far
reaching.

116
00:10:40,660 --> 00:10:47,589
He was a big fan of the automobile, and he
wanted to build a highway that would run right

117
00:10:47,589 --> 00:10:52,590
down the middle of Washington Square Park,
and carve up the middle of the park, carve

118
00:10:52,590 --> 00:10:56,660
up the middle of the neighbourhood, and just
send trucks and cars right down the middle

119
00:10:56,660 --> 00:11:01,730
of the neighbourhood, because he felt innovation
in automobiles was more important than the

120
00:11:01,730 --> 00:11:04,080
lives of the people in the communities.

121
00:11:04,080 --> 00:11:12,221
He was also making a lot of money for a lot
of other people, and felt that that was more

122
00:11:12,221 --> 00:11:16,190
important than the people who were living
in these homes.

123
00:11:16,190 --> 00:11:19,760
But, he was stopped.

124
00:11:19,760 --> 00:11:23,779
Washington Square Park, if you've ever been
there, has no highway running through it,

125
00:11:23,779 --> 00:11:26,430
and that's because of this person.

126
00:11:26,430 --> 00:11:28,970
This is Jane Jacobs.

127
00:11:28,970 --> 00:11:30,980
She was a writer.

128
00:11:30,980 --> 00:11:35,321
She most notably published a book called The
death and Life of Great American Cities, and

129
00:11:35,321 --> 00:11:38,790
she was an activist in New York City.

130
00:11:38,790 --> 00:11:44,589
When Robert Moses came to that neighbourhood,
when he came to Washington Square Park, Jane

131
00:11:44,589 --> 00:11:55,020
Jacobs organised the people of the neighbourhood
and put together activism to keep Robert Moses

132
00:11:55,020 --> 00:11:58,900
from enacting his plan.

133
00:11:58,900 --> 00:12:01,750
Jane Jacobs was really good at the photo op.

134
00:12:01,750 --> 00:12:08,190
She put moms with strollers in the middle
of the park to protest, and she had a ribbon

135
00:12:08,190 --> 00:12:14,930
re-tying ceremony in the park to mock Moses'
ribbon-cutting ceremony that he would do when

136
00:12:14,930 --> 00:12:21,230
he opened new highways and bridges, about
and because she rallied all of these people

137
00:12:21,230 --> 00:12:29,080
and participated in civil disobedience, got
arrested for protesting at city hearings,

138
00:12:29,080 --> 00:12:35,340
the mayor was forced to stop these plans that
Robert Moses had to build a highway through

139
00:12:35,340 --> 00:12:41,390
the middle of the park.

140
00:12:41,390 --> 00:12:45,460
What ended up happening was he went on and
he tried to continue to do this, and, as he

141
00:12:45,460 --> 00:12:50,160
continued to try to do this in other neighbourhoods,
tried to carve the middle of Canal Street

142
00:12:50,160 --> 00:12:57,220
down in New York City, and Jane Jacobs came
back and pushed him down, they kept and got

143
00:12:57,220 --> 00:13:00,920
to maintain the living that they already had.

144
00:13:00,920 --> 00:13:06,339
They were able to defend themselves, and,
we recently saw this happen too in our industry,

145
00:13:06,339 --> 00:13:15,000
because just yesterday, Google - Gizmodo reported
Google will not be renewing their contract

146
00:13:15,000 --> 00:13:21,089
for Project Maven with the DOD because the
actions employees took.

147
00:13:21,089 --> 00:13:27,600
A dozen employees resigned, signed paperwork
saying they didn't believe that this was analytical

148
00:13:27,600 --> 00:13:35,550
decision, didn't want to participate in writing
AI software for drones, and, because of this,

149
00:13:35,550 --> 00:13:40,190
just like the civil disobedience and the activism
of Jane Jacobs, we, too, can have an effect

150
00:13:40,190 --> 00:13:43,730
on our industry.

151
00:13:43,730 --> 00:13:46,270
Companies are willing to ... [Applause].

152
00:13:46,270 --> 00:13:54,959
Consider how much money companies are willing
to pay to get our attention.

153
00:13:54,959 --> 00:13:59,520
How much money they're willing to pay us in
salaries; how much money they're spending

154
00:13:59,520 --> 00:14:02,410
to sponsor conferences just like this one.

155
00:14:02,410 --> 00:14:08,290
We have power over them as a collective group,
and we need to - it's our moral imperative

156
00:14:08,290 --> 00:14:14,140
to use that power for good and not for evil,
and we need to come up with an ethical framework

157
00:14:14,140 --> 00:14:18,380
to do that that is more explicit than just
good versions evil.

158
00:14:18,380 --> 00:14:23,370
We need to decide on codes of ethics, the
same way that we decide on codes of conduct

159
00:14:23,370 --> 00:14:24,570
for our conferences.

160
00:14:24,570 --> 00:14:31,100
Codes of conduct are just ethical - codes
of ethics that we apply to our internal group.

161
00:14:31,100 --> 00:14:35,250
It is deciding the ways that we want to treat
the people in our society.

162
00:14:35,250 --> 00:14:46,430
So we need to apply the same kind of codes
to our work and the world at large.

163
00:14:46,430 --> 00:14:51,140
[Applause].

164
00:14:51,140 --> 00:14:58,120
It's our job to design computers, to design
phones, to design software, in a way that

165
00:14:58,120 --> 00:15:06,360
no-one dies, and to consider the historical,
global, political, social, and economic repercussions

166
00:15:06,360 --> 00:15:09,160
of the work that we do.

167
00:15:09,160 --> 00:15:19,029
I want to encourage each and every one of
you to consider the ways that you can make

168
00:15:19,029 --> 00:15:23,810
your work more ethical, the ways that you
can prevent people from being harmed by the

169
00:15:23,810 --> 00:15:29,779
software that you write, whether it be through
gentrification in maps, through AI having

170
00:15:29,779 --> 00:15:38,490
bias, in machine-learning, or even on your
team, when the team-mates are the subject

171
00:15:38,490 --> 00:15:40,530
of their own oppression.

172
00:15:40,530 --> 00:15:44,670
What are you doing with your power to help
the people who you work with and the people

173
00:15:44,670 --> 00:15:48,500
of the world at large?

174
00:15:48,500 --> 00:15:52,910
God will not have his work made manifest by
cowards before you have to be brave, you have

175
00:15:52,910 --> 00:15:55,290
to take that responsibility on yourself.

176
00:15:55,290 --> 00:15:56,850
Thank you.

