1
00:00:09,800 --> 00:00:13,761
Hi, everybody. I am Ashe, I will be your guide today to describe our exploration 

2
00:00:13,761 --> 00:00:24,009
Learning and JavaScript World. Sorry i'm not a machine learning expert

3
00:00:24,009 --> 00:00:34,330
That, but my mom is. She is an audiologist by training and she does work in digital filters

4
00:00:34,330 --> 00:00:39,430
Acoustic models for hearing aids and then continue to work as speech recognizers.

5
00:00:39,430 --> 00:00:46,980
I remember working in her lab one summer and people said these weird and intimidating

6
00:00:46,980 --> 00:00:53,881
Then, and all these strange things on the wall, I'm overwhelmed. in

7
00:00:53,881 --> 00:00:58,460
I basically didn't understand the time for it. So the process is also impressive

8
00:00:58,460 --> 00:01:04,920
-A process of approaching her I can go to her and say, "I know what a cross is

9
00:01:04,920 --> 00:01:12,780
Entropy is, "She's like," That's fine. Can you explain? !! "I think I can,

10
00:01:12,780 --> 00:01:21,149
Explain to you and be fully prepared to use it. I have been learning deep learning

11
00:01:21,149 --> 00:01:28,890
Out of curiosity, a little excited about the future, and also a sense of presence

12
00:01:28,890 --> 00:01:34,140
The coming horrors of robots, they are going to consume all our work, and maybe

13
00:01:34,140 --> 00:01:39,549
Our society may be ourselves. There might be the case for this matrix pod

14
00:01:39,549 --> 00:01:46,850
Will happen. I have some exciting news. This is very, very impressive.

15
00:01:46,850 --> 00:01:52,399
And they are a bit stupid. Like a fool in a very basic way. They are like this

16
00:01:52,399 --> 00:01:57,399
May not accept your job-at least not in the next year or two-but they are

17
00:01:57,399 --> 00:02:02,679
Change it, and change it very significantly. Now this is a very exciting moment

18
00:02:02,679 --> 00:02:06,459
Enter this field. There is a huge amount of research and many new tools

19
00:02:06,459 --> 00:02:12,430
We can use. Let's dive in. Before we dive in, I want to give a single definition.

20
00:02:12,430 --> 00:02:19,599
I made this speech and a student was like, "So you say tension is like 100 times, that's

21
00:02:19,599 --> 00:02:32,540
A terrible word. I feel nervous now. "Tension, if you look it up on Wikipedia, it's a number

22
00:02:32,540 --> 00:02:40,030
Fields closed by some free operation. I would say that a tensor is a block of numbers.

23
00:02:40,030 --> 00:02:44,879
We can have an actual block of numbers just a number, i.e. rank zero

24
00:02:44,879 --> 00:02:53,739
Tensor or scaler. We can have a number, a line of numbers, a vector, an ice rink

25
00:02:53,739 --> 00:03:01,780
1 tensor, matrix, square or square newspaper rectangle arranged by tensor, we can

26
00:03:01,780 --> 00:03:10,600
Prisms are arranged in tensors, and they are constantly changing to become more and more difficult to draw.

27
00:03:10,600 --> 00:03:16,260
So I don't plan to paint them. I just defined your tensor because I'm about to talk

28
00:03:16,260 --> 00:03:25,969
About Tensor Flow, it is a state art machine learning framework. Available now

29
00:03:25,969 --> 00:03:37,069
In JavaScript. So, let's break down the available, so in use

30
00:03:37,069 --> 00:03:48,300
C ++ API, I am using the Pipeline API, I have a good grasp of mathematical operations on the CPU

31
00:03:48,300 --> 00:03:55,120
GPU, capable of doing more operations in a, the parallel precision is slightly lower, and,

32
00:03:55,120 --> 00:04:01,709
Then on the TPU, it's like a GPU, but there are even more, even crazy, computing units. it

33
00:04:01,709 --> 00:04:08,540
Is Google's dedicated hardware, which is optimized for machine learning

34
00:04:08,540 --> 00:04:15,709
especially. It turns out that machine learning is a whole bunch of very simple operations,

35
00:04:15,709 --> 00:04:23,100
It is therefore desirable to be able to simply parallelize operations. JavaScript binding

36
00:04:23,100 --> 00:04:32,889
Currently giving us CPU calculations under Node, and then Web binding is executed using WebGL

37
00:04:32,889 --> 00:04:40,010
mathematics. Soon, node binding and the Tensorflow team promised to use a C ++ backend

38
00:04:40,010 --> 00:04:46,530
Means that we should have performance equal Python libraries. Web binding

39
00:04:46,530 --> 00:04:53,110
Performance using GPU is only half unfortunately C ++ libraries, but

40
00:04:53,110 --> 00:04:58,550
You can do this in your browser, which is pretty cool. Another important part about doing

41
00:04:58,550 --> 00:05:05,770
Machine learning research and development of these models is an ecosystem around core processing

42
00:05:05,770 --> 00:05:11,060
The library and ecosystem we are using is huge in Python, and in the ecosystem

43
00:05:11,060 --> 00:05:20,460
JavaScript is sad. And, it doesn't matter. If any Propel people or anyone doing science

44
00:05:20,460 --> 00:05:24,700
Computing in JavaScript is here, I want to say your job is great, I really

45
00:05:24,700 --> 00:05:32,040
Expect it, and its size community is currently small, but if

46
00:05:32,040 --> 00:05:38,260
The history of the JavaScript framework is any indication that we will soon build a big and interesting one,

47
00:05:38,260 --> 00:05:44,009
And a strong software ecosystem. Just the current situation is if you want to build

48
00:05:44,009 --> 00:05:50,040
Your own great deep learning models and training their datasets

49
00:05:50,040 --> 00:05:56,460
You might need to train on multiple computers in order to access, then you might go

50
00:05:56,460 --> 00:06:02,660
Do this with Python in the cloud, but you can take these models and this is

51
00:06:02,660 --> 00:06:08,180
Exciting things about tensorflow.js-you can take them and run them in the browser. it

52
00:06:08,180 --> 00:06:14,720
Means you can use the power of machine learning to not send all of you in the browser

53
00:06:14,720 --> 00:06:21,460
User data is sent to a provider in the sky, and you can continue to train those models

54
00:06:21,460 --> 00:06:26,910
local. We can do something called "transfer learning" where we cut off the last point

55
00:06:26,910 --> 00:06:33,660
Model, we adapt it without retraining all models deep

56
00:06:33,660 --> 00:06:39,790
To give users machine learning, there are no advantages of machine learning

57
00:06:39,790 --> 00:06:49,280
Privacy impact or entanglement surveillance. I just say "model", for example, 500

58
00:06:49,280 --> 00:06:55,750
Times. What is a model? Let's say we have already happened

59
00:06:55,750 --> 00:07:04,150
The world, this is a snake or it is a picture of a snake we want to model. we think

60
00:07:04,150 --> 00:07:09,052
Understand it somehow. We want to simplify its version. This is the model: it

61
00:07:09,052 --> 00:07:14,220
Is a simplified version of the world transformation into mathematics. So, in this case, we are leaving

62
00:07:14,220 --> 00:07:21,550
Turn our snake into a wave shape. Through machine learning, we go through the training process there

63
00:07:21,550 --> 00:07:29,349
We want to find a set of model parameters that allow us to adapt to the world as much as possible.

64
00:07:29,349 --> 00:07:35,750
We can imagine trying different parameter sets, like different waves, random

65
00:07:35,750 --> 00:07:45,199
Until we find something suitable for this snake. This is not ideal. We can sit here all day.

66
00:07:45,199 --> 00:07:50,200
We do n’t have good metrics we are doing and we do n’t have that feeling

67
00:07:50,200 --> 00:07:55,360
We are making progress. So we really want to find a way to pick

68
00:07:55,360 --> 00:08:00,141
Some parameters, waveforms and its set iteratively improve it and do what he wants

69
00:08:00,141 --> 00:08:06,990
We naturally improve ourselves and understand the situation before we find it

70
00:08:06,990 --> 00:08:17,780
It fits well. We can implement a process called stochastic gradient descent. What if you are

71
00:08:17,780 --> 00:08:22,610
Machine learning experts in the audience have various gradient descent techniques.

72
00:08:22,610 --> 00:08:29,020
We now look at the simplest. Let's talk about a paint splash and i have no model

73
00:08:29,020 --> 00:08:35,190
it. If I wanted to simulate the splash of paint, I would almost certainly not do it,

74
00:08:35,190 --> 00:08:41,380
But I will treat it as a line because there are two parameters to make it easy to visualize

75
00:08:41,380 --> 00:08:46,140
We need to visualize all kinds of things for them so i want to imitate splash

76
00:08:46,140 --> 00:08:50,720
Paint as a line and we will become happy about it. First, I ’m going to throw

77
00:08:50,720 --> 00:08:56,440
A coordination system under it, I turned around and divided these into X and Y points. I want to go back

78
00:08:56,440 --> 00:09:02,019
Entering my depressed high school recall algebra to remember the equation

79
00:09:02,019 --> 00:09:17,360
One line is Y = MXB. I have Y, line, B is line interception. If I choose random values

80
00:09:17,360 --> 00:09:22,410
For these two parameters, I will get a line. I need-any two random values

81
00:09:22,410 --> 00:09:30,190
Give me a line. This line is not very good. So, at this point, it is obsolete

82
00:09:30,190 --> 00:09:38,200
These two points are good, if we go through and find that deviation

83
00:09:38,200 --> 00:09:48,470
The whole set of examples, then what are we looking at a quantity called "loss". Lose,

84
00:09:48,470 --> 00:09:54,510
How bad a long-term relationship is, just like the feeling you end up feeling

85
00:09:54,510 --> 00:10:03,240
We did, and our model fits the data very well. This is a self-whipping machine learning ...

86
00:10:03,240 --> 00:10:08,839
A common loss we use, especially for regression, and this is what we are doing

87
00:10:08,839 --> 00:10:14,779
Now called mean square error, meaning we take the difference between the averages

88
00:10:14,779 --> 00:10:20,800
Model and ground true squared. If we write it in JavaScript, it will

89
00:10:20,800 --> 00:10:26,740
It looks like this. We can reduce the data and find the differences between the data

90
00:10:26,740 --> 00:10:31,781
Our model predicts the value of this data point and the actual point, squared, divided

91
00:10:31,781 --> 00:10:41,520
It presses the length and then gives us this function we can pass in-we have

92
00:10:41,520 --> 00:10:48,510
Line function. We can pass model parameters here. Any two model parameters are in progress

93
00:10:48,510 --> 00:10:54,610
This data generates specific losses. This means because we have two

94
00:10:54,610 --> 00:11:00,220
Them, I can imagine it on a plane and say that this will be the slope of our line

95
00:11:00,220 --> 00:11:08,279
-邋--and the height of the X axis it is, and, for some given model parameter set,

96
00:11:08,279 --> 00:11:13,480
In fact, for each set of model parameters, there will be some losses. So what can we do

97
00:11:13,480 --> 00:11:20,250
Now figure out what was the loss and poke around that if my line

98
00:11:20,250 --> 00:11:27,800
Comparison ? What if it's not so fuzzy? What about higher or lower? one of

99
00:11:27,800 --> 00:11:33,680
Direction, we will reduce losses, and so we have to take a step in that direction

100
00:11:33,680 --> 00:11:41,831
Along two axes. We will do it again. More slopy, less slopy, higher or lower. once again

101
00:11:41,831 --> 00:11:55,930
Then again. At each step, we are using losses to indicate our direction of movement. Lose

102
00:11:55,930 --> 00:12:04,470
Tell us where to go. It reveals that it is a lost landscape for us. what

103
00:12:04,470 --> 00:12:09,631
What we are doing is we are working on the slope of this landscape at every point,

104
00:12:09,631 --> 00:12:14,830
The general mapping of the slope of the landscape is its gradient, so this process

105
00:12:14,830 --> 00:12:21,510
What we are doing is gradient descent. We roll down this landscape like rain

106
00:12:21,510 --> 00:12:29,649
Fall into the closest valley truth. So there are many ways

107
00:12:29,649 --> 00:12:35,460
We may adjust this process. One is to notice that if we calculate the loss for everyone

108
00:12:35,460 --> 00:12:41,670
In the example, all the paint, that takes a while. Is not

109
00:12:41,670 --> 00:12:47,100
It will take a long time to find a line and X, Y points, but if we have a larger model,

110
00:12:47,100 --> 00:12:54,240
Then calculating the loss can be quite expensive, so we might catch some examples,

111
00:12:54,240 --> 00:13:03,930
random. Randomly, you might say, if you want to say casually rather than randomly,

112
00:13:03,930 --> 00:13:10,079
So this gives us a random descent gradient. We may choose other parameters

113
00:13:10,079 --> 00:13:18,920
The size of the steps we took. This is called the learning rate. These quantities, sizes

114
00:13:18,920 --> 00:13:23,150
The number of batches, such as the number we see, or the learning rate, they are

115
00:13:23,150 --> 00:13:29,370
Did n’t learn, we do n’t train them, so they are not called model parameters, but super

116
00:13:29,370 --> 00:13:34,010
Parameters are a very exciting word, and I don't think the model learns them

117
00:13:34,010 --> 00:13:39,810
During training, we set it manually. We train the model, which usually runs hundreds of

118
00:13:39,810 --> 00:13:48,650
Experiment and stare at the chart until our eyes bleed. Ok. This is a line.

119
00:13:48,650 --> 00:13:55,490
It's like a very simple, very simple feature, and maybe not very useful, right? Have

120
00:13:55,490 --> 00:14:00,410
We may use it for further functional learning. For example, we might use one of these

121
00:14:00,410 --> 00:14:11,769
A set of sigmoid functions. These mimic neurons. Here, neurons are firing,

122
00:14:11,769 --> 00:14:19,070
This is not the case here. They go well because they-because of this they are different

123
00:14:19,070 --> 00:14:54,980
Essential .. [Sound Distortion]. This is a difficult feature

124
00:14:54,980 --> 00:15:21,230
A complicated function. That's it

125
00:15:21,230 --> 00:15:29,470
X's math and zero. That's it. That function is easy to think of. We can imagine

126
00:15:29,470 --> 00:15:35,300
Right, and-can't write a JavaScript series. The original one

127
00:15:35,300 --> 00:15:40,420
Simple and easy to calculate is great for deep learning, where,

128
00:15:40,420 --> 00:15:46,089
Once again, we did not do very interesting or complicated operations, we are definitely doing

129
00:15:46,089 --> 00:15:53,269
A lot of people. We can imagine stacking up these rectifiers and we will be here

130
00:15:53,269 --> 00:16:03,040
There are four layers, and four neurons are densely interconnected into two layers. Because they are densely interconnected,

131
00:16:03,040 --> 00:16:10,149
We want to say that each neuron is fed by everyone on the second layer

132
00:16:10,149 --> 00:16:16,079
Neurons in the first layer. So this one, for example, its input will be weighted

133
00:16:16,079 --> 00:16:21,339
Sum inputs from all neurons to the previous layer, if you want

134
00:16:21,339 --> 00:16:28,449
It, because of the shape of this function, what we are really doing is nesting if statements.

135
00:16:28,449 --> 00:16:33,759
We nest if statements in if statements whose value depends on the previous output

136
00:16:33,759 --> 00:16:41,509
If statements, their thresholds are basically completely hard-coded. Next time you see the protector

137
00:16:41,509 --> 00:16:46,759
Created a deep neural network at google just think about it, this will do something impressive

138
00:16:46,759 --> 00:16:52,250
Google researchers have figured out 50 million random values ​​hard-coded in order

139
00:16:52,250 --> 00:16:57,310
Do something impressive, which is basically what's going on. The impressive part is obviously

140
00:16:57,310 --> 00:17:04,370
The training process found out our own hard-coded values, however, in

141
00:17:04,370 --> 00:17:09,390
In the end, what the model is doing is basically lucky with a bunch of spaghetti codes

142
00:17:09,390 --> 00:17:16,761
Simulating our brains is good. Even a model like this, the relatively small one, if

143
00:17:16,761 --> 00:17:22,580
We consider the number of interconnections we see that between these two layers of neurons

144
00:17:22,580 --> 00:17:28,930
We have 16. For a line we have two parameters and we are able to think

145
00:17:28,930 --> 00:17:36,680
About its losses. This model has 16 parameters, I do n’t know about you, but

146
00:17:36,680 --> 00:17:48,320
I can hardly imagine a 17-dimensional surface. It gets worse. What we see

147
00:17:48,320 --> 00:17:58,080
Disclosed here is the visualisation of Loss Resnet's landscape, which is an image classifier.

148
00:17:58,080 --> 00:18:08,281
Resnet has about 60 million parameters. It means that this is a heavy approximation. These ones

149
00:18:08,281 --> 00:18:12,910
People made some interesting projections in order to make it even like something

150
00:18:12,910 --> 00:18:18,800
Three-dimensional. Someone said the terrible thing of living in the base

151
00:18:18,800 --> 00:18:23,690
The sea will wake up one day and consume the world. They have length, width, depth,

152
00:18:23,690 --> 00:18:32,280
There are several other things, and maybe what this Lovecraft is talking about. Ok

153
00:18:32,280 --> 00:18:36,480
News is that you don't have to train those role models. You don't even have to think about

154
00:18:36,480 --> 00:18:44,410
They or keep their loss landscape heads in your loss because you can install them with MPM! with

155
00:18:44,410 --> 00:18:51,190
It looks like-of course, if you want to train those models, I highly encourage it.

156
00:18:51,190 --> 00:18:55,700
We will look at a transfer example to learn where we take a pre-trained model

157
00:18:55,700 --> 00:19:02,590
Then train it to do something else. It lets us make the most of all our training time

158
00:19:02,590 --> 00:19:10,320
In the bigger case, in this case, image recognition model and then use it to solve another problem

159
00:19:10,320 --> 00:19:17,210
space. So we are going to transfer learning, what are we going to do, this is an example,

160
00:19:17,210 --> 00:19:26,410
You can mention it on GitHub. I'm going to play Pac-Man with my elephant friend Tallula.

161
00:19:26,410 --> 00:19:34,640
The way this works is I choose a bunch of examples using a webcam that represents an image

162
00:19:34,640 --> 00:19:39,370
up down left right. I'm turning to the left. I'm trying to get into the framework,

163
00:19:39,370 --> 00:19:45,080
Try not to find a representative in the framework, or give the network a representative

164
00:19:45,080 --> 00:19:50,760
Where I'm going and how to hold her feel, you can see that I don't. we

165
00:19:50,760 --> 00:19:59,330
Go train it. It's low. Then, when I play, the network will be highlighted

166
00:19:59,330 --> 00:20:05,810
In the yellow direction it thinks I'm moving in, we can see that it works well,

167
00:20:05,810 --> 00:20:12,270
At least Pac-Man didn't hold Tallula as well until I started getting stressed

168
00:20:12,270 --> 00:20:19,360
My way during training. If you want to destroy friendship, use your friends as

169
00:20:19,360 --> 00:20:26,860
Controllers are a great way! Now I am eaten. I was eaten. And i am happy

170
00:20:26,860 --> 00:20:38,930
The report says that we are still friends! Thing we do is what we do is MPM installation

171
00:20:38,930 --> 00:20:51,090
Everything including Tensorflow. One thing we did was install Tensorflow. then we

172
00:20:51,090 --> 00:20:58,340
The model is loaded. Our model you can also install MPM, this special

173
00:20:58,340 --> 00:21:04,251
The model is provided somewhere. And because we are transferring, we are

174
00:21:04,251 --> 00:21:10,030
Going to make a little surgical model, so we're going to pull out this layer

175
00:21:10,030 --> 00:21:18,400
conPW13relu, whatever that means, then we will build a new model

176
00:21:18,400 --> 00:21:26,630
Use the main input as the mobile network output that is low but not the last layer. Actual final

177
00:21:26,630 --> 00:21:35,190
The mobile network layer will be, like, 200 probabilities, or probabilities

178
00:21:35,190 --> 00:21:40,510
There is a cat in this photo. Probability this photo contains a cow. Probability

179
00:21:40,510 --> 00:21:45,070
This photo contains a laptop and then turns on an image of whatever category

180
00:21:45,070 --> 00:21:50,970
The mobile web has been trained for recognition. What we wanted before, what was in it

181
00:21:50,970 --> 00:21:58,140
This image has been reshaped into some arbitrary chunks of interesting data, but there are

182
00:21:58,140 --> 00:22:09,221
It hasn't been eliminated to what it contains. We will see more in a second. When

183
00:22:09,221 --> 00:22:14,170
I am adding examples and this is what is happening, it is controlling the dataset being built

184
00:22:14,170 --> 00:22:22,780
A set of sample data. Then we build our model. Our model will take

185
00:22:22,780 --> 00:22:27,480
The output of this Tensorflow layer, it is squashing it, it will finish it

186
00:22:27,480 --> 00:22:35,560
A configurable number-let's call it 100-densely interconnected relu neurons, and,

187
00:22:35,560 --> 00:22:41,630
So, in the end, we will have a soft Max layer. It is a different activation function

188
00:22:41,630 --> 00:22:47,000
This is useful if you want a probability distribution, so, in this case, what we want

189
00:22:47,000 --> 00:22:55,430
Probability distributions. The Num course is under four years old because we have up, down, left,

190
00:22:55,430 --> 00:22:58,950
And that's between all the decisions we have to make. Output of our network

191
00:22:58,950 --> 00:23:08,080
Will become relative probability I hold her, left, right, or

192
00:23:08,080 --> 00:23:16,280
and many more. Then we configure an optimizer. We did not use stochastic gradient descent,

193
00:23:16,280 --> 00:23:24,460
We are using the Adam technique of random gradient which is better. This is a little

194
00:23:24,460 --> 00:23:32,430
A little smarter about how it decides the steps it needs. We want to compile this model

195
00:23:32,430 --> 00:23:43,680
Has a loss function, and the loss function is "cross-functional entropy. The reason

196
00:23:43,680 --> 00:23:48,640
If we have this example, that's an example I hold Tallula upside down and say

197
00:23:48,640 --> 00:23:53,730
Down, the network predicts this-which is technically predicting what I am holding

198
00:23:53,730 --> 00:24:03,280
Her right-how bad is this, really? Because, like, it ’s very close-predict

199
00:24:03,280 --> 00:24:08,550
Is wrong. If these are flipped, it will think it is still a bit wrong

200
00:24:08,550 --> 00:24:13,240
I have a 10% chance of holding her correctly, you know what? Reply

201
00:24:13,240 --> 00:24:20,980
That question is what classification cross entropy does. How much is confused by this model

202
00:24:20,980 --> 00:24:27,310
Different probability classes? Now you know. Finally, we call it suitable for this

203
00:24:27,310 --> 00:24:33,381
Is actually starting to dispatch things on the GPU and we get these callbacks every time

204
00:24:33,381 --> 00:24:39,250
When the batch completed. So every time we have calculated our loss

205
00:24:39,250 --> 00:24:47,630
If we have updated the weights, then we have taken a step. Row. To play games,

206
00:24:47,630 --> 00:24:54,160
We ask the mobile web to make predictions and we run our model to give us one of four probabilities

207
00:24:54,160 --> 00:25:01,830
Course, then we find out which one is most likely, we do it. That is

208
00:25:01,830 --> 00:25:08,680
Parkman. That is the tense of transfer learning or flow.js. I hope we go back

209
00:25:08,680 --> 00:25:14,630
Understand what mobile web we are getting rid of, and, to do, I want to load

210
00:25:14,630 --> 00:25:21,950
Mobile web, load the JSON file we loaded back in the browser. Here we will see

211
00:25:21,950 --> 00:25:30,920
This is a deep learning system that lets us describe the Caris model as a bunch of layers,

212
00:25:30,920 --> 00:25:43,400
Here it is the layer. come on. Click. So a deep learning network that recognizes images

213
00:25:43,400 --> 00:25:49,140
It usually looks like this. We have got convolutional layers and normalization

214
00:25:49,140 --> 00:25:53,130
Layers and activation layers. Activation layer we know what they look like-those

215
00:25:53,130 --> 00:26:00,900
This is the relu layer we saw before. Normalization ensures that our values ​​are between 0 and 0

216
00:26:00,900 --> 00:26:08,401
One person, they span a single batch, which is why they call it the batch layer. convolution

217
00:26:08,401 --> 00:26:13,130
Layers have configuration parameters, like many things in machine learning,

218
00:26:13,130 --> 00:26:20,400
They sound difficult, but they are not difficult. Convolutions are basically Photoshop filters.

219
00:26:20,400 --> 00:26:25,770
If we have a bunch of input pixels, the convolutional layer will grab some blocks

220
00:26:25,770 --> 00:26:33,400
Those pixels pass the filter and output it. It will filter the filter

221
00:26:33,400 --> 00:26:40,090
The entire image produces an output image. If we do not allow this, you will notice

222
00:26:40,090 --> 00:26:44,240
The filter slides down the edges and then we get something slightly smaller. We can

223
00:26:44,240 --> 00:26:49,800
Decide what we want. This is one of many tunable parameters. Here comes the convolution

224
00:26:49,800 --> 00:26:56,230
Various shapes and sizes. This is three times three. The key here is this

225
00:26:56,230 --> 00:27:03,220
The filter is the same image throughout the process and it is trainable, which means

226
00:27:03,220 --> 00:27:10,670
Actually, let's see what it means. So I left and went to the mobile network. place it here.

227
00:27:10,670 --> 00:27:15,610
This is what happens in each one we see many, many convolutional layers

228
00:27:15,610 --> 00:27:20,590
prior to. Yes it looks like a bunch of crappy Photoshop filters because it's a bunch

229
00:27:20,590 --> 00:27:29,790
Terrible Photoshop filter. The interesting thing here is that it has started edge detection

230
00:27:29,790 --> 00:27:36,300
And other distractions in our vision that mimic visual processes

231
00:27:36,300 --> 00:27:43,700
Core twelve. This happens naturally when you train a system that can create these

232
00:27:43,700 --> 00:27:50,720
Isolated image classification filter mimics image class tasks

233
00:27:50,720 --> 00:27:55,430
We ourselves realized that it started to do the same thing as we did

234
00:27:55,430 --> 00:28:03,500
Do, this is an interesting validation I think of this model. So I hope this will be popular

235
00:28:03,500 --> 00:28:09,020
Learning is a bit less scary, implementing it is just a whole bunch of operations,

236
00:28:09,020 --> 00:28:24,390
A bunch of adjusted spaghetti codes go through a very simple but big process. our world

237
00:28:24,390 --> 00:28:30,360
Full of information and our interaction comes with it

238
00:28:30,360 --> 00:28:35,380
Machine learning system. The system is not perfect. They have no training

239
00:28:35,380 --> 00:28:40,980
We give them the data, and like them, they internalize the data's biases, and,

240
00:28:40,980 --> 00:28:46,770
Just like us, they can be pushed into the services of anyone who wants to use them. There

241
00:28:46,770 --> 00:28:52,440
This proves that the neural network is a universal approximation, which means that any function you have

242
00:28:52,440 --> 00:29:01,710
Give them they can approach some level of precision if you believe in our own perception

243
00:29:01,710 --> 00:29:06,450
Is a computable function and then we are moving into the basic task of the world

244
00:29:06,450 --> 00:29:14,200
Cognition is now a machine where we can train things. So these are not real.

245
00:29:14,200 --> 00:29:22,600
These are conceived by deep learning networks whose loss function is another network. The

246
00:29:22,600 --> 00:29:30,910
The two networks improve each other and learn what dreams look like.

247
00:29:30,910 --> 00:29:38,260
This is not Barack Obama. This is an actual recording of machine learning Obama synced to

248
00:29:38,260 --> 00:29:45,390
Obama speaks. Existing systems can produce speech that sounds like anyone

249
00:29:45,390 --> 00:29:53,260
it is good. So how do we respond? With the world we cannot believe in our eyes and the world

250
00:29:53,260 --> 00:29:59,360
ear? One way is to ignore it and say that these techniques are not so good

251
00:29:59,360 --> 00:30:07,640
-But. But if cognition is a computable function, then our society and ourselves are games,

252
00:30:07,640 --> 00:30:14,950
It turns into a robot and is very good at playing games. In the history of computing,

253
00:30:14,950 --> 00:30:21,450
We see these trends. So, first of all, very important work is done on a large main frame. then

254
00:30:21,450 --> 00:30:27,851
The processor was improved and work turned to personal computers. Then the network improved and we

255
00:30:27,851 --> 00:30:33,510
Put everything in the cloud. Now we are seeing the tide disappear when we start

256
00:30:33,510 --> 00:30:40,080
Realizing how much power we have given up to know everything about everyone,

257
00:30:40,080 --> 00:30:45,390
How much danger we rely on to feed on picking boxes outside our control

258
00:30:45,390 --> 00:30:51,590
We have knowledge. So the information I give you is that these technologies are not necessarily opaque,

259
00:30:51,590 --> 00:30:56,830
And they do n’t have to concentrate, and we can master the power of robotic thinking

260
00:30:56,830 --> 00:31:05,510
pocket. We can use them to create, not just create forgeries, but previously discern the truth

261
00:31:05,510 --> 00:31:09,780
So this is just the beginning. Everything we see here today, I think it's quite

262
00:31:09,780 --> 00:31:15,870
Impressive, I think it will look downright awkward in a few years

263
00:31:15,870 --> 00:31:21,980
You can talk to euro bot-your robot assistant, your voice mode will never leave

264
00:31:21,980 --> 00:31:24,480
Your wrist. Thank you. [Cheers and applause]. If you are interested, you can find some people here

