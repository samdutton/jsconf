1
00:00:18,930 --> 00:00:25,310
Hello everybody, my name is Iliyan, I am working
as a software engineer for an L.A. based company

2
00:00:25,310 --> 00:00:33,360
called Liferay, and today, I will talk about
HTTP 2.0 and QUIC protocols (of the very new

3
00:00:33,360 --> 00:00:41,420
future), which will replace the protocol which
is currently used by Worldwide Web - HTTP

4
00:00:41,420 --> 00:00:43,260
1.1.

5
00:00:43,260 --> 00:00:49,730
So, first is that question - why do we need
a the new version of HTTP protocol?

6
00:00:49,730 --> 00:00:52,329
What is wrong with the [current version of
the] protocol?

7
00:00:52,329 --> 00:00:57,730
After all it's used by worldwide web since
1990, right?

8
00:00:57,730 --> 00:01:05,899
It works pretty well, it's very simple, actually
browser sends request the server, server responds

9
00:01:05,899 --> 00:01:07,330
and this is it.

10
00:01:07,330 --> 00:01:14,500
Very simple, but if you check this chart which
is just for four years, you'll see that there

11
00:01:14,500 --> 00:01:20,490
is not only one request, and one response,
but there are actually multiple requests and

12
00:01:20,490 --> 00:01:27,170
responses - about 80‑90 for the top 100
sites according to HTTP Archive.

13
00:01:27,170 --> 00:01:32,400
So, this comes to tell us that the Web has
changed drastically, it's a web platform,

14
00:01:32,400 --> 00:01:39,200
it's no more platform for reading different
cross‑linked documents.

15
00:01:39,200 --> 00:01:47,460
And because of that, we have to admit that
HTTP 1.1 has issues.

16
00:01:47,460 --> 00:01:54,850
Those issues are so big that unfortunately
we need a new version of the protocol and

17
00:01:54,850 --> 00:01:59,750
those issues are so many that I will mention
only three of them.

18
00:01:59,750 --> 00:02:03,159
The first one is that it's very latency sensitive.

19
00:02:03,159 --> 00:02:11,060
Right, imagine we have a server in L.A. and
our client is right here in Berlin, all those

20
00:02:11,060 --> 00:02:15,590
requests should go from Berlin to L.A. and
then get back.

21
00:02:15,590 --> 00:02:24,000
This is very expensive, and, latency and protocol
is very latency sensitive, right.

22
00:02:24,000 --> 00:02:26,450
Apart from that, the specification is huge.

23
00:02:26,450 --> 00:02:31,680
The original specification has been replaced
by ability ten different RFCs for message

24
00:02:31,680 --> 00:02:36,750
syntax for semantics and content and so on.

25
00:02:36,750 --> 00:02:43,440
There are optional parts, like HTTP pipelining,
which are sometimes implemented, sometimes

26
00:02:43,440 --> 00:02:46,489
they are not.

27
00:02:46,489 --> 00:02:53,590
And more - like buggy proxy servers, so this
is the key - latency kills and bandwidth is

28
00:02:53,590 --> 00:02:59,940
not everything, if you check this chart from,
I guess, most of you are very well aware of

29
00:02:59,940 --> 00:03:06,590
it already, from some point how fast is your
network is your Internet at home for daily

30
00:03:06,590 --> 00:03:11,620
browsing it doesn't matter, if you have about
five megabits, it's pretty much enough.

31
00:03:11,620 --> 00:03:18,590
I'm not talking about streaming or downloading
huge video files, but for daily browsing.

32
00:03:18,590 --> 00:03:23,120
What matters is the round trip time.

33
00:03:23,120 --> 00:03:30,160
For every 20 milliseconds, as you see, the
page loading time increases.

34
00:03:30,160 --> 00:03:37,050
So, we are stuck by latency, so what could
be the solution?

35
00:03:37,050 --> 00:03:42,799
It's obvious that sending one request and
returning responses is not enough, so what

36
00:03:42,799 --> 00:03:44,670
could be the solution?

37
00:03:44,670 --> 00:03:48,280
One possible solution could be HTTP pipelining,
right?

38
00:03:48,280 --> 00:03:53,650
Since that's exactly it's purpose, instead
to send one request to the server and waiting

39
00:03:53,650 --> 00:04:00,519
for the response, browser sends three or more
requests and waiting for three or more responses.

40
00:04:00,519 --> 00:04:09,730
Sounds very good in theory, but unfortunately,
it's not like this, that's not the solution.

41
00:04:09,730 --> 00:04:11,859
So why not?

42
00:04:11,859 --> 00:04:17,699
It's not because by the specification the
server must sends it's responses in the same

43
00:04:17,699 --> 00:04:21,800
order that requests were received.

44
00:04:21,800 --> 00:04:22,800
So this is it.

45
00:04:22,800 --> 00:04:28,680
In fact, the entire connection remains first
in and first out, and head of line blocking

46
00:04:28,680 --> 00:04:30,729
can occur.

47
00:04:30,729 --> 00:04:32,120
What exactly this means?

48
00:04:32,120 --> 00:04:38,560
Imagine we have three questions and the first
of them requires some heavy database request

49
00:04:38,560 --> 00:04:39,560
on the server, right.

50
00:04:39,560 --> 00:04:48,360
The rest, the rest of those could be just
one JavaScript and one CSS file and they will

51
00:04:48,360 --> 00:04:49,360
be blocked.

52
00:04:49,360 --> 00:04:57,240
There are more buggy proxy servers which are
not properly implementing HTTP pipelining.

53
00:04:57,240 --> 00:05:06,460
And as a result, in most browsers, HTTP pipelining
is disabled or not implemented at all.

54
00:05:06,460 --> 00:05:10,300
So keeping that in mind, what could be\h‑‑
what could browsers do?

55
00:05:10,300 --> 00:05:14,669
What can they do in order to improve the performance?

56
00:05:14,669 --> 00:05:21,460
There is no other way except to achieve multiplexing
by opening multiple connections to the servers

57
00:05:21,460 --> 00:05:24,900
and that's exactly what they do.

58
00:05:24,900 --> 00:05:26,939
So this is the one side of the question.

59
00:05:26,939 --> 00:05:35,120
The second side or the next side is that developers,
as very creative people, they implemented

60
00:05:35,120 --> 00:05:39,210
a lot of workarounds, so‑called optimizations.

61
00:05:39,210 --> 00:05:44,419
Most of you will be very well aware of them,
they are creating images sprites, sharding,

62
00:05:44,419 --> 00:05:52,719
resolution sharding, resource inlining, concatenating
file, combo services, preloading service when

63
00:05:52,719 --> 00:05:59,210
the user is idle, reducing cookie size or
even using cookie‑free domains exactly for

64
00:05:59,210 --> 00:06:04,969
that purpose, using link instead @import,
to pack components into multipart documents

65
00:06:04,969 --> 00:06:06,060
and so on.

66
00:06:06,060 --> 00:06:13,690
In fact, a whole industry has been created
to deal with those issues, sites have been

67
00:06:13,690 --> 00:06:20,939
created, performance groups in the big companies
like Google and Yahoo have been created, but

68
00:06:20,939 --> 00:06:28,830
if you're a regular developer, and as soon
as you read all those guidelines, you start

69
00:06:28,830 --> 00:06:32,199
looking like this ...

70
00:06:32,199 --> 00:06:33,419
Why should I do all that?

71
00:06:33,419 --> 00:06:35,440
Why is that needed?

72
00:06:35,440 --> 00:06:45,090
And, if you also got tired of all that, then,
say welcome to HTTP 2.0 and QUIC protocols.

73
00:06:45,090 --> 00:06:52,960
HTTP 2.0 which will replace 1.1 is currently
on draft 14.

74
00:06:52,960 --> 00:06:58,939
It's being actively developed and not yet
finalized, it will be probably this year or

75
00:06:58,939 --> 00:07:01,719
maybe the next one.

76
00:07:01,719 --> 00:07:10,710
And, it's purpose or it's objective is very
simple: To fix the issues in version 1.1,

77
00:07:10,710 --> 00:07:14,080
but without breaking the web.

78
00:07:14,080 --> 00:07:21,520
Sounds very ambitious and this protocol is
actually based on SPDY which has been developed

79
00:07:21,520 --> 00:07:25,479
by Google in 2009.

80
00:07:25,479 --> 00:07:35,540
The very first draft of HTTP 2.0 version 0
was based on the latest specification of SPDY.

81
00:07:35,540 --> 00:07:40,499
So, going inside to the protocol, how browsers
switches to that protocol?

82
00:07:40,499 --> 00:07:51,150
There is HTTP and HTTPS, and if we switch
from HTTP we use the upgrade mechanism, however

83
00:07:51,150 --> 00:07:53,370
this means a round trip.

84
00:07:53,370 --> 00:07:57,509
That's why it's better to look for another
solution.

85
00:07:57,509 --> 00:08:06,750
And if we switch from HTTPS to HTTP 2.0, an
Application Layer Protocol Negotiation Extension

86
00:08:06,750 --> 00:08:12,620
has been used, which avoids this draw back.

87
00:08:12,620 --> 00:08:21,259
The protocol has many exciting features, and
I will note some of them.

88
00:08:21,259 --> 00:08:26,240
The main difference with version 1.1 is that
it's binary protocol.

89
00:08:26,240 --> 00:08:28,050
It's not text one.

90
00:08:28,050 --> 00:08:33,750
So, for browsers, it makes huge difference
for us, it makes that if you use telnet to

91
00:08:33,750 --> 00:08:39,789
look to some server, to log in to some server
that won't be possible anymore, but because

92
00:08:39,789 --> 00:08:45,610
it's binary protocol it's much easier a parser
to be created and if you are NodeJS developer,

93
00:08:45,610 --> 00:08:53,240
and you want to create your own server it
will be much easier for you to create a parser

94
00:08:53,240 --> 00:08:56,740
based on this binary protocol.

95
00:08:56,740 --> 00:09:02,690
There won't be a need to deal with string
limited, with strings at all.

96
00:09:02,690 --> 00:09:11,190
On the very low level browser and server exchange
frames and each frame belongs to a stream.

97
00:09:11,190 --> 00:09:16,899
If there is a frame, which doesn't belong
to a stream, that's protocol error.

98
00:09:16,899 --> 00:09:21,660
What is really important is that those streams
are multiplexed.

99
00:09:21,660 --> 00:09:27,800
We have one single connection, but multiple
streams, they are multiplexed and they also

100
00:09:27,800 --> 00:09:29,850
have priorities.

101
00:09:29,850 --> 00:09:34,709
This is really important - as you know, every
page there are some resources which are critical

102
00:09:34,709 --> 00:09:37,769
for rendering the page and there are others
which are not.

103
00:09:37,769 --> 00:09:45,160
CSS [files] are critical, JavaScript could
be critical if the page requires JavaScript,

104
00:09:45,160 --> 00:09:46,940
but the images usually are not.

105
00:09:46,940 --> 00:09:54,199
So, now, we can balance with those different
streams.

106
00:09:54,199 --> 00:10:00,800
Something which we were doing for years was
called resource inlining, now it's on the

107
00:10:00,800 --> 00:10:03,620
protocol level and it's called server push.

108
00:10:03,620 --> 00:10:13,029
Again, if I have a NodeJS server, I can check
if the browser allows server push and to push

109
00:10:13,029 --> 00:10:21,660
my critical resources when the page is being
loaded, right, instead to put everything on

110
00:10:21,660 --> 00:10:27,240
the page, and to deal with caches and something
like this.

111
00:10:27,240 --> 00:10:32,790
This is what we have to emphasize: One connection
to the server that should be enough, and not

112
00:10:32,790 --> 00:10:36,930
six connection per domain as most browsers
do now.

113
00:10:36,930 --> 00:10:41,500
Some of them actually open more than six connections.

114
00:10:41,500 --> 00:10:49,350
But, in general, this is not good for no one,
neither for the browser nor for the server.

115
00:10:49,350 --> 00:10:57,250
The browser has to deal with memory, to balance
those connections and that's not good.

116
00:10:57,250 --> 00:11:03,680
Now, just one connection, we go back as to
the point, as it should be.

117
00:11:03,680 --> 00:11:10,180
As I said, frames - that's on the very low
level.

118
00:11:10,180 --> 00:11:13,399
And this is how a frame looks like:

119
00:11:13,399 --> 00:11:16,839
We want to take a look on all of this one
by one.

120
00:11:16,839 --> 00:11:24,130
I will mention just two of those, the type
of the frame, and stream identifier.

121
00:11:24,130 --> 00:11:28,899
Every page has stream identifier, so as I
said [if there is no stream identifier] this

122
00:11:28,899 --> 00:11:31,610
would be protocol error.

123
00:11:31,610 --> 00:11:38,240
There are about ten different frame types
currently in the latest draft.

124
00:11:38,240 --> 00:11:43,960
Like data frame, which conveys arbitrary data
associated with a stream there are frames

125
00:11:43,960 --> 00:11:51,870
which deal with headers, some of those are
related to stream priority, some of them are

126
00:11:51,870 --> 00:11:57,300
related to promises, to push promises, this
is related server push.

127
00:11:57,300 --> 00:12:04,300
For some of them the purpose is to reset a
stream - just abort the stream, I don't want

128
00:12:04,300 --> 00:12:08,220
the see it anymore.

129
00:12:08,220 --> 00:12:16,380
And those frames are grouped in streams, so
streams, by definition is logical bidirectional

130
00:12:16,380 --> 00:12:23,500
sequence of frames, frames which are being
exchanged between the browser and the server.

131
00:12:23,500 --> 00:12:33,180
So, one single connection can has multiple
open streams, like in this example, you may\h‑‑

132
00:12:33,180 --> 00:12:36,560
you may see that we currently have three streams.

133
00:12:36,560 --> 00:12:39,269
Stream one, stream two, and stream four.

134
00:12:39,269 --> 00:12:45,420
So, there are\h‑‑ they all contain some
frames, and they are multiplexed.

135
00:12:45,420 --> 00:12:52,490
One stream may represent an HTML page, another
stream may represent a JavaScript file or

136
00:12:52,490 --> 00:12:59,199
a third one some style, but they're multiplexed
and prioritized by the browser.

137
00:12:59,199 --> 00:13:03,639
So, going to priorities:

138
00:13:03,639 --> 00:13:11,730
Each stream has priority, that priority is
specified by the browser and what is important

139
00:13:11,730 --> 00:13:15,660
is that priority can be changed on runtime.

140
00:13:15,660 --> 00:13:21,510
So, the exact order in which frames are delivered
can be further optimized.

141
00:13:21,510 --> 00:13:29,300
The priorities are optional, 31 birth value,
and stream with priority 0 has the highest

142
00:13:29,300 --> 00:13:31,310
priority.

143
00:13:31,310 --> 00:13:33,330
Right ...

144
00:13:33,330 --> 00:13:40,940
streams also have dependencies, and like in
this example we have two streams B and C which

145
00:13:40,940 --> 00:13:48,170
depend on A, so if we have another stream
called D, and it just has priority, it will

146
00:13:48,170 --> 00:13:57,620
have the same value as B and C, however, if
we say that D depends on A, then B and C will

147
00:13:57,620 --> 00:14:04,160
start depending on the D too.

148
00:14:04,160 --> 00:14:11,459
Another key part of the protocol is related
to headers, as I said, we cannot break the

149
00:14:11,459 --> 00:14:13,360
web, right, this is the deal.

150
00:14:13,360 --> 00:14:17,290
So, HTTP 2.0 is stateless protocol too.

151
00:14:17,290 --> 00:14:24,800
Nothing has change changed here, the client
still has to send data to to the server, however,

152
00:14:24,800 --> 00:14:26,670
there is a big difference:

153
00:14:26,670 --> 00:14:32,190
The headers in version 2.0 are now compressed.

154
00:14:32,190 --> 00:14:36,970
This is the big difference.

155
00:14:36,970 --> 00:14:41,610
Just a few words about the compression, it's
stateful, it's not stateless, which means

156
00:14:41,610 --> 00:14:49,139
that a single compression context for the
entire connection is used, and the algorithm

157
00:14:49,139 --> 00:15:00,750
is especially designed for the protocol, it's
called HPAC because of some evil guys who

158
00:15:00,750 --> 00:15:08,610
applied attacks like CRIME and BREACH, so
we have a specially designed compression algorithm

159
00:15:08,610 --> 00:15:11,740
for the protocol.

160
00:15:11,740 --> 00:15:17,269
Going back to what we know very well, server
push, we did that for years, nothing new here,

161
00:15:17,269 --> 00:15:24,820
I guess all of you applied or embedded some
resources to the page - CSS or JavaScript

162
00:15:24,820 --> 00:15:27,839
or SVG impage or what ever.

163
00:15:27,839 --> 00:15:34,850
So here, this is the difference - server pre‑emptively
sends resources to a client in association

164
00:15:34,850 --> 00:15:38,129
with the previous client‑initiated request.

165
00:15:38,129 --> 00:15:44,700
So, I'm requesting indexed HTML on the server
side, me as JavaScript developer who works

166
00:15:44,700 --> 00:15:50,569
on NodeJS server, I can push the critical
resources which are needed for the page to

167
00:15:50,569 --> 00:15:53,319
be rendered, right?

168
00:15:53,319 --> 00:16:00,209
Of there is, this is a trick here - the client
explicitly must allow it.

169
00:16:00,209 --> 00:16:05,320
So, on the server side when we implement our
servers, we have to check if that is allowed

170
00:16:05,320 --> 00:16:11,139
otherwise that could [lead] to a reset stream
or something like this.

171
00:16:11,139 --> 00:16:21,540
A client cannot push, which means the browser
cannot push resources to the server - only

172
00:16:21,540 --> 00:16:26,089
the server is allowed to do that .

173
00:16:26,089 --> 00:16:28,990
So - very briefly, this is it.

174
00:16:28,990 --> 00:16:31,410
HTTP 2 Rulez, right?

175
00:16:31,410 --> 00:16:39,560
Because now as developers we have full power
to apply different kind of optimizations,

176
00:16:39,560 --> 00:16:46,209
different than those which we are currently
doing, or we can apply more - we can get heuristics

177
00:16:46,209 --> 00:16:53,190
from how the user behaves with the page [in
order] to push resources, to optimize our

178
00:16:53,190 --> 00:16:57,540
pages in different way.

179
00:16:57,540 --> 00:17:02,970
This is not the end because now we have\h‑‑
because the research continues.

180
00:17:02,970 --> 00:17:08,940
We have another protocol, also started by
Google, research by Google, it's called QUIC.

181
00:17:08,940 --> 00:17:23,170
QUIC is a natural extension of SPDY and HTTP
2.0 research, this is also multiplexing transport

182
00:17:23,170 --> 00:17:29,900
protocol but there is a big difference - this
protocol runs on top of UDP, it doesn't run

183
00:17:29,900 --> 00:17:36,760
on top of TCP as SPDY and HTTP 2.0 do.

184
00:17:36,760 --> 00:17:41,130
This protocol runs on top of UDP.

185
00:17:41,130 --> 00:17:48,500
Because it runs on top of UDP, the browsers
cannot use those out of the box features which

186
00:17:48,500 --> 00:17:51,390
are available in TCP, right?

187
00:17:51,390 --> 00:17:55,900
So those should be implemented on application
level.

188
00:17:55,900 --> 00:17:57,100
So why should do we that?

189
00:17:57,100 --> 00:18:00,660
Because it's very hard to update TCP.

190
00:18:00,660 --> 00:18:07,380
It's everywhere, it's on servers, routers,
it needs years until it's being updated.

191
00:18:07,380 --> 00:18:15,640
But now we can apply the results of the research,
how to improve the web on QUIC, Google applied

192
00:18:15,640 --> 00:18:21,440
to their servers, applied to Chrome and now
everything is clear.

193
00:18:21,440 --> 00:18:27,950
You can see how QUIC works right now in your
browser, there is a flag, you can enable it

194
00:18:27,950 --> 00:18:28,950
if you want.

195
00:18:28,950 --> 00:18:34,100
I actually do it to see how it's going.

196
00:18:34,100 --> 00:18:43,300
So, that's not the first time when people
are trying to fix the issues of using UDP,

197
00:18:43,300 --> 00:18:50,410
people haven't slept those years, and there
were already existing solutions, like, SCTP

198
00:18:50,410 --> 00:18:56,720
over DTLS, that's one possible solution, those
are two different protocols.

199
00:18:56,720 --> 00:19:03,280
After all SCTP provides, among other thing,
stream multiplexing, right?

200
00:19:03,280 --> 00:19:09,260
DTLS provides SSL quality encryption and authentication
over UDP stream.

201
00:19:09,260 --> 00:19:14,320
So why not use those protocols?

202
00:19:14,320 --> 00:19:22,560
The answer is very simple, because roughly
four round trips are needed to SCTP over DTLS

203
00:19:22,560 --> 00:19:23,560
connection.

204
00:19:23,560 --> 00:19:30,340
You can imagine how expensive, again with
the expensive of Berlin client and L.A. server,

205
00:19:30,340 --> 00:19:31,530
that's too expensive.

206
00:19:31,530 --> 00:19:38,280
In contrast, the goal of QUIC is to perform
a connection establishment with zero round

207
00:19:38,280 --> 00:19:39,790
trip time overhead.

208
00:19:39,790 --> 00:19:43,560
This is the goal.

209
00:19:43,560 --> 00:19:51,810
QUIC has all the benefits of SPDY and HTTP
2.0, but, there there is a big difference,

210
00:19:51,810 --> 00:19:57,710
there is no head‑of‑line blocking in QUIC!

211
00:19:57,710 --> 00:20:04,500
As you can imagine, delaying of only one packet
causes the entire set of SPDY streams to pause,

212
00:20:04,500 --> 00:20:11,700
HTTP/2 streams to pause because TCP only provides
a single serialized stream interface, but

213
00:20:11,700 --> 00:20:18,320
in QUIC when a single packet is lost, only
one stream is being delayed.

214
00:20:18,320 --> 00:20:24,250
We can illustrate that with an image, and
here is how does it look like.

215
00:20:24,250 --> 00:20:31,850
Here we have three streams, with red, green,
and blue color, and if a packet in the stream

216
00:20:31,850 --> 00:20:41,900
marked with red color is being lost, the rest
two streams are not being affected.

217
00:20:41,900 --> 00:20:48,100
Here we can compare how QUIC behaves versus
TCP + TLS.

218
00:20:48,100 --> 00:20:55,420
For repeat connection, it's zero milliseconds
round trip time, right, for TCP plus TLS it's

219
00:20:55,420 --> 00:20:56,620
200 ms.

220
00:20:56,620 --> 00:21:03,120
For new connectio with QUIC it's 100ms, for
TCP + TLS it is 300 milliseconds, that's too

221
00:21:03,120 --> 00:21:04,120
much.

222
00:21:04,120 --> 00:21:11,540
As I sad this protocol rests on top of UDP,
so many of the out of the box features in

223
00:21:11,540 --> 00:21:20,000
TCP are now not available so they have to
be implemented on the application level.

224
00:21:20,000 --> 00:21:26,200
QUIC also provides encryption, which is compatible
to TLS, but with more efficient handshake.

225
00:21:26,200 --> 00:21:33,290
It has replay attack and IP spoofing protection,
and it also has something very important - in

226
00:21:33,290 --> 00:21:40,260
order to minimize all those round trips, which
really decreases the performance, there is

227
00:21:40,260 --> 00:21:44,970
forward error correction, which means on the
price of an additional packet (for example),

228
00:21:44,970 --> 00:21:53,040
we can restore a packet which has been lost,
so the server won't have to send this packet

229
00:21:53,040 --> 00:21:54,550
again.

230
00:21:54,550 --> 00:22:01,600
A really cool feature for us, like especially
for us who use mobile phones, if we\h‑‑

231
00:22:01,600 --> 00:22:09,620
that the communication channels are not defined
by IP plus port, but by an ID, so, in this\h‑‑

232
00:22:09,620 --> 00:22:18,260
in this room there is WiFi and if you leave
the WiFi and go outside of this building for

233
00:22:18,260 --> 00:22:26,730
using mobile Internet, that one\h‑‑ that
won't be so bad because the connection continues.

234
00:22:26,730 --> 00:22:27,730
So ...

235
00:22:27,730 --> 00:22:35,020
all this sounds very cool, very exciting,
and I wanted to see what is the current status

236
00:22:35,020 --> 00:22:41,960
of those two protocols where we are going\h‑‑
how are we going, especially for this presentation,

237
00:22:41,960 --> 00:22:50,890
I created the small site, which is\h‑‑
which has really cool name HTTPRulez.com,

238
00:22:50,890 --> 00:22:57,050
it consists on the two pages, on the first
page we have\h‑‑ on the first page we

239
00:22:57,050 --> 00:23:06,490
have.. it represents an online shop with CSS
JavaScript and images and the second page

240
00:23:06,490 --> 00:23:11,110
I have a few web components, they are supposed
to be the future of web development, so I

241
00:23:11,110 --> 00:23:19,320
wanted to see if we serve the content via
different protocols it will make some difference.

242
00:23:19,320 --> 00:23:20,610
So ...

243
00:23:20,610 --> 00:23:26,900
from my point of view that site is like Open
Source platform for testing different optimizations

244
00:23:26,900 --> 00:23:34,240
strategies for HTTP 2.0, there is\h‑‑
this is the idea, same content, but, served

245
00:23:34,240 --> 00:23:43,040
via different end points, via HTTP, HTTPS,
SPDY and HTTP 2, that's really cool, isn't

246
00:23:43,040 --> 00:23:44,040
it?

247
00:23:44,040 --> 00:23:48,001
I don't have QUIC end point yet, because there
is no production ready server, not this there

248
00:23:48,001 --> 00:23:55,530
is HTTP 2.0 production server - I implemented
this on NodeJS, so I had to build version

249
00:23:55,530 --> 00:24:00,770
of Node with ALPN support, but it currently
works.

250
00:24:00,770 --> 00:24:12,910
On each page we can see the load time so we
can compare, and because we have great tools

251
00:24:12,910 --> 00:24:19,230
like Webpage, now we can check what is the
difference\h‑‑ I intentionally did not

252
00:24:19,230 --> 00:24:24,010
apply any of those dark workarounds we were
doing for years.

253
00:24:24,010 --> 00:24:28,880
I did not create any sprite for images, there
are about 30 images, I did not create any

254
00:24:28,880 --> 00:24:39,360
sprite, I wanted to start with something simple
and see how it behaves if we avoid all of

255
00:24:39,360 --> 00:24:40,360
that.

256
00:24:40,360 --> 00:24:43,110
On top we can implement further optimizations.

257
00:24:43,110 --> 00:24:52,041
So, this is what HTT\h‑‑ what webpagepast
shows as waterfall when we use HTTPS, this

258
00:24:52,041 --> 00:25:00,260
is the waterfall, you can see how it looks
like, you can also see that the browser Chrome

259
00:25:00,260 --> 00:25:06,960
opened six connections for this domain and
now for each connection we have initializing,

260
00:25:06,960 --> 00:25:17,680
we have SSL negotiation, which is expensive,
TCP needs three round trips, right, so this

261
00:25:17,680 --> 00:25:19,370
is not so good.

262
00:25:19,370 --> 00:25:23,650
Six connections, you can imagine how much
time we are losing for [establishing] all

263
00:25:23,650 --> 00:25:26,000
those connections.

264
00:25:26,000 --> 00:25:35,770
And here I have a waterfall, again with webpagetest,
which represents HTTP/2 waterfall without

265
00:25:35,770 --> 00:25:37,240
server push.

266
00:25:37,240 --> 00:25:42,820
On the server, I haven't pushed any of those
resources, I just relied on the browser to

267
00:25:42,820 --> 00:25:44,240
do its best.

268
00:25:44,240 --> 00:25:46,540
Currently this only works on Canary.

269
00:25:46,540 --> 00:25:51,590
It doesn't work on Chrome, I hope in a few
months that it will be available in Chrome

270
00:25:51,590 --> 00:25:59,480
too, but for now we have to use Canary and
apply a special flag - if you go to chrome://flags/

271
00:25:59,480 --> 00:26:08,340
you is to enable SPDY/4 which is the internal
name of HTTP/2 for Canary.

272
00:26:08,340 --> 00:26:14,570
But, now it works, it supports draft 14, which
is the latest one so that is really cool.

273
00:26:14,570 --> 00:26:22,900
Here you can see that the browser created
only one connection to the server, not six,

274
00:26:22,900 --> 00:26:23,900
this is the big deal.

275
00:26:23,900 --> 00:26:30,140
All those resources were multiplexed and sent
to the browser via just one connection.

276
00:26:30,140 --> 00:26:33,090
This is huge improvement.

277
00:26:33,090 --> 00:26:38,670
But I wanted to see how it behaves if I push
some resources so here is the waterfall, what

278
00:26:38,670 --> 00:26:45,860
currently webpagetest shows, probably that
will change because if you open Canary Dev

279
00:26:45,860 --> 00:26:52,581
tools you can see all these resources, but
in webpagetest it looks like if the browser

280
00:26:52,581 --> 00:26:59,010
made only one request to the server which
is index HTML or slash (/), but not - actually

281
00:26:59,010 --> 00:27:06,770
I intentionally pushed all CSS, JavaScript
and images files when the browser requests

282
00:27:06,770 --> 00:27:10,530
index HTML page.

283
00:27:10,530 --> 00:27:11,590
So ...

284
00:27:11,590 --> 00:27:12,590
again ...

285
00:27:12,590 --> 00:27:24,780
we have connection view, it shows only one
connection, not six.

286
00:27:24,780 --> 00:27:30,080
If we have to draw the line, I would like
to say the following:

287
00:27:30,080 --> 00:27:36,970
If you remember the workarounds, which we
did to speed up our sites for years, all those

288
00:27:36,970 --> 00:27:46,250
workarounds... ask yourself how HTTP 2.0 will
affect the way you develop and optimize web

289
00:27:46,250 --> 00:27:50,280
applications for you or for your company.

