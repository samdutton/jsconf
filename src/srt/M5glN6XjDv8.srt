1
00:00:12,379 --> 00:00:15,629

thank you guys I was here last year

2
00:00:15,629 --> 00:00:18,599
giving my first ever talk a lightning

3
00:00:18,599 --> 00:00:21,720
talks around last J Hong Kong so I'm

4
00:00:21,720 --> 00:00:24,390
really glad that I can be here again

5
00:00:24,390 --> 00:00:26,130
this year and give this talk to you guys

6
00:00:26,130 --> 00:00:29,490
and yeah I'm late to the gym no party

7
00:00:29,490 --> 00:00:34,290
I'm going concluding this party today so

8
00:00:34,290 --> 00:00:37,980
let's start that's me I'm a developer

9
00:00:37,980 --> 00:00:41,190
Jim do I am NOT a machine learning

10
00:00:41,190 --> 00:00:43,890
expert or a data scientist data engineer

11
00:00:43,890 --> 00:00:47,510
I'm neither of those I just did that

12
00:00:47,510 --> 00:00:50,369
project in my free time because I wanted

13
00:00:50,369 --> 00:00:52,830
to so if you have specific machine

14
00:00:52,830 --> 00:00:54,180
learning questions please don't direct

15
00:00:54,180 --> 00:00:57,320
them to me ask your local data engineer

16
00:00:57,320 --> 00:01:01,140
you can also reach me on twitter at

17
00:01:01,140 --> 00:01:05,339
jimmy 40 to get up and if anyone's still

18
00:01:05,339 --> 00:01:08,070
on Google+ just check if I'm there too I

19
00:01:08,070 --> 00:01:12,090
think so okay so let's start with a

20
00:01:12,090 --> 00:01:14,910
funny quote this is actually a former

21
00:01:14,910 --> 00:01:17,370
football player and he said I never

22
00:01:17,370 --> 00:01:21,240
predict anything and I never will so

23
00:01:21,240 --> 00:01:24,210
neither am i I already put the

24
00:01:24,210 --> 00:01:26,870
predictions up for this match today

25
00:01:26,870 --> 00:01:31,770
today tomorrow yesterday I put them on

26
00:01:31,770 --> 00:01:34,530
the back side of this wall I think and I

27
00:01:34,530 --> 00:01:36,720
will be updating the results as they

28
00:01:36,720 --> 00:01:38,760
come in I think the game will start in

29
00:01:38,760 --> 00:01:43,050
15 minutes so let's see how well my

30
00:01:43,050 --> 00:01:46,920
neural network actually does things okay

31
00:01:46,920 --> 00:01:49,590
so first things first we have to

32
00:01:49,590 --> 00:01:51,690
understand neural networks and this will

33
00:01:51,690 --> 00:01:54,900
conclude some math but I hope that most

34
00:01:54,900 --> 00:01:56,760
of you will get it because it's really

35
00:01:56,760 --> 00:01:57,420
not that hard

36
00:01:57,420 --> 00:02:00,720
but let's look at neurons first those

37
00:02:00,720 --> 00:02:03,860
are naturally occurring neurons - and

38
00:02:03,860 --> 00:02:07,170
the sneaky fuzzy stuff to the left are

39
00:02:07,170 --> 00:02:09,959
dendrites dendrites received the input

40
00:02:09,959 --> 00:02:13,470
then the neuron does stuff with it sends

41
00:02:13,470 --> 00:02:15,569
a signal to the next dendrite via the

42
00:02:15,569 --> 00:02:17,969
axon and then the whole thing is done

43
00:02:17,969 --> 00:02:20,790
again and the very interesting thing

44
00:02:20,790 --> 00:02:22,060
about a neuron

45
00:02:22,060 --> 00:02:24,849
is that neurons could do which to one

46
00:02:24,849 --> 00:02:26,800
thing can learn to do another thing so

47
00:02:26,800 --> 00:02:28,540
let's say those are neurons for seeing

48
00:02:28,540 --> 00:02:31,720
and then the person gets blind for some

49
00:02:31,720 --> 00:02:34,030
reason so those neurons aren't useless

50
00:02:34,030 --> 00:02:36,640
now because they can learn to hear for

51
00:02:36,640 --> 00:02:39,310
example and they will just get some

52
00:02:39,310 --> 00:02:42,220
input do some stuff and then just send

53
00:02:42,220 --> 00:02:43,959
another signal to the next neuron and it

54
00:02:43,959 --> 00:02:46,170
doesn't matter really what the signal is

55
00:02:46,170 --> 00:02:49,930
so the thing how it how neurons do that

56
00:02:49,930 --> 00:02:52,420
is by experience so they they get a lot

57
00:02:52,420 --> 00:02:54,730
of input over time and they there's a

58
00:02:54,730 --> 00:02:56,830
lot of repetition and iteration and by

59
00:02:56,830 --> 00:03:00,220
by that neurons learn to see the pattern

60
00:03:00,220 --> 00:03:03,180
or the logic behind things

61
00:03:03,180 --> 00:03:06,400
so what neurons do need is a variety of

62
00:03:06,400 --> 00:03:09,400
training training examples and that's

63
00:03:09,400 --> 00:03:13,239
the most important thing so let's look

64
00:03:13,239 --> 00:03:15,670
at an artificial Network that is a

65
00:03:15,670 --> 00:03:18,519
simple multi-layer network we have three

66
00:03:18,519 --> 00:03:21,450
layers here the first is the input layer

67
00:03:21,450 --> 00:03:24,700
that's where we receive our input then

68
00:03:24,700 --> 00:03:27,880
we have a hidden layer I just chose to

69
00:03:27,880 --> 00:03:29,350
have one hidden layer it doesn't really

70
00:03:29,350 --> 00:03:31,989
matter or you can just try it out what

71
00:03:31,989 --> 00:03:34,570
works for you and then we have an output

72
00:03:34,570 --> 00:03:38,290
layer so in in this graph the arrows

73
00:03:38,290 --> 00:03:40,930
would be our dendrites or more axons and

74
00:03:40,930 --> 00:03:45,549
the circles would be our neurons okay so

75
00:03:45,549 --> 00:03:49,600
let's look at it more closely so in the

76
00:03:49,600 --> 00:03:51,220
first layer we get our input like I said

77
00:03:51,220 --> 00:03:54,069
we call those X for the purpose of

78
00:03:54,069 --> 00:03:55,870
things so we have three different inputs

79
00:03:55,870 --> 00:04:01,090
two different features X 1 2 X 3 and if

80
00:04:01,090 --> 00:04:02,769
we look at the first neuron or the first

81
00:04:02,769 --> 00:04:06,489
layer we see that it puts out 3

82
00:04:06,489 --> 00:04:10,750
different values from the neuron so this

83
00:04:10,750 --> 00:04:12,850
is the math part I was talking about I

84
00:04:12,850 --> 00:04:15,160
put a legend to the side so you remember

85
00:04:15,160 --> 00:04:17,049
what I'm talking about so X 1 would be

86
00:04:17,049 --> 00:04:20,650
our input and theta 1 1 1 is just a

87
00:04:20,650 --> 00:04:22,930
weight so theta in this case is a matrix

88
00:04:22,930 --> 00:04:25,180
if you don't know what a matrix is just

89
00:04:25,180 --> 00:04:26,860
imagine it as being some kind of

90
00:04:26,860 --> 00:04:29,710
JavaScript object and the 1 1 1 would be

91
00:04:29,710 --> 00:04:32,169
the keys to access the value behind so I

92
00:04:32,169 --> 00:04:35,020
could just say x1 times

93
00:04:35,020 --> 00:04:40,569
or whatever pony unicorn and so then we

94
00:04:40,569 --> 00:04:42,970
send another signal x1 times another

95
00:04:42,970 --> 00:04:46,360
wait and then another and the way that

96
00:04:46,360 --> 00:04:49,000
we choose theta here the weight is

97
00:04:49,000 --> 00:04:51,819
actually that for our initial run for

98
00:04:51,819 --> 00:04:53,440
the first iteration we just choose it

99
00:04:53,440 --> 00:04:56,349
randomly just we don't know what the way

100
00:04:56,349 --> 00:04:58,780
it would be and we just initiated

101
00:04:58,780 --> 00:05:02,380
randomly okay so if we look at the first

102
00:05:02,380 --> 00:05:04,000
neuron the hidden layer and the second

103
00:05:04,000 --> 00:05:06,940
layer then we see we get three different

104
00:05:06,940 --> 00:05:09,160
features multiplied with three different

105
00:05:09,160 --> 00:05:13,210
weights so we get X 1 times something X

106
00:05:13,210 --> 00:05:15,550
2 times something and X 3 times

107
00:05:15,550 --> 00:05:17,650
something so this neuron actually gets

108
00:05:17,650 --> 00:05:20,919
all the input with some weight and then

109
00:05:20,919 --> 00:05:24,550
it just sums it all up so it just adds

110
00:05:24,550 --> 00:05:27,789
the three input values and then it does

111
00:05:27,789 --> 00:05:28,479
something

112
00:05:28,479 --> 00:05:30,580
activate there's in fact of an

113
00:05:30,580 --> 00:05:32,830
activation function and that is the

114
00:05:32,830 --> 00:05:34,479
magic that's happening so there are

115
00:05:34,479 --> 00:05:36,970
different activation functions I won't

116
00:05:36,970 --> 00:05:38,440
go into them because that takes a lot of

117
00:05:38,440 --> 00:05:41,770
math important to know it's just that we

118
00:05:41,770 --> 00:05:44,680
sum up all the input and then do stuff

119
00:05:44,680 --> 00:05:47,889
with it and then get to our new value

120
00:05:47,889 --> 00:05:51,250
which would be a 1/2 but again this is

121
00:05:51,250 --> 00:05:52,990
just a value in a matrix doesn't really

122
00:05:52,990 --> 00:05:57,099
matter what it says so the same thing

123
00:05:57,099 --> 00:05:59,320
actually also happens in our last layer

124
00:05:59,320 --> 00:06:00,370
now output layer

125
00:06:00,370 --> 00:06:03,310
we receive our input so we get now have

126
00:06:03,310 --> 00:06:07,539
a 1 to a 2 2 and a 3 2 and then again

127
00:06:07,539 --> 00:06:10,830
some Thetas again initially random

128
00:06:10,830 --> 00:06:15,280
randomly initialized and then we get our

129
00:06:15,280 --> 00:06:19,930
last value a 1 3 which is just the sum

130
00:06:19,930 --> 00:06:21,759
of everything we have and then we

131
00:06:21,759 --> 00:06:26,139
activate it with something ok so what we

132
00:06:26,139 --> 00:06:28,810
have in the end is we have values we do

133
00:06:28,810 --> 00:06:31,060
stuff with it get new values to do stuff

134
00:06:31,060 --> 00:06:33,820
again then get a third value that is the

135
00:06:33,820 --> 00:06:36,490
result of our hypothesis so our

136
00:06:36,490 --> 00:06:39,190
hypothesis is kind of like the model

137
00:06:39,190 --> 00:06:42,789
that we try to build to represent the

138
00:06:42,789 --> 00:06:44,500
logic or the pattern we're trying to

139
00:06:44,500 --> 00:06:48,200
learn and so what age theta of X

140
00:06:48,200 --> 00:06:51,800
means is like you put excess in like X 1

141
00:06:51,800 --> 00:06:56,150
X 2 X 3 would be 3 values in a vector or

142
00:06:56,150 --> 00:06:59,360
an array if you want and then so the

143
00:06:59,360 --> 00:07:02,480
function says this is the logic that I

144
00:07:02,480 --> 00:07:05,270
think it's gonna be and then we get our

145
00:07:05,270 --> 00:07:07,760
result this would be a numerical value

146
00:07:07,760 --> 00:07:10,340
probably and it's ideally really close

147
00:07:10,340 --> 00:07:13,280
to the reality and the reality is called

148
00:07:13,280 --> 00:07:17,750
Y in this case ok so there was really a

149
00:07:17,750 --> 00:07:20,200
lot so let's pause with a funny quote

150
00:07:20,200 --> 00:07:23,420
this is by George EP box and he says all

151
00:07:23,420 --> 00:07:25,670
models are wrong but some of them are

152
00:07:25,670 --> 00:07:34,640
useful ok so let's say we did all that

153
00:07:34,640 --> 00:07:36,890
stuff with our neurons and become fewer

154
00:07:36,890 --> 00:07:39,380
all that those things with their magic

155
00:07:39,380 --> 00:07:42,890
activation function and now we have a

156
00:07:42,890 --> 00:07:45,110
result but the result is actually

157
00:07:45,110 --> 00:07:47,390
probably not that good probably not that

158
00:07:47,390 --> 00:07:50,390
close to reality because we initialize

159
00:07:50,390 --> 00:07:52,340
data randomly so we just said just to

160
00:07:52,340 --> 00:07:59,540
random stuff whether for this case which

161
00:07:59,540 --> 00:08:01,400
is called supervised learning because we

162
00:08:01,400 --> 00:08:06,020
have we have a output that we know is

163
00:08:06,020 --> 00:08:06,530
the truth

164
00:08:06,530 --> 00:08:09,590
so we supervise our machine while it

165
00:08:09,590 --> 00:08:12,470
learns and supervised learning is used

166
00:08:12,470 --> 00:08:15,050
for for example quota for betting places

167
00:08:15,050 --> 00:08:17,360
the P win quota there is one where there

168
00:08:17,360 --> 00:08:20,270
are supervised learning and also that is

169
00:08:20,270 --> 00:08:24,050
a really nice example as housing price

170
00:08:24,050 --> 00:08:27,830
predictions where you say this is the

171
00:08:27,830 --> 00:08:29,990
square meters and this is a number of

172
00:08:29,990 --> 00:08:32,210
rooms and this is the price amount of

173
00:08:32,210 --> 00:08:35,060
money that I have to pay for and so you

174
00:08:35,060 --> 00:08:37,550
train your neural network with stuff

175
00:08:37,550 --> 00:08:40,040
that you know is the truth

176
00:08:40,040 --> 00:08:43,610
there's also unsupervised learning that

177
00:08:43,610 --> 00:08:45,580
is for example the Netflix

178
00:08:45,580 --> 00:08:48,200
recommendation system so there is no

179
00:08:48,200 --> 00:08:51,110
real answer that we the people who build

180
00:08:51,110 --> 00:08:54,500
it know about we just say probably

181
00:08:54,500 --> 00:08:56,030
there's some pattern here maybe you can

182
00:08:56,030 --> 00:08:59,720
try to figure it out and clustering is

183
00:08:59,720 --> 00:09:02,140
one of the most often used

184
00:09:02,140 --> 00:09:05,510
methods to do unsupervised learning but

185
00:09:05,510 --> 00:09:07,970
I won't be talking about that only to be

186
00:09:07,970 --> 00:09:11,240
talking about supervised learning okay

187
00:09:11,240 --> 00:09:12,950
so like I said we initialize data

188
00:09:12,950 --> 00:09:17,000
randomly and this could be an example of

189
00:09:17,000 --> 00:09:19,070
our training data so we have four

190
00:09:19,070 --> 00:09:23,000
training examples let's just say this is

191
00:09:23,000 --> 00:09:26,660
housing prices so x1 would be the number

192
00:09:26,660 --> 00:09:29,540
of I don't know square meters and the

193
00:09:29,540 --> 00:09:31,130
other one would be the number of rooms

194
00:09:31,130 --> 00:09:33,350
which is weird but doesn't matter

195
00:09:33,350 --> 00:09:37,399
so XS are the inputs Y is the result

196
00:09:37,399 --> 00:09:39,770
that we know it is and H of X would be

197
00:09:39,770 --> 00:09:41,660
the result that our neural network just

198
00:09:41,660 --> 00:09:44,779
gave us so as you can see it's off of

199
00:09:44,779 --> 00:09:46,640
course because we just did it randomly

200
00:09:46,640 --> 00:09:49,850
and the question now becomes is how do

201
00:09:49,850 --> 00:09:53,089
we actually learn the correct patterns

202
00:09:53,089 --> 00:09:58,360
or the correct weights for our theta so

203
00:09:58,360 --> 00:10:02,180
what we can do is calculate the cost so

204
00:10:02,180 --> 00:10:06,860
we have our output 60 and our y which is

205
00:10:06,860 --> 00:10:10,339
43 and the cost would be the difference

206
00:10:10,339 --> 00:10:13,790
between those two and this cost is also

207
00:10:13,790 --> 00:10:15,680
described described by another function

208
00:10:15,680 --> 00:10:18,680
it's called J of theta doesn't really

209
00:10:18,680 --> 00:10:20,900
matter what you call it it just means

210
00:10:20,900 --> 00:10:22,610
this calculates the difference between

211
00:10:22,610 --> 00:10:26,300
our result and the actual thing that the

212
00:10:26,300 --> 00:10:30,980
reality okay so what we do now is we use

213
00:10:30,980 --> 00:10:33,050
a very complicated mathematical function

214
00:10:33,050 --> 00:10:36,560
and then we take our cost and then

215
00:10:36,560 --> 00:10:38,029
propagate back through our network

216
00:10:38,029 --> 00:10:41,180
that's called back propagation and try

217
00:10:41,180 --> 00:10:44,450
to change the values for theta in a way

218
00:10:44,450 --> 00:10:46,339
that minimizes the cost because we

219
00:10:46,339 --> 00:10:49,100
wanted the error to be small as small as

220
00:10:49,100 --> 00:10:51,170
possible so we try to minimize J of

221
00:10:51,170 --> 00:10:55,760
theta and the way that we do that with

222
00:10:55,760 --> 00:10:58,100
synaptic it's called gradient descent

223
00:10:58,100 --> 00:11:00,920
there are other ways and other functions

224
00:11:00,920 --> 00:11:03,649
to actually do that but I will only talk

225
00:11:03,649 --> 00:11:06,800
about gradient descent for now so this

226
00:11:06,800 --> 00:11:08,750
is very complicated so I'm trying to

227
00:11:08,750 --> 00:11:09,570
make

228
00:11:09,570 --> 00:11:12,240
in a very basic way so let's say this is

229
00:11:12,240 --> 00:11:14,339
a graphical representation of our cost

230
00:11:14,339 --> 00:11:16,829
function we have theta 1 we have theta 2

231
00:11:16,829 --> 00:11:20,100
and then we have a circle which is stay

232
00:11:20,100 --> 00:11:23,399
of theta so what that means is that all

233
00:11:23,399 --> 00:11:25,410
the combinations of theta 1 and theta 2

234
00:11:25,410 --> 00:11:28,980
that are on the circle have the same

235
00:11:28,980 --> 00:11:31,829
value for theta J of theta so let's say

236
00:11:31,829 --> 00:11:36,470
the the the the bottom dot there is

237
00:11:36,470 --> 00:11:40,920
theta 1 2 and theta 2 1 that's not a

238
00:11:40,920 --> 00:11:43,079
very good example but so there those

239
00:11:43,079 --> 00:11:44,459
will have the same value for J of theta

240
00:11:44,459 --> 00:11:46,860
or like the upper point where theta 1

241
00:11:46,860 --> 00:11:50,720
would be 2 and theta 2 would be 4 maybe

242
00:11:50,720 --> 00:11:53,579
okay and then if we calculate J of theta

243
00:11:53,579 --> 00:11:56,610
4 a lot of combinations of theta 1 and

244
00:11:56,610 --> 00:11:59,100
theta 2 we have something that looks

245
00:11:59,100 --> 00:12:01,079
like this and the way you have to look

246
00:12:01,079 --> 00:12:03,180
at it is actually that there is a third

247
00:12:03,180 --> 00:12:05,940
dimension going into the back which is

248
00:12:05,940 --> 00:12:08,220
there of theta so the value of J of

249
00:12:08,220 --> 00:12:10,769
theta becomes smaller to the back so

250
00:12:10,769 --> 00:12:14,510
imagine you're looking into a funnel now

251
00:12:14,510 --> 00:12:17,880
we can see here that where the Red Dot

252
00:12:17,880 --> 00:12:20,459
is that would represent the minimum so

253
00:12:20,459 --> 00:12:22,019
where J of theta would be the smallest

254
00:12:22,019 --> 00:12:24,769
and this is what we want to reach and

255
00:12:24,769 --> 00:12:27,750
what gradient descent does is now that

256
00:12:27,750 --> 00:12:30,360
let's say the gray dot I hope you can

257
00:12:30,360 --> 00:12:32,610
see it yes and the way dot is where we

258
00:12:32,610 --> 00:12:35,160
come out initially and we understand

259
00:12:35,160 --> 00:12:38,910
basically checks for the values of theta

260
00:12:38,910 --> 00:12:41,610
1 and theta 2 that will get us closer to

261
00:12:41,610 --> 00:12:43,620
the minimum doesn't have to go there

262
00:12:43,620 --> 00:12:45,779
directly sometimes it will wander around

263
00:12:45,779 --> 00:12:49,860
a little bit but eventually it will try

264
00:12:49,860 --> 00:12:52,019
to reach the global minimum it doesn't

265
00:12:52,019 --> 00:12:53,699
happen all the time sometimes we just

266
00:12:53,699 --> 00:12:57,389
wander around it but you you will reach

267
00:12:57,389 --> 00:13:01,050
the vicinity of it and the size of the

268
00:13:01,050 --> 00:13:03,120
steps actually is called the learning

269
00:13:03,120 --> 00:13:05,010
rate you can change that rate you can

270
00:13:05,010 --> 00:13:06,990
make the size as bigger or smaller and

271
00:13:06,990 --> 00:13:09,000
usually the learning rate gets smaller

272
00:13:09,000 --> 00:13:11,040
the closer you are to the minimum so

273
00:13:11,040 --> 00:13:12,689
what you can't really see here is that

274
00:13:12,689 --> 00:13:15,899
the steps will get smaller the more you

275
00:13:15,899 --> 00:13:21,300
approach the red dot okay

276
00:13:21,310 --> 00:13:24,620
that was a lot of theory so let's

277
00:13:24,620 --> 00:13:26,030
implement something and you will see

278
00:13:26,030 --> 00:13:29,450
it's really not that hard so I use

279
00:13:29,450 --> 00:13:31,970
synaptic because synaptic does all that

280
00:13:31,970 --> 00:13:33,860
stuff that I showed you already and I

281
00:13:33,860 --> 00:13:35,210
don't really have to care about it it's

282
00:13:35,210 --> 00:13:38,990
a framework to do exactly - then do

283
00:13:38,990 --> 00:13:41,690
stuff with the neural network and it

284
00:13:41,690 --> 00:13:43,400
gives us everything we need to build a

285
00:13:43,400 --> 00:13:45,770
simple network and we have an architect

286
00:13:45,770 --> 00:13:47,780
and the architect will build or a

287
00:13:47,780 --> 00:13:50,450
network and then we have a trainer and

288
00:13:50,450 --> 00:13:53,210
that will train our network with the e

289
00:13:53,210 --> 00:13:55,610
training examples that are not included

290
00:13:55,610 --> 00:13:59,480
we have to provide them - for our neural

291
00:13:59,480 --> 00:14:02,480
network and we can make predictions with

292
00:14:02,480 --> 00:14:04,790
our trained network by calling the

293
00:14:04,790 --> 00:14:10,250
function activate ok code I will give

294
00:14:10,250 --> 00:14:21,430
you a moment to parse it

295
00:14:21,440 --> 00:14:25,579
so I have two objects there one is the

296
00:14:25,579 --> 00:14:28,129
historic match it has an input array

297
00:14:28,129 --> 00:14:31,190
those are market values for the team I

298
00:14:31,190 --> 00:14:33,110
just thought that market values are a

299
00:14:33,110 --> 00:14:35,600
pretty good approximation of how strong

300
00:14:35,600 --> 00:14:38,269
a team is so I just chose that one but

301
00:14:38,269 --> 00:14:39,620
feel free to choose anything else you

302
00:14:39,620 --> 00:14:42,800
want and so the first is for the home

303
00:14:42,800 --> 00:14:44,269
team and the second value is for the

304
00:14:44,269 --> 00:14:47,480
relay team and if you always provide the

305
00:14:47,480 --> 00:14:50,509
home team first then the numeral network

306
00:14:50,509 --> 00:14:52,250
will also learn from that because that

307
00:14:52,250 --> 00:14:53,990
is also a pattern that it will pick up

308
00:14:53,990 --> 00:14:56,660
if there is something like a home team

309
00:14:56,660 --> 00:14:58,850
advantage then maybe our new network

310
00:14:58,850 --> 00:15:01,670
will learn that and then we have an

311
00:15:01,670 --> 00:15:05,240
output that is a vector or an array

312
00:15:05,240 --> 00:15:07,100
where we have basically three

313
00:15:07,100 --> 00:15:09,949
classifications so the first value would

314
00:15:09,949 --> 00:15:11,990
mean the home team win if it's a one

315
00:15:11,990 --> 00:15:14,750
over the zero no the second value says

316
00:15:14,750 --> 00:15:16,819
that it is a draw and the third value

317
00:15:16,819 --> 00:15:19,459
would be that the away team wins so we

318
00:15:19,459 --> 00:15:21,079
actually we do three types of

319
00:15:21,079 --> 00:15:23,240
classifications one for each of the

320
00:15:23,240 --> 00:15:25,550
possible outcomes and in this example

321
00:15:25,550 --> 00:15:27,740
the home team won because there's a one

322
00:15:27,740 --> 00:15:30,680
as the first value then we have the

323
00:15:30,680 --> 00:15:32,120
match in the future which basically

324
00:15:32,120 --> 00:15:34,639
looks exactly the same just except that

325
00:15:34,639 --> 00:15:36,829
it doesn't have an output that's logical

326
00:15:36,829 --> 00:15:38,509
because we want our annual network to

327
00:15:38,509 --> 00:15:42,259
give us the output so then we have our

328
00:15:42,259 --> 00:15:45,500
training set that is consistent of the

329
00:15:45,500 --> 00:15:47,720
historic matches and we have our network

330
00:15:47,720 --> 00:15:49,899
that is trained by the architect

331
00:15:49,899 --> 00:15:52,490
perceptron actually means the smallest

332
00:15:52,490 --> 00:15:55,040
possible network which is just one

333
00:15:55,040 --> 00:15:57,949
neuron input and output but it doesn't

334
00:15:57,949 --> 00:15:59,410
really matter it could also say in it

335
00:15:59,410 --> 00:16:01,910
it's just the way that we build the

336
00:16:01,910 --> 00:16:06,889
network and so we give the perceptron

337
00:16:06,889 --> 00:16:11,209
the number of nodes per layer so first

338
00:16:11,209 --> 00:16:14,300
is two because we have two input values

339
00:16:14,300 --> 00:16:17,990
I chose two hidden layers of six nodes

340
00:16:17,990 --> 00:16:19,670
but it doesn't really matter you can

341
00:16:19,670 --> 00:16:22,059
just try it out actually that's what I

342
00:16:22,059 --> 00:16:25,160
also did I just try out what would seem

343
00:16:25,160 --> 00:16:27,500
to work best and then we have three

344
00:16:27,500 --> 00:16:29,180
nodes four output layer because we have

345
00:16:29,180 --> 00:16:33,380
three values in our output then we

346
00:16:33,380 --> 00:16:34,930
trained our trainer

347
00:16:34,930 --> 00:16:37,810
the training set and I just chose a

348
00:16:37,810 --> 00:16:39,580
learning rate that you can see there

349
00:16:39,580 --> 00:16:41,589
there's learning rate the steps that we

350
00:16:41,589 --> 00:16:43,870
take in gradient descent and that is

351
00:16:43,870 --> 00:16:46,360
also something I just figured out with

352
00:16:46,360 --> 00:16:49,600
some trial and error and then there is a

353
00:16:49,600 --> 00:16:51,610
number of iterations so you say I want

354
00:16:51,610 --> 00:16:53,410
to iterate through my whole training set

355
00:16:53,410 --> 00:16:55,209
a hundred thousand times in this case

356
00:16:55,209 --> 00:16:59,470
and after we trained or trainer or

357
00:16:59,470 --> 00:17:02,470
network we can actually make predictions

358
00:17:02,470 --> 00:17:05,949
like I said before we give our network

359
00:17:05,949 --> 00:17:09,870
the activations or the input with the

360
00:17:09,870 --> 00:17:15,100
function called activate and this is

361
00:17:15,100 --> 00:17:18,670
what the output could look like so here

362
00:17:18,670 --> 00:17:21,280
you can see the activations the inputs

363
00:17:21,280 --> 00:17:24,640
the market values and then the output

364
00:17:24,640 --> 00:17:27,880
and as you can see there's no output

365
00:17:27,880 --> 00:17:31,120
like 1 0 0 0 0 1 this is a probability

366
00:17:31,120 --> 00:17:34,390
distribution so all the values added up

367
00:17:34,390 --> 00:17:37,000
in this array will come to one so

368
00:17:37,000 --> 00:17:38,830
there's a hundred percent possibility

369
00:17:38,830 --> 00:17:40,870
that one of those outcomes will happen

370
00:17:40,870 --> 00:17:43,570
and then there's a distribution of how

371
00:17:43,570 --> 00:17:46,510
likely it is so for the first one you

372
00:17:46,510 --> 00:17:51,240
can see that the team who has 320 1.15

373
00:17:51,240 --> 00:17:52,420
euro

374
00:17:52,420 --> 00:17:55,840
Paulie millions of euros will more

375
00:17:55,840 --> 00:18:01,630
likely win than the other team ok this

376
00:18:01,630 --> 00:18:03,760
is a lot of code and it's also a thinker

377
00:18:03,760 --> 00:18:13,580
so take your time everyone got that

378
00:18:13,580 --> 00:18:19,080
good okay so now we have our network we

379
00:18:19,080 --> 00:18:20,760
have our output it looks pretty good so

380
00:18:20,760 --> 00:18:23,220
far but how do we know how well it

381
00:18:23,220 --> 00:18:27,570
performs I will explain some error rates

382
00:18:27,570 --> 00:18:30,720
that I personally focused on but as when

383
00:18:30,720 --> 00:18:32,520
I try to learn how to improve my

384
00:18:32,520 --> 00:18:35,880
algorithm but feel free to think come up

385
00:18:35,880 --> 00:18:38,550
with your own errors it's just what I

386
00:18:38,550 --> 00:18:39,980
did

387
00:18:39,980 --> 00:18:42,510
so there's something called the synaptic

388
00:18:42,510 --> 00:18:47,460
data error and that is already provided

389
00:18:47,460 --> 00:18:52,770
with synaptic and I call it data error

390
00:18:52,770 --> 00:18:54,960
because it just did the name is arrow

391
00:18:54,960 --> 00:18:57,230
and I call it on the data object and

392
00:18:57,230 --> 00:19:00,570
what we do here is we add a schedule to

393
00:19:00,570 --> 00:19:04,020
our trainer and we tell them that every

394
00:19:04,020 --> 00:19:07,530
ten thousandth iteration I want to log

395
00:19:07,530 --> 00:19:13,230
the current data error to the console so

396
00:19:13,230 --> 00:19:17,910
this is bad okay and this could be the

397
00:19:17,910 --> 00:19:20,940
output and in this case you can see

398
00:19:20,940 --> 00:19:22,860
actually that the error actually goes

399
00:19:22,860 --> 00:19:24,630
down a little bit over time and over

400
00:19:24,630 --> 00:19:26,790
iterations and that is really really

401
00:19:26,790 --> 00:19:31,080
good to see and also 18 percent is also

402
00:19:31,080 --> 00:19:32,820
pretty good because we only give it our

403
00:19:32,820 --> 00:19:34,560
market values and as seems to perform

404
00:19:34,560 --> 00:19:38,130
pretty good but the error doesn't really

405
00:19:38,130 --> 00:19:39,630
tell us anything it doesn't really

406
00:19:39,630 --> 00:19:43,140
translate to anything that you can then

407
00:19:43,140 --> 00:19:47,520
interpret so I thought I need to display

408
00:19:47,520 --> 00:19:49,470
a different error that was more of an

409
00:19:49,470 --> 00:19:52,140
intuitive representation of how well my

410
00:19:52,140 --> 00:19:55,380
algorithm actually performed so what I

411
00:19:55,380 --> 00:19:59,100
did then is come up with something

412
00:19:59,100 --> 00:20:02,730
called the classification error so give

413
00:20:02,730 --> 00:20:08,860
you a moment to parse this

414
00:20:08,870 --> 00:20:13,350
okay what i'm doing here is in our do

415
00:20:13,350 --> 00:20:17,400
function for every 10,000 iteration i

416
00:20:17,400 --> 00:20:20,040
want to actually make the prediction

417
00:20:20,040 --> 00:20:22,800
because now i only first i only have the

418
00:20:22,800 --> 00:20:24,870
probabilities but i don't actually say

419
00:20:24,870 --> 00:20:27,870
so the home team's gonna win so this is

420
00:20:27,870 --> 00:20:29,910
what i'm doing here the predict from

421
00:20:29,910 --> 00:20:32,700
probability function that you can see

422
00:20:32,700 --> 00:20:35,820
actually just looks for the key where

423
00:20:35,820 --> 00:20:38,780
the value is the biggest in the array so

424
00:20:38,780 --> 00:20:41,760
if the home team won this would be 0 if

425
00:20:41,760 --> 00:20:44,400
it was a draw would be 1 and the

426
00:20:44,400 --> 00:20:48,830
weighting 1 would be 2 and then i just

427
00:20:48,830 --> 00:20:52,140
look just make the prediction actually

428
00:20:52,140 --> 00:20:54,120
and then do the same thing and then

429
00:20:54,120 --> 00:20:57,450
compare whether the prediction actually

430
00:20:57,450 --> 00:21:00,720
met the expectation count up my errors

431
00:21:00,720 --> 00:21:02,970
and then divided through the length of

432
00:21:02,970 --> 00:21:06,390
the training set and what we actually

433
00:21:06,390 --> 00:21:08,730
see here is that the error rate is much

434
00:21:08,730 --> 00:21:12,240
higher with 45% and this is actually

435
00:21:12,240 --> 00:21:14,730
what i had suspected and i will probably

436
00:21:14,730 --> 00:21:17,580
tell you later why and also you can see

437
00:21:17,580 --> 00:21:19,350
that it doesn't continuously go down it

438
00:21:19,350 --> 00:21:23,220
just seems to hover around 44% but i

439
00:21:23,220 --> 00:21:27,060
still thought this really wasn't telling

440
00:21:27,060 --> 00:21:28,920
me how well my algorithm algorithm

441
00:21:28,920 --> 00:21:31,410
performs because i make those

442
00:21:31,410 --> 00:21:32,970
predictions with the training set so i

443
00:21:32,970 --> 00:21:36,090
already know about the data that i get

444
00:21:36,090 --> 00:21:37,620
in and then i make a prediction from it

445
00:21:37,620 --> 00:21:39,690
doesn't tell me anything about how well

446
00:21:39,690 --> 00:21:41,940
my algorithm performs for future

447
00:21:41,940 --> 00:21:45,240
predictions so then i came up with a

448
00:21:45,240 --> 00:21:47,310
thing called the cross-validation error

449
00:21:47,310 --> 00:21:51,150
and what i do there is actually first

450
00:21:51,150 --> 00:21:54,990
split the data set into a training set

451
00:21:54,990 --> 00:21:58,110
and then a cross validation set and the

452
00:21:58,110 --> 00:22:00,210
reason i do that is because i want the

453
00:22:00,210 --> 00:22:02,520
training data to only train my neural

454
00:22:02,520 --> 00:22:04,590
network and then I take the cross

455
00:22:04,590 --> 00:22:07,320
rotation data I act like it would be the

456
00:22:07,320 --> 00:22:09,720
future and then check how well

457
00:22:09,720 --> 00:22:13,200
algorithm performs so I'm doing the

458
00:22:13,200 --> 00:22:15,690
exact same thing as I did before just I

459
00:22:15,690 --> 00:22:17,070
make the predictions on the

460
00:22:17,070 --> 00:22:18,690
cross-validation set and not on the

461
00:22:18,690 --> 00:22:23,400
training set okay so this is the output

462
00:22:23,400 --> 00:22:27,930
and you can see that it seemed to

463
00:22:27,930 --> 00:22:30,450
perform even worse a little bit but also

464
00:22:30,450 --> 00:22:31,890
this is something that I already

465
00:22:31,890 --> 00:22:36,000
suspected would happen and what's weird

466
00:22:36,000 --> 00:22:38,340
is that it seems like the error rate

467
00:22:38,340 --> 00:22:40,140
goes up for at first and then goes down

468
00:22:40,140 --> 00:22:42,900
and then up again just like the arrow we

469
00:22:42,900 --> 00:22:49,550
saw before so the question now becomes

470
00:22:49,550 --> 00:22:52,350
why what what does this mean what what

471
00:22:52,350 --> 00:22:53,610
do these arrow rates

472
00:22:53,610 --> 00:22:57,780
tell me actually so the thing is if we

473
00:22:57,780 --> 00:23:00,300
have one example where our result would

474
00:23:00,300 --> 00:23:02,670
be this probability distribution and

475
00:23:02,670 --> 00:23:04,980
then we have the actual result which

476
00:23:04,980 --> 00:23:08,780
would be this 0 0 1 so they're waiting 1

477
00:23:08,780 --> 00:23:12,270
what the data error gives us is called

478
00:23:12,270 --> 00:23:16,170
the mean squared error and the squared

479
00:23:16,170 --> 00:23:20,910
error is this so what this means is the

480
00:23:20,910 --> 00:23:22,590
probability distribution would be a

481
00:23:22,590 --> 00:23:24,660
vector and we measure the we get the

482
00:23:24,660 --> 00:23:28,560
length of the vector and then to the

483
00:23:28,560 --> 00:23:32,010
power of 2 which would come up to 0.98

484
00:23:32,010 --> 00:23:35,940
about 0.9 and then the mean means we

485
00:23:35,940 --> 00:23:38,100
take all training examples not just the

486
00:23:38,100 --> 00:23:40,020
one and then calculate the average

487
00:23:40,020 --> 00:23:43,230
squared error so probably the average

488
00:23:43,230 --> 00:23:46,950
would be somewhere close to 0.98

489
00:23:46,950 --> 00:23:50,310
let's say it's that now for a

490
00:23:50,310 --> 00:23:54,290
classification error we only have one

491
00:23:54,290 --> 00:23:59,580
because you can either be a hundred

492
00:23:59,580 --> 00:24:01,650
percent wrong or a hundred percent right

493
00:24:01,650 --> 00:24:03,000
there's no in between there's no

494
00:24:03,000 --> 00:24:05,100
probabilities so in this case our error

495
00:24:05,100 --> 00:24:07,410
would be just one and also for a

496
00:24:07,410 --> 00:24:09,540
cross-validation error it would be one

497
00:24:09,540 --> 00:24:13,320
but still it's it's logical that our

498
00:24:13,320 --> 00:24:15,990
would perform worse with data it doesn't

499
00:24:15,990 --> 00:24:17,720
know about it's just not in the

500
00:24:17,720 --> 00:24:24,210
experienced realm of the algorithm okay

501
00:24:24,210 --> 00:24:27,540
so what does it actually mean when my

502
00:24:27,540 --> 00:24:29,790
cross-validation error is bigger or

503
00:24:29,790 --> 00:24:32,610
greater than my classification error so

504
00:24:32,610 --> 00:24:35,460
that could mean that we have over fitted

505
00:24:35,460 --> 00:24:38,790
or model or have high variance and I'm

506
00:24:38,790 --> 00:24:42,780
trying to explain that let's say this is

507
00:24:42,780 --> 00:24:46,290
our training data so this one is X 1 and

508
00:24:46,290 --> 00:24:49,530
X 2 we have two features and this is our

509
00:24:49,530 --> 00:24:52,530
training data and now our model is the

510
00:24:52,530 --> 00:24:55,680
line there so the model describes the

511
00:24:55,680 --> 00:24:58,860
training data pretty good but maybe it's

512
00:24:58,860 --> 00:25:00,780
not that good for future predictions

513
00:25:00,780 --> 00:25:03,440
because it's really it just follows this

514
00:25:03,440 --> 00:25:06,510
exact model and it doesn't really

515
00:25:06,510 --> 00:25:10,950
generalize very well and that is called

516
00:25:10,950 --> 00:25:13,200
overhead over fit or high variance and

517
00:25:13,200 --> 00:25:16,380
the other thing the other extreme would

518
00:25:16,380 --> 00:25:18,770
be if you have the same training set and

519
00:25:18,770 --> 00:25:21,990
this would be a model it's a straight

520
00:25:21,990 --> 00:25:24,810
line it's it doesn't fit so well on the

521
00:25:24,810 --> 00:25:27,240
existing training data but it might

522
00:25:27,240 --> 00:25:29,520
perform better with future predictions

523
00:25:29,520 --> 00:25:32,430
than the one on the left and that is

524
00:25:32,430 --> 00:25:37,620
called under fit or high bias so what we

525
00:25:37,620 --> 00:25:41,580
maybe can do is try to get more data or

526
00:25:41,580 --> 00:25:44,040
more features so either get more

527
00:25:44,040 --> 00:25:47,130
historical data go to the last year the

528
00:25:47,130 --> 00:25:49,110
year before that also get the data from

529
00:25:49,110 --> 00:25:50,130
I don't know

530
00:25:50,130 --> 00:25:52,650
Premier League or the Super League or

531
00:25:52,650 --> 00:25:56,460
whatever or we can do some or we can get

532
00:25:56,460 --> 00:25:59,730
more features like the match day or the

533
00:25:59,730 --> 00:26:02,370
table position or I don't know that a

534
00:26:02,370 --> 00:26:07,670
number of sexy coaches in the team and

535
00:26:07,670 --> 00:26:10,260
then there's this other thing called

536
00:26:10,260 --> 00:26:13,950
regularization and that is basically

537
00:26:13,950 --> 00:26:16,500
another parameter that we add to our

538
00:26:16,500 --> 00:26:20,970
cost function ya know to the gradient

539
00:26:20,970 --> 00:26:21,770
descent function

540
00:26:21,770 --> 00:26:26,059
and what does actually does is trying to

541
00:26:26,059 --> 00:26:29,240
keep the Thetas as small or big as

542
00:26:29,240 --> 00:26:31,580
possible it depends really on what you

543
00:26:31,580 --> 00:26:33,650
value you choose for your regularization

544
00:26:33,650 --> 00:26:38,270
parameter if you think about the model I

545
00:26:38,270 --> 00:26:39,530
showed you before

546
00:26:39,530 --> 00:26:42,410
keeping the theta small would mean to

547
00:26:42,410 --> 00:26:44,660
straighten out the line and making a big

548
00:26:44,660 --> 00:26:46,850
would mean to curve the line more so

549
00:26:46,850 --> 00:26:48,710
this is how you could counteract

550
00:26:48,710 --> 00:26:51,070
overfitting or underfitting

551
00:26:51,070 --> 00:26:54,679
and also the hovering that we saw before

552
00:26:54,679 --> 00:26:56,240
that the error rate seemed to hover

553
00:26:56,240 --> 00:26:58,910
around a number but never seem to get

554
00:26:58,910 --> 00:27:01,400
anywhere could could mean that we

555
00:27:01,400 --> 00:27:03,110
actually already reached our minimum

556
00:27:03,110 --> 00:27:05,990
it's just not very optimal so maybe it's

557
00:27:05,990 --> 00:27:07,820
just we're not that we didn't describe

558
00:27:07,820 --> 00:27:11,720
our feature pretty good pretty well so

559
00:27:11,720 --> 00:27:19,250
hmm sorry what I actually did is I chose

560
00:27:19,250 --> 00:27:22,340
to go with more features and I want to

561
00:27:22,340 --> 00:27:24,770
show you what happened so I this is for

562
00:27:24,770 --> 00:27:26,270
the market values you can see the error

563
00:27:26,270 --> 00:27:29,090
rates it's kind of basically what we saw

564
00:27:29,090 --> 00:27:32,350
before and now I have two five features

565
00:27:32,350 --> 00:27:35,750
matched a the market values and then the

566
00:27:35,750 --> 00:27:41,540
table positions and now we can try to

567
00:27:41,540 --> 00:27:43,340
interpret there seems to be something

568
00:27:43,340 --> 00:27:45,080
that is going there was a little bit

569
00:27:45,080 --> 00:27:47,450
better other things seem to be a little

570
00:27:47,450 --> 00:27:50,900
bit worse but I personally decided that

571
00:27:50,900 --> 00:27:53,360
the variation in numbers is just too

572
00:27:53,360 --> 00:27:55,429
little to really read anything into it

573
00:27:55,429 --> 00:27:57,890
so the only thing that that really told

574
00:27:57,890 --> 00:28:00,559
me was that investing time and maybe

575
00:28:00,559 --> 00:28:02,960
money in getting more features might not

576
00:28:02,960 --> 00:28:04,490
be that beneficial because it just

577
00:28:04,490 --> 00:28:06,530
doesn't have that effect on the error

578
00:28:06,530 --> 00:28:08,840
rates that I hoped it would be I would

579
00:28:08,840 --> 00:28:11,840
have so my next step would just be to

580
00:28:11,840 --> 00:28:13,610
try get more data and see what it does

581
00:28:13,610 --> 00:28:19,690
to my error rate so another funny quote

582
00:28:19,690 --> 00:28:21,830
prediction is very difficult especially

583
00:28:21,830 --> 00:28:24,290
about the future and as you saw not even

584
00:28:24,290 --> 00:28:25,910
for the future just for the faked future

585
00:28:25,910 --> 00:28:28,130
just for the present it's not that easy

586
00:28:28,130 --> 00:28:32,669
at all

587
00:28:32,679 --> 00:28:36,679
okay so now you hopefully know how

588
00:28:36,679 --> 00:28:40,100
neural networks do their magic and how

589
00:28:40,100 --> 00:28:41,510
to implement a machine learning

590
00:28:41,510 --> 00:28:43,309
algorithm with synaptic and you don't

591
00:28:43,309 --> 00:28:45,110
even have to do all the math stuff just

592
00:28:45,110 --> 00:28:49,100
go and do stuff then you might have

593
00:28:49,100 --> 00:28:50,660
learned how to check the performance of

594
00:28:50,660 --> 00:28:53,480
your algorithm and how to interpret your

595
00:28:53,480 --> 00:28:56,690
error rates so before I said I would

596
00:28:56,690 --> 00:28:58,730
make a prediction for this matchday and

597
00:28:58,730 --> 00:29:02,320
I did I already gave this talk yesterday

598
00:29:02,320 --> 00:29:05,570
ahjuma so they can actually vouch that I

599
00:29:05,570 --> 00:29:09,380
made this prediction and I got the first

600
00:29:09,380 --> 00:29:10,460
game right already

601
00:29:10,460 --> 00:29:12,620
that's awesome like I said I had the

602
00:29:12,620 --> 00:29:15,169
predictions over there and you can check

603
00:29:15,169 --> 00:29:16,790
for yourself how well my algorithm

604
00:29:16,790 --> 00:29:19,130
actually performs I'm using data from

605
00:29:19,130 --> 00:29:21,080
the last four years of the bundesliga so

606
00:29:21,080 --> 00:29:24,410
it's really not that good depending on

607
00:29:24,410 --> 00:29:25,820
the error rate it's probably worse than

608
00:29:25,820 --> 00:29:31,280
a coin toss maybe okay if you want to

609
00:29:31,280 --> 00:29:33,740
know more the first link would be to the

610
00:29:33,740 --> 00:29:35,650
repository it's called positronic brain

611
00:29:35,650 --> 00:29:39,610
every Star Trek watcher should laugh now

612
00:29:39,610 --> 00:29:44,419
okay no one fine the second link is to

613
00:29:44,419 --> 00:29:46,580
the slides I will also tweet them and

614
00:29:46,580 --> 00:29:49,190
then you can just check it out sorry for

615
00:29:49,190 --> 00:29:50,270
the slight mess up I don't know what

616
00:29:50,270 --> 00:29:53,809
happened there oh sorry and then there's

617
00:29:53,809 --> 00:29:56,720
this funny page called coding games

618
00:29:56,720 --> 00:29:58,490
where it's actually super fun you can

619
00:29:58,490 --> 00:30:00,290
code for fun there and they have a

620
00:30:00,290 --> 00:30:02,480
machine learning section now and there's

621
00:30:02,480 --> 00:30:04,490
stuff you can read about and there's

622
00:30:04,490 --> 00:30:06,380
also the link the second one to synaptic

623
00:30:06,380 --> 00:30:09,620
the framework so check it out and highly

624
00:30:09,620 --> 00:30:13,130
recommend it and finally I want to thank

625
00:30:13,130 --> 00:30:15,980
my master was not here he's actually my

626
00:30:15,980 --> 00:30:17,390
boyfriend and he helped me with

627
00:30:17,390 --> 00:30:20,890
implementing a lot of data pausing stuff

628
00:30:20,890 --> 00:30:23,450
transfer mark we crawl our data from

629
00:30:23,450 --> 00:30:26,270
their page and I don't really ask them I

630
00:30:26,270 --> 00:30:29,809
just did also I want to thank Jim drew

631
00:30:29,809 --> 00:30:31,520
for providing me the time and the

632
00:30:31,520 --> 00:30:33,679
resources to actually do that and of

633
00:30:33,679 --> 00:30:35,179
course all of you for your interest

634
00:30:35,179 --> 00:30:37,270
thanks a lot

