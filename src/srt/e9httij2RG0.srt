1
00:00:21,630 --> 00:00:25,300

you

2
00:00:25,310 --> 00:00:29,300
so my name is Julian I am the CEO of a

3
00:00:29,300 --> 00:00:30,410
company called superheater and I'm going

4
00:00:30,410 --> 00:00:31,700
to come back to this later this is a

5
00:00:31,700 --> 00:00:33,830
presentation about streaming algorithms

6
00:00:33,830 --> 00:00:36,920
also as as a note I tend to speak very

7
00:00:36,920 --> 00:00:38,890
very fast so please do not hesitate to

8
00:00:38,890 --> 00:00:41,720
kill me with science and slow down I'll

9
00:00:41,720 --> 00:00:44,270
try so this is a presentation about

10
00:00:44,270 --> 00:00:49,309
streaming algorithms and JavaScript as I

11
00:00:49,309 --> 00:00:51,410
told earlier I am Julian 51 on Twitter

12
00:00:51,410 --> 00:00:55,970
and other silos and my website is super

13
00:00:55,970 --> 00:00:58,160
calm

14
00:00:58,160 --> 00:01:00,440
please also I wanna take one minute for

15
00:01:00,440 --> 00:01:01,670
you guys to enjoy yourself to each other

16
00:01:01,670 --> 00:01:03,769
maybe find someone close to you that you

17
00:01:03,769 --> 00:01:04,699
don't know that you've never talked to

18
00:01:04,699 --> 00:01:07,220
because every time I sit I usually don't

19
00:01:07,220 --> 00:01:08,390
know my neighbor so you have one minute

20
00:01:08,390 --> 00:01:10,700
please introduce yourself exchange maybe

21
00:01:10,700 --> 00:01:12,980
your email or something and then come

22
00:01:12,980 --> 00:01:23,740
back to the presentation

23
00:01:23,750 --> 00:01:30,789
all right ready

24
00:01:30,799 --> 00:01:35,039
ready ready

25
00:01:35,049 --> 00:01:38,619
oh alright alright let's go

26
00:01:38,619 --> 00:01:46,840
hello let's go okay so some of you might

27
00:01:46,840 --> 00:01:48,520
know what that is this is kind of a

28
00:01:48,520 --> 00:01:49,929
picture of the web and I'm gonna start

29
00:01:49,929 --> 00:01:51,579
with a little a history of how the web

30
00:01:51,579 --> 00:01:54,579
is and how it is turning into now so for

31
00:01:54,579 --> 00:01:55,869
the longest time the web has been kind

32
00:01:55,869 --> 00:01:59,229
of a geography world where you move from

33
00:01:59,229 --> 00:02:01,209
a site to another site following links

34
00:02:01,209 --> 00:02:03,399
you will have web addresses which is

35
00:02:03,399 --> 00:02:06,399
very geographically even the icons of

36
00:02:06,399 --> 00:02:08,410
our browser are very Geographic when you

37
00:02:08,410 --> 00:02:10,390
look at obviously Firefox it's it's it's

38
00:02:10,390 --> 00:02:12,790
a it's a map of the world with a fox on

39
00:02:12,790 --> 00:02:14,260
it but if you look at suffering it's

40
00:02:14,260 --> 00:02:15,819
it's a compass itself so it's a very

41
00:02:15,819 --> 00:02:18,430
geographical approach of the web and I

42
00:02:18,430 --> 00:02:19,630
think we're slowly turning into

43
00:02:19,630 --> 00:02:21,130
something that is more like this where

44
00:02:21,130 --> 00:02:22,989
we have more and more of a time based

45
00:02:22,989 --> 00:02:24,370
approach of the web where it's it's just

46
00:02:24,370 --> 00:02:26,440
too big to just even think about how big

47
00:02:26,440 --> 00:02:28,780
it is and and more and more the

48
00:02:28,780 --> 00:02:30,280
information comes to us directly if you

49
00:02:30,280 --> 00:02:32,230
think that Twitter I usually don't go

50
00:02:32,230 --> 00:02:33,849
very far in my timeline maybe not even

51
00:02:33,849 --> 00:02:35,829
birthday or something like I would go

52
00:02:35,829 --> 00:02:38,260
back maybe 1020 tweets and the rest of

53
00:02:38,260 --> 00:02:40,030
the data just comes to me an RSS reader

54
00:02:40,030 --> 00:02:41,950
has been the same forever Facebook is

55
00:02:41,950 --> 00:02:43,510
the same and more and more the

56
00:02:43,510 --> 00:02:45,160
information that we consume online these

57
00:02:45,160 --> 00:02:47,350
devices have notifications and it's just

58
00:02:47,350 --> 00:02:48,850
data coming through us so it's less and

59
00:02:48,850 --> 00:02:51,220
less about me browsing sites and more

60
00:02:51,220 --> 00:02:52,540
and more about me sitting and like

61
00:02:52,540 --> 00:02:54,940
getting vast amounts of data in my face

62
00:02:54,940 --> 00:02:56,980
and this is actually happening at all

63
00:02:56,980 --> 00:02:59,829
stages of the web more and more our

64
00:02:59,829 --> 00:03:03,220
website which is to be basically some

65
00:03:03,220 --> 00:03:04,930
kind of application code and a database

66
00:03:04,930 --> 00:03:07,030
which has a lot of states is more and

67
00:03:07,030 --> 00:03:09,760
more of a system with a lot of cues and

68
00:03:09,760 --> 00:03:11,500
system where the data goes in and move

69
00:03:11,500 --> 00:03:12,639
to the next step and then to the next

70
00:03:12,639 --> 00:03:14,109
step and then to next step and then is

71
00:03:14,109 --> 00:03:16,120
eventually served back to the user or to

72
00:03:16,120 --> 00:03:19,540
or even to another user and and and

73
00:03:19,540 --> 00:03:21,099
that's super feature we exactly deal

74
00:03:21,099 --> 00:03:23,230
with this for those we're not familiar

75
00:03:23,230 --> 00:03:24,579
with super feat what super filter is

76
00:03:24,579 --> 00:03:27,730
it's basically a distributed

77
00:03:27,730 --> 00:03:30,370
architecture to push RSS feeds in real

78
00:03:30,370 --> 00:03:32,169
time so people will come to us give us

79
00:03:32,169 --> 00:03:33,910
to use lists of feeds and when you say

80
00:03:33,910 --> 00:03:35,169
people it's mostly companies they come

81
00:03:35,169 --> 00:03:37,120
to us and say hey I've got like half a

82
00:03:37,120 --> 00:03:38,349
million houses feeds that I need to

83
00:03:38,349 --> 00:03:40,419
fetch I just don't wanna do it please

84
00:03:40,419 --> 00:03:42,130
push it to me so we try to get this data

85
00:03:42,130 --> 00:03:44,380
build this massive pipe of data and push

86
00:03:44,380 --> 00:03:45,759
it to them in real time as soon as we

87
00:03:45,759 --> 00:03:46,550
can

88
00:03:46,550 --> 00:03:48,890
doing this has a lot of a lot of

89
00:03:48,890 --> 00:03:51,050
challenges that we need to learn we need

90
00:03:51,050 --> 00:03:53,270
to solve the first one is obviously a

91
00:03:53,270 --> 00:03:54,770
memory constraint if the pipe is huge

92
00:03:54,770 --> 00:03:56,930
getting all this data in real time and

93
00:03:56,930 --> 00:03:58,760
when when I'm talking about all this

94
00:03:58,760 --> 00:04:00,250
data it's a couple of hundreds or

95
00:04:00,250 --> 00:04:02,360
thousands of updates per second

96
00:04:02,360 --> 00:04:04,700
it's it has a huge memory impact and you

97
00:04:04,700 --> 00:04:06,110
need to store it and forget it as soon

98
00:04:06,110 --> 00:04:07,790
as you can because if you even store it

99
00:04:07,790 --> 00:04:09,230
for like a minute or so in memory then

100
00:04:09,230 --> 00:04:12,280
everything explodes the other

101
00:04:12,280 --> 00:04:14,000
consequence of this as well is like

102
00:04:14,000 --> 00:04:15,890
there is no like well I'm gonna get all

103
00:04:15,890 --> 00:04:17,390
the data and then when I'm done with it

104
00:04:17,390 --> 00:04:19,160
then I'll start processing it like the

105
00:04:19,160 --> 00:04:20,930
data comes to you and it never ends it's

106
00:04:20,930 --> 00:04:22,370
actually even increasing in volume so

107
00:04:22,370 --> 00:04:24,170
you start to have to do it right away

108
00:04:24,170 --> 00:04:25,640
when it happens you need to process it

109
00:04:25,640 --> 00:04:28,640
and then fire-and-forget again it's also

110
00:04:28,640 --> 00:04:30,320
real time like you cannot again wait for

111
00:04:30,320 --> 00:04:32,570
the data to be processed before you send

112
00:04:32,570 --> 00:04:33,200
it to the next step

113
00:04:33,200 --> 00:04:36,140
you cannot Twitter cannot wait for the

114
00:04:36,140 --> 00:04:37,640
number of retweets or start on an item

115
00:04:37,640 --> 00:04:38,780
before they actually show you on the

116
00:04:38,780 --> 00:04:40,580
timeline like it arrives they show it to

117
00:04:40,580 --> 00:04:42,110
you and then maybe later that will show

118
00:04:42,110 --> 00:04:43,310
that there's been retweets and there's

119
00:04:43,310 --> 00:04:45,500
been stars or favourites on your on your

120
00:04:45,500 --> 00:04:48,770
on your content this actually changes

121
00:04:48,770 --> 00:04:51,140
pretty much everything that we've known

122
00:04:51,140 --> 00:04:53,120
and used when building applications

123
00:04:53,120 --> 00:04:54,770
including and starting with algorithms

124
00:04:54,770 --> 00:04:56,540
that you cannot use what we've used so

125
00:04:56,540 --> 00:04:58,880
far to build applications algorithms

126
00:04:58,880 --> 00:04:59,990
that we've been that we've been using

127
00:04:59,990 --> 00:05:03,680
for years to build the next app the easy

128
00:05:03,680 --> 00:05:05,330
solution that rubidium I'm sure a

129
00:05:05,330 --> 00:05:06,860
starting thing about is like well why

130
00:05:06,860 --> 00:05:08,630
can't we do just windowing or sampling

131
00:05:08,630 --> 00:05:10,070
when doing it would basically be like

132
00:05:10,070 --> 00:05:11,690
fine I'm gonna keep the data for like a

133
00:05:11,690 --> 00:05:14,090
minute do all my competition then trash

134
00:05:14,090 --> 00:05:15,890
it and then start again so buffering for

135
00:05:15,890 --> 00:05:17,290
like wait a minute or a couple seconds

136
00:05:17,290 --> 00:05:21,710
this works even though it's very it's

137
00:05:21,710 --> 00:05:24,320
also very inaccurate in a way that if

138
00:05:24,320 --> 00:05:25,850
you want to start growing the window you

139
00:05:25,850 --> 00:05:27,710
will eventually come back to the memory

140
00:05:27,710 --> 00:05:29,780
foam that we've had sampling is maybe

141
00:05:29,780 --> 00:05:31,250
rather than keeping all the data is

142
00:05:31,250 --> 00:05:33,590
keeping maybe one every ten or one every

143
00:05:33,590 --> 00:05:35,330
100 and then you start seeing patterns

144
00:05:35,330 --> 00:05:37,550
from this same problem you might be

145
00:05:37,550 --> 00:05:39,530
missing what the important important

146
00:05:39,530 --> 00:05:42,460
data is and you actually want to have a

147
00:05:42,460 --> 00:05:44,720
global vision and compute on the whole

148
00:05:44,720 --> 00:05:46,070
set so we're not going to talk about

149
00:05:46,070 --> 00:05:47,150
windowing and sampling in this

150
00:05:47,150 --> 00:05:50,600
presentation so let's start with Max and

151
00:05:50,600 --> 00:05:51,800
min this is actually very easy if you

152
00:05:51,800 --> 00:05:53,720
get a flow of data that comes to you you

153
00:05:53,720 --> 00:05:54,979
want to know what is the max what is the

154
00:05:54,979 --> 00:05:56,870
mean the most common use case for this

155
00:05:56,870 --> 00:05:57,710
oops sorry

156
00:05:57,710 --> 00:06:00,289
is obviously finding kind of the best

157
00:06:00,289 --> 00:06:03,200
the winner in a set of data that coming

158
00:06:03,200 --> 00:06:05,690
that comes to you that's easy that's

159
00:06:05,690 --> 00:06:07,970
really really easy you just keep a track

160
00:06:07,970 --> 00:06:09,800
of the maximum that you've seen if the

161
00:06:09,800 --> 00:06:11,990
new data that you get is bigger than the

162
00:06:11,990 --> 00:06:12,979
previous one then this is the new

163
00:06:12,979 --> 00:06:14,990
maximum and then keep keep will keep

164
00:06:14,990 --> 00:06:17,870
doing this it's it's pretty simple I'm

165
00:06:17,870 --> 00:06:19,160
gonna build a little JavaScript code and

166
00:06:19,160 --> 00:06:20,960
so that you can see with a visualization

167
00:06:20,960 --> 00:06:23,090
how it's done we have an object called

168
00:06:23,090 --> 00:06:24,800
stream F which is basically doing math

169
00:06:24,800 --> 00:06:28,820
better than hopefully regular JavaScript

170
00:06:28,820 --> 00:06:31,039
does it gets data through the feed

171
00:06:31,039 --> 00:06:33,530
method method compares the maximum with

172
00:06:33,530 --> 00:06:35,450
the new value the new values greater

173
00:06:35,450 --> 00:06:40,550
than keep it as the max we to test this

174
00:06:40,550 --> 00:06:42,200
and to show presentation let's do a

175
00:06:42,200 --> 00:06:43,280
little thing that generates number

176
00:06:43,280 --> 00:06:44,870
randomly so this is a function that

177
00:06:44,870 --> 00:06:47,810
generates a number randomly and they're

178
00:06:47,810 --> 00:06:50,960
arbitrarily low I mean there's no normal

179
00:06:50,960 --> 00:06:55,070
distribution and this new input thing

180
00:06:55,070 --> 00:06:57,950
just yields number every 10 milliseconds

181
00:06:57,950 --> 00:07:00,440
between 0 and 1 using the distribution

182
00:07:00,440 --> 00:07:03,530
determined here and we just put this

183
00:07:03,530 --> 00:07:05,900
that we can put that into blue plot so

184
00:07:05,900 --> 00:07:10,010
let's do this just gonna run this you

185
00:07:10,010 --> 00:07:12,349
see that's basically what it does and if

186
00:07:12,349 --> 00:07:14,210
I put that into a new plot and then plot

187
00:07:14,210 --> 00:07:18,410
this hopefully you will see this which

188
00:07:18,410 --> 00:07:20,479
makes sense like the Green Line is the

189
00:07:20,479 --> 00:07:22,700
maximum and all the points are the new

190
00:07:22,700 --> 00:07:24,110
points that are being added forever

191
00:07:24,110 --> 00:07:25,520
looks like I'm missing the end of the

192
00:07:25,520 --> 00:07:29,840
screen here so what we can see here very

193
00:07:29,840 --> 00:07:32,780
easily is basically this algorithm as a

194
00:07:32,780 --> 00:07:37,729
little problem here it knows too much

195
00:07:37,729 --> 00:07:39,380
basically it keeps track of the old

196
00:07:39,380 --> 00:07:41,060
maximum even though like maybe in a year

197
00:07:41,060 --> 00:07:43,220
this all Maxima is not really worth

198
00:07:43,220 --> 00:07:44,599
remembering we need to make sure that

199
00:07:44,599 --> 00:07:46,220
the algorithm is smart enough to forget

200
00:07:46,220 --> 00:07:48,229
and how do we do that without again

201
00:07:48,229 --> 00:07:50,930
keeping you a window it's it's actually

202
00:07:50,930 --> 00:07:53,479
pretty easy we just use what we call

203
00:07:53,479 --> 00:07:56,660
amortize values so doing this we rather

204
00:07:56,660 --> 00:07:58,610
than keeping track of I mean we keep top

205
00:07:58,610 --> 00:07:59,930
of the row value but rather than

206
00:07:59,930 --> 00:08:02,030
covering the row value we add some kind

207
00:08:02,030 --> 00:08:03,560
of factor which maybe is the time at

208
00:08:03,560 --> 00:08:05,000
which we've seen this value or the index

209
00:08:05,000 --> 00:08:08,270
in the sequence and by incrementing this

210
00:08:08,270 --> 00:08:10,099
we know that all the values eventually

211
00:08:10,099 --> 00:08:10,350
are going

212
00:08:10,350 --> 00:08:13,020
be smaller I mean for two values that

213
00:08:13,020 --> 00:08:14,550
are the same the oldest one is going to

214
00:08:14,550 --> 00:08:16,800
be smaller than the new one basically

215
00:08:16,800 --> 00:08:21,420
and that actually works pretty well I'm

216
00:08:21,420 --> 00:08:35,010
gonna go back here if we go back to the

217
00:08:35,010 --> 00:08:36,930
visualization here we see that quite

218
00:08:36,930 --> 00:08:39,960
quickly obviously the the highest one or

219
00:08:39,960 --> 00:08:42,180
kept in track but if there's nothing

220
00:08:42,180 --> 00:08:43,680
very high for a long time then it's

221
00:08:43,680 --> 00:08:46,320
slowly the the next higher value is kind

222
00:08:46,320 --> 00:08:50,240
of the new maximum does that make sense

223
00:08:50,240 --> 00:08:52,350
so this is an easy way to compute

224
00:08:52,350 --> 00:08:53,910
maximum and have some kind of memory and

225
00:08:53,910 --> 00:08:56,160
like not just take the first highest

226
00:08:56,160 --> 00:08:58,680
value that we have forever being the top

227
00:08:58,680 --> 00:09:03,780
one all right it works pretty much the

228
00:09:03,780 --> 00:09:05,340
same with minimums rather than adding

229
00:09:05,340 --> 00:09:07,470
value if you see then you remove the

230
00:09:07,470 --> 00:09:09,960
value at which you are you've seen them

231
00:09:09,960 --> 00:09:12,630
okay next step is the average this is a

232
00:09:12,630 --> 00:09:15,690
slightly more complex problem in the way

233
00:09:15,690 --> 00:09:17,460
that usually you compute averages by

234
00:09:17,460 --> 00:09:18,900
summing all the values that you've seen

235
00:09:18,900 --> 00:09:20,370
in the dataset and then you count all

236
00:09:20,370 --> 00:09:22,260
these values and you divide the first by

237
00:09:22,260 --> 00:09:24,570
the second one problem that we have

238
00:09:24,570 --> 00:09:26,430
obviously we do not try keep of the data

239
00:09:26,430 --> 00:09:28,140
we don't keep track of the of the values

240
00:09:28,140 --> 00:09:30,540
so we have no way of summing them and

241
00:09:30,540 --> 00:09:32,520
counting them afterwards well the easy

242
00:09:32,520 --> 00:09:34,140
way to do this is to keep running counts

243
00:09:34,140 --> 00:09:36,390
so rather than just having the data and

244
00:09:36,390 --> 00:09:37,800
comparing to the previous one that we

245
00:09:37,800 --> 00:09:40,530
did before now we keep track of both the

246
00:09:40,530 --> 00:09:42,780
sum and the count works pretty well as

247
00:09:42,780 --> 00:09:54,430
well as we can see here

248
00:09:54,440 --> 00:09:56,810
so as you can see I'm pretty sure you

249
00:09:56,810 --> 00:09:59,120
may be them the function that we've used

250
00:09:59,120 --> 00:10:01,730
here is actually a random number between

251
00:10:01,730 --> 00:10:05,449
0 and 100 and quickly as we've seen here

252
00:10:05,449 --> 00:10:07,759
as we can see here the pH converges

253
00:10:07,759 --> 00:10:09,110
toward 50 that's the green eye in the

254
00:10:09,110 --> 00:10:11,149
middle can everybody see it yeah

255
00:10:11,149 --> 00:10:14,089
quickly converges with around the middle

256
00:10:14,089 --> 00:10:16,399
of the curve this is great but the

257
00:10:16,399 --> 00:10:19,009
problem is sometimes your data set might

258
00:10:19,009 --> 00:10:26,329
have interesting properties so like like

259
00:10:26,329 --> 00:10:30,079
a different distribution and the problem

260
00:10:30,079 --> 00:10:31,220
that we see with different distribution

261
00:10:31,220 --> 00:10:40,509
is is obviously that if you have a

262
00:10:40,509 --> 00:10:42,759
distribution that does basically a

263
00:10:42,759 --> 00:10:46,310
sinusoid quickly the average becomes

264
00:10:46,310 --> 00:10:47,689
uninteresting because it just sits in

265
00:10:47,689 --> 00:10:49,250
the middle and doesn't really give you

266
00:10:49,250 --> 00:10:51,439
any day any insight on what the data is

267
00:10:51,439 --> 00:10:54,199
I've started this 10,000 milliseconds

268
00:10:54,199 --> 00:10:55,670
ago and it's already very close to zero

269
00:10:55,670 --> 00:10:58,209
if I keep this running for like an hour

270
00:10:58,209 --> 00:11:00,439
nobody will be able to see anything in

271
00:11:00,439 --> 00:11:03,730
the India bridge it would be meaningless

272
00:11:03,730 --> 00:11:08,689
the the the value the the solution to to

273
00:11:08,689 --> 00:11:11,240
counter this is to kind of wait the

274
00:11:11,240 --> 00:11:13,880
averages and rather than having the same

275
00:11:13,880 --> 00:11:16,970
wait for all the values in the past that

276
00:11:16,970 --> 00:11:19,430
the the new data has the new the new I

277
00:11:19,430 --> 00:11:20,899
mean rather than having the same wait

278
00:11:20,899 --> 00:11:23,269
for all the data points that you've had

279
00:11:23,269 --> 00:11:27,019
just use the previous average to come to

280
00:11:27,019 --> 00:11:28,759
compute the new average with the new

281
00:11:28,759 --> 00:11:30,620
number so basically you take the average

282
00:11:30,620 --> 00:11:34,009
at the t minus 1 multiplied by weight

283
00:11:34,009 --> 00:11:36,860
add the new value and then divide by the

284
00:11:36,860 --> 00:11:38,630
weight plus 1 what this does is

285
00:11:38,630 --> 00:11:40,850
basically it just shrinks the the

286
00:11:40,850 --> 00:11:43,339
important of the older values into the

287
00:11:43,339 --> 00:11:45,620
into the into the current weight the

288
00:11:45,620 --> 00:11:49,899
current average and so we can do this

289
00:11:49,899 --> 00:12:00,510
here very easily

290
00:12:00,520 --> 00:12:04,190
and we can see that in this context

291
00:12:04,190 --> 00:12:06,820
basically the the average still is

292
00:12:06,820 --> 00:12:09,530
obviously lower than the maximum and the

293
00:12:09,530 --> 00:12:10,760
minimum because it's an average but at

294
00:12:10,760 --> 00:12:12,350
the same time it's it slowly starts to

295
00:12:12,350 --> 00:12:14,570
not decrease anymore not change much

296
00:12:14,570 --> 00:12:17,450
based on what the input is so this is

297
00:12:17,450 --> 00:12:19,070
obviously bending the math in a way that

298
00:12:19,070 --> 00:12:21,110
is it's not the actual average over the

299
00:12:21,110 --> 00:12:22,610
whole data stream but again the whole

300
00:12:22,610 --> 00:12:24,770
data seam doesn't mean anything and when

301
00:12:24,770 --> 00:12:27,110
you have this size of data you need to

302
00:12:27,110 --> 00:12:28,880
be able to have a kind of a running

303
00:12:28,880 --> 00:12:32,360
average without the windows and keeping

304
00:12:32,360 --> 00:12:36,200
the data which is exactly what we do

305
00:12:36,200 --> 00:12:39,230
here the next one is standard deviation

306
00:12:39,230 --> 00:12:41,180
this really starts to be complex because

307
00:12:41,180 --> 00:12:43,160
I mean the main goal of standard

308
00:12:43,160 --> 00:12:44,540
deviation is to identify the outliers

309
00:12:44,540 --> 00:12:47,000
the the number that are either way above

310
00:12:47,000 --> 00:12:49,430
the average or way below the average you

311
00:12:49,430 --> 00:12:51,710
compare basically how far they are from

312
00:12:51,710 --> 00:12:55,430
the from the from the from the standard

313
00:12:55,430 --> 00:12:59,390
deviation the variance is the average of

314
00:12:59,390 --> 00:13:01,220
the distance to the average this is kind

315
00:13:01,220 --> 00:13:02,960
of the the math lesson that everybody

316
00:13:02,960 --> 00:13:04,400
learns basically you compute the average

317
00:13:04,400 --> 00:13:06,020
and then the variance is you compute the

318
00:13:06,020 --> 00:13:07,700
distance for between all the points and

319
00:13:07,700 --> 00:13:11,030
the average and you get what we call the

320
00:13:11,030 --> 00:13:13,700
variance the problem of this is as we've

321
00:13:13,700 --> 00:13:17,540
seen here it use it uses the average and

322
00:13:17,540 --> 00:13:18,650
if we change the average with every

323
00:13:18,650 --> 00:13:20,000
point we have we need to recompute the

324
00:13:20,000 --> 00:13:21,410
distance for all of the points that

325
00:13:21,410 --> 00:13:22,490
we've seen in the past we've the new

326
00:13:22,490 --> 00:13:25,640
average is that clear so that's kind of

327
00:13:25,640 --> 00:13:26,720
a problem because obviously we don't

328
00:13:26,720 --> 00:13:28,190
give the data and we don't really want

329
00:13:28,190 --> 00:13:29,390
to waste our time we're computing the

330
00:13:29,390 --> 00:13:31,910
average all the time well that's great

331
00:13:31,910 --> 00:13:36,050
because some scientist mathematician

332
00:13:36,050 --> 00:13:39,500
Koenig and ujin's have actually fine

333
00:13:39,500 --> 00:13:41,780
changed I mean not changed but like

334
00:13:41,780 --> 00:13:44,570
found that the the the equation as an

335
00:13:44,570 --> 00:13:46,190
equivalent that is much easier to use in

336
00:13:46,190 --> 00:13:48,140
our context which is rather than

337
00:13:48,140 --> 00:13:50,480
computing the average the distance to

338
00:13:50,480 --> 00:13:52,280
the average for all the values you keep

339
00:13:52,280 --> 00:13:54,560
track of a running square I mean the sum

340
00:13:54,560 --> 00:13:56,600
of the running square and you just

341
00:13:56,600 --> 00:14:00,260
abstract a square of the average and

342
00:14:00,260 --> 00:14:02,330
then so that's exactly what we're going

343
00:14:02,330 --> 00:14:04,780
to do here we have

344
00:14:04,780 --> 00:14:08,520
Bowlings we keep the basically the

345
00:14:08,520 --> 00:14:10,200
the new rolling number which is the sum

346
00:14:10,200 --> 00:14:11,910
of squares and for each new value that

347
00:14:11,910 --> 00:14:13,740
we get we still increment the Sam

348
00:14:13,740 --> 00:14:15,000
stealing from an account because we need

349
00:14:15,000 --> 00:14:17,850
it for the average we also keep track of

350
00:14:17,850 --> 00:14:21,120
the new sum of square and then when we

351
00:14:21,120 --> 00:14:23,430
when we need the standard deviation we

352
00:14:23,430 --> 00:14:27,240
just compute the square root of the sum

353
00:14:27,240 --> 00:14:29,550
of squares divided by the count minus

354
00:14:29,550 --> 00:14:32,280
the square of the average and we can do

355
00:14:32,280 --> 00:14:43,680
that as well you know we'll see that the

356
00:14:43,680 --> 00:14:45,870
oh this is the same distribution as

357
00:14:45,870 --> 00:14:48,240
before a random number between 1 and 100

358
00:14:48,240 --> 00:14:53,990
and the standard deviation stays around

359
00:14:53,990 --> 00:14:57,780
30 I guess yeah below of this which is

360
00:14:57,780 --> 00:14:59,340
expected I guess for this kind of

361
00:14:59,340 --> 00:15:03,360
distribution makes sense anyone is a

362
00:15:03,360 --> 00:15:13,890
question maybe not okay all right what

363
00:15:13,890 --> 00:15:26,820
what do you don't understand

364
00:15:26,830 --> 00:15:32,180
all right no it's it's my fault I should

365
00:15:32,180 --> 00:15:34,300
be able to tell you but basically like

366
00:15:34,300 --> 00:15:37,310
the regular Matt the point here is the

367
00:15:37,310 --> 00:15:39,770
regular standard deviation calculation

368
00:15:39,770 --> 00:15:41,300
cannot work because obviously we need to

369
00:15:41,300 --> 00:15:42,890
keep track of all the data right so we

370
00:15:42,890 --> 00:15:45,740
transform that equation and use another

371
00:15:45,740 --> 00:15:48,410
one that allows us to do incremental

372
00:15:48,410 --> 00:15:49,910
calculation of the standard deviation

373
00:15:49,910 --> 00:15:53,030
that make sense better

374
00:15:53,030 --> 00:15:56,330
okay next one medians and percentiles

375
00:15:56,330 --> 00:15:59,480
this one is nearly I mean it is at this

376
00:15:59,480 --> 00:16:01,700
point not possible without keeping some

377
00:16:01,700 --> 00:16:04,030
kind of window or some kind of sampling

378
00:16:04,030 --> 00:16:07,070
which is sad but it also makes sense

379
00:16:07,070 --> 00:16:08,660
because the median is trying to find

380
00:16:08,660 --> 00:16:14,150
which value divides the set of data into

381
00:16:14,150 --> 00:16:17,060
two sets of the same size one of Greater

382
00:16:17,060 --> 00:16:18,740
elements and the other one of smaller

383
00:16:18,740 --> 00:16:20,780
elements there's a way to do that with

384
00:16:20,780 --> 00:16:22,820
two heaps by keeping track of some

385
00:16:22,820 --> 00:16:24,650
elements so it can still be pretty low

386
00:16:24,650 --> 00:16:26,960
in terms of memory the algorithm is

387
00:16:26,960 --> 00:16:30,920
relatedly simple the first one if start

388
00:16:30,920 --> 00:16:33,260
with obviously 2m g hips and then get

389
00:16:33,260 --> 00:16:33,550
going

390
00:16:33,550 --> 00:16:35,660
when you get a new number if it's

391
00:16:35,660 --> 00:16:37,520
greater than the previously calculated

392
00:16:37,520 --> 00:16:40,100
median ID to the greater the heap of

393
00:16:40,100 --> 00:16:42,020
greater numbers if you get a small one

394
00:16:42,020 --> 00:16:43,370
then you hide it to the other one and

395
00:16:43,370 --> 00:16:47,480
then basically if the length of either

396
00:16:47,480 --> 00:16:48,800
EEP is greater than the other one you

397
00:16:48,800 --> 00:16:50,090
rebalance so you take one element from

398
00:16:50,090 --> 00:16:51,500
one and you put it in the other one and

399
00:16:51,500 --> 00:16:54,500
the median is always the smallest value

400
00:16:54,500 --> 00:16:58,370
of the greater hip that makes sense so

401
00:16:58,370 --> 00:17:00,320
it's basically you always keep track of

402
00:17:00,320 --> 00:17:02,960
the middle here the prime again if like

403
00:17:02,960 --> 00:17:06,380
if you try to do this on a long time you

404
00:17:06,380 --> 00:17:08,300
will keep the the two heaps will

405
00:17:08,300 --> 00:17:09,410
obviously keep increasing so you have a

406
00:17:09,410 --> 00:17:11,600
huge amount of memory that you need to

407
00:17:11,600 --> 00:17:16,640
keep luckily you can still king the data

408
00:17:16,640 --> 00:17:19,370
by removing data from the two heaps the

409
00:17:19,370 --> 00:17:20,630
problem of this is that you need to make

410
00:17:20,630 --> 00:17:21,530
sure that you don't remove too much

411
00:17:21,530 --> 00:17:23,390
because obviously if all of a sudden

412
00:17:23,390 --> 00:17:25,460
your your time series or your series is

413
00:17:25,460 --> 00:17:27,500
slowly decreasing you will slowly move

414
00:17:27,500 --> 00:17:28,790
all the data from one hip to the other

415
00:17:28,790 --> 00:17:29,990
one and then you won't have enough data

416
00:17:29,990 --> 00:17:32,390
to keep track of what is the what if the

417
00:17:32,390 --> 00:17:35,240
what is the the median here which is a

418
00:17:35,240 --> 00:17:37,790
problem

419
00:17:37,800 --> 00:17:41,580
all right another one that is

420
00:17:41,580 --> 00:17:44,010
interesting is in a huge data if you've

421
00:17:44,010 --> 00:17:46,950
got a huge data set and of incoming data

422
00:17:46,950 --> 00:17:48,600
and you want to keep track of the thing

423
00:17:48,600 --> 00:17:50,070
that you might have already seen and

424
00:17:50,070 --> 00:17:52,140
make sure that either you've already

425
00:17:52,140 --> 00:17:53,550
seen because you don't want to look it

426
00:17:53,550 --> 00:17:55,610
up or you don't you want to make sure

427
00:17:55,610 --> 00:18:00,440
any context of say a distributed

428
00:18:00,440 --> 00:18:02,580
algorithm that would run across a used

429
00:18:02,580 --> 00:18:04,440
data set you want to identify unique

430
00:18:04,440 --> 00:18:09,180
profiles on social networks you would

431
00:18:09,180 --> 00:18:10,950
have to keep track of all the profile

432
00:18:10,950 --> 00:18:13,350
that you've seen which can be too big so

433
00:18:13,350 --> 00:18:17,160
a good way to do this would be to use to

434
00:18:17,160 --> 00:18:19,410
use bloom filters basically bloom

435
00:18:19,410 --> 00:18:21,930
filters work in a way that there are

436
00:18:21,930 --> 00:18:23,190
very space efficient probability

437
00:18:23,190 --> 00:18:25,020
probably probabilistic data structures

438
00:18:25,020 --> 00:18:28,580
that uses hash functions and and

439
00:18:28,580 --> 00:18:32,940
dimensions to identify items that you

440
00:18:32,940 --> 00:18:34,230
might have already seen so you set bits

441
00:18:34,230 --> 00:18:36,990
to one when you've seen some of the ITM

442
00:18:36,990 --> 00:18:40,590
using the hashes the problem with bloom

443
00:18:40,590 --> 00:18:42,750
filters is you can have false positive

444
00:18:42,750 --> 00:18:46,200
which we'll see later which means that

445
00:18:46,200 --> 00:18:47,520
they will tell you that you might have

446
00:18:47,520 --> 00:18:49,260
already seen some piece of data even

447
00:18:49,260 --> 00:18:51,240
though you haven't seen it already but

448
00:18:51,240 --> 00:18:52,860
you can never have false negative which

449
00:18:52,860 --> 00:18:54,150
would be saying that you've seen

450
00:18:54,150 --> 00:18:55,830
something that you actually have you

451
00:18:55,830 --> 00:18:56,610
haven't seen something that you actually

452
00:18:56,610 --> 00:19:00,180
have seen before it's also very

453
00:19:00,180 --> 00:19:03,420
efficient and in increasing the the I

454
00:19:03,420 --> 00:19:04,440
mean decreasing the probability of

455
00:19:04,440 --> 00:19:07,500
errors is kind of easy because you add a

456
00:19:07,500 --> 00:19:09,660
little couple bits and it's and it's

457
00:19:09,660 --> 00:19:11,400
still very small to reduce significantly

458
00:19:11,400 --> 00:19:13,620
the amount of false positives that you

459
00:19:13,620 --> 00:19:17,340
may see on the data set here's a little

460
00:19:17,340 --> 00:19:18,960
example that I've stolen online and it

461
00:19:18,960 --> 00:19:20,460
actually works pretty well so basically

462
00:19:20,460 --> 00:19:22,740
for each data that you see you will add

463
00:19:22,740 --> 00:19:25,290
it to the your vector so these are the

464
00:19:25,290 --> 00:19:26,730
blue dots can you see the blue eyes

465
00:19:26,730 --> 00:19:28,320
maybe they're too small but basically

466
00:19:28,320 --> 00:19:36,409
for each word that you see you add them

467
00:19:36,419 --> 00:19:38,470
something's wrong okay so basically you

468
00:19:38,470 --> 00:19:39,970
see like the yellow ones or the one that

469
00:19:39,970 --> 00:19:41,320
I've added previously the blue ones are

470
00:19:41,320 --> 00:19:42,970
the one that I've just added so every

471
00:19:42,970 --> 00:19:44,980
time I add a new word I'm gonna add

472
00:19:44,980 --> 00:19:48,059
Billy in here it will just add new

473
00:19:48,059 --> 00:19:50,049
rectangles of the data that I've seen

474
00:19:50,049 --> 00:19:54,400
and if I look for some data it will tell

475
00:19:54,400 --> 00:19:55,990
me if it's Darren or not so I've added

476
00:19:55,990 --> 00:19:57,820
Berlin Paris is not there if I look for

477
00:19:57,820 --> 00:20:01,780
building obviously no it doesn't work

478
00:20:01,780 --> 00:20:03,669
something is wrong here obviously it's

479
00:20:03,669 --> 00:20:11,880
there sorry oh ah yes good point okay

480
00:20:11,880 --> 00:20:16,840
and basically Berlin now is in there so

481
00:20:16,840 --> 00:20:18,690
bloom filters are a great way to

482
00:20:18,690 --> 00:20:20,500
identify data at the problem that they

483
00:20:20,500 --> 00:20:24,130
have though is if you input a large

484
00:20:24,130 --> 00:20:25,630
enough chunk of data you will end up

485
00:20:25,630 --> 00:20:28,059
increasing the amount of false positive

486
00:20:28,059 --> 00:20:29,710
by a lot and you may eventually end up

487
00:20:29,710 --> 00:20:31,450
to a point where the bloom filter will

488
00:20:31,450 --> 00:20:33,640
always say that it seemed that it you

489
00:20:33,640 --> 00:20:34,990
have already seen the data that you've

490
00:20:34,990 --> 00:20:36,640
included because it basically filled the

491
00:20:36,640 --> 00:20:40,030
whole data structure with once I'm gonna

492
00:20:40,030 --> 00:20:42,760
switch the next like and you have

493
00:20:42,760 --> 00:20:44,710
different ways to deal with this either

494
00:20:44,710 --> 00:20:47,740
you can time limit the these bomb

495
00:20:47,740 --> 00:20:48,909
features so basically you do a bloom

496
00:20:48,909 --> 00:20:51,039
filter per hour per minute per year and

497
00:20:51,039 --> 00:20:52,900
then you can do things like finding

498
00:20:52,900 --> 00:20:55,270
trending trends basically so if you do

499
00:20:55,270 --> 00:20:58,179
bloom filters per minute you fill it up

500
00:20:58,179 --> 00:20:59,200
and then every time you get the data

501
00:20:59,200 --> 00:21:00,730
that matches it you check if it was also

502
00:21:00,730 --> 00:21:02,020
in the last minute in the previous

503
00:21:02,020 --> 00:21:03,370
minute in the previous hour and you can

504
00:21:03,370 --> 00:21:05,230
see that a given term has been showing

505
00:21:05,230 --> 00:21:07,600
up pretty often in the past time very

506
00:21:07,600 --> 00:21:10,330
easily with this you can also extend

507
00:21:10,330 --> 00:21:13,179
them by adding more bytes to the vector

508
00:21:13,179 --> 00:21:14,740
the problem of this is like an infinite

509
00:21:14,740 --> 00:21:16,030
data stream obviously the number of

510
00:21:16,030 --> 00:21:17,470
bytes is gonna increase as well so it

511
00:21:17,470 --> 00:21:19,900
doesn't increase as fast but still you

512
00:21:19,900 --> 00:21:22,870
might with thousands and thousands and

513
00:21:22,870 --> 00:21:25,000
billions of items you may end up having

514
00:21:25,000 --> 00:21:27,610
to keep extending which eventually might

515
00:21:27,610 --> 00:21:30,190
be expensive in terms of memory you can

516
00:21:30,190 --> 00:21:32,200
do what we call counting filters which

517
00:21:32,200 --> 00:21:33,580
which are basically filters to which you

518
00:21:33,580 --> 00:21:35,200
add in remove data and you can either

519
00:21:35,200 --> 00:21:38,080
remove data by removing the data itself

520
00:21:38,080 --> 00:21:40,570
or just decide that you'd agreement all

521
00:21:40,570 --> 00:21:43,510
the elements in the bloom filter by one

522
00:21:43,510 --> 00:21:45,309
every amount of time so every 5 minutes

523
00:21:45,309 --> 00:21:47,440
every hour or so you just decide that

524
00:21:47,440 --> 00:21:49,130
the bloom filter is going to lose

525
00:21:49,130 --> 00:21:50,510
some of the data that it seems before

526
00:21:50,510 --> 00:21:52,460
that it's seen before using this

527
00:21:52,460 --> 00:21:57,080
technique and this is kind of the end

528
00:21:57,080 --> 00:21:59,510
here this was really barely scratching

529
00:21:59,510 --> 00:22:01,940
the surface of how you need to handle

530
00:22:01,940 --> 00:22:04,100
these kind of problems of streams rather

531
00:22:04,100 --> 00:22:06,530
than considering use datasets and state

532
00:22:06,530 --> 00:22:08,330
that you have I would definitely

533
00:22:08,330 --> 00:22:10,340
recommend that you look into this this

534
00:22:10,340 --> 00:22:11,090
is something that I've completely

535
00:22:11,090 --> 00:22:13,930
discovered a couple a couple months ago

536
00:22:13,930 --> 00:22:18,020
and this has been fascinating to me this

537
00:22:18,020 --> 00:22:19,760
is now the end I will happily take

538
00:22:19,760 --> 00:22:37,250
questions if you have any this

