1
00:00:10,560 --> 00:00:21,520

you

2
00:00:21,530 --> 00:00:23,730
but come to my talk about

3
00:00:23,730 --> 00:00:27,060
is awesome but low latency is sublime my

4
00:00:27,060 --> 00:00:29,460
name is hannah's i'm working for google

5
00:00:29,460 --> 00:00:32,700
in munich and I member of TV a team how

6
00:00:32,700 --> 00:00:36,870
many people in here know what v8 is yeah

7
00:00:36,870 --> 00:00:40,470
quite some you know right soviet is for

8
00:00:40,470 --> 00:00:42,089
people who don't know the JavaScript

9
00:00:42,089 --> 00:00:43,920
engine which is for example used in

10
00:00:43,920 --> 00:00:46,610
chrome and is the thingy that

11
00:00:46,610 --> 00:00:48,420
understands or at least tries to

12
00:00:48,420 --> 00:00:51,000
understand your JavaScript program and

13
00:00:51,000 --> 00:00:55,650
execute it right so what is latency and

14
00:00:55,650 --> 00:00:58,170
why is it important what we understand

15
00:00:58,170 --> 00:01:02,219
by latency is the time DVM in our case

16
00:01:02,219 --> 00:01:05,220
v8 interrupts your JavaScript program

17
00:01:05,220 --> 00:01:07,950
and has to do some work right and this

18
00:01:07,950 --> 00:01:10,920
work is mainly garbage collector work so

19
00:01:10,920 --> 00:01:15,300
the DVM has to when it runs out of

20
00:01:15,300 --> 00:01:17,160
memory it has to go through the

21
00:01:17,160 --> 00:01:19,200
allocated memory and figure out which

22
00:01:19,200 --> 00:01:21,090
memory is life and which one is dead and

23
00:01:21,090 --> 00:01:23,460
free the dead memories so that the

24
00:01:23,460 --> 00:01:25,200
JavaScript application can allocate more

25
00:01:25,200 --> 00:01:27,929
memory and this process can also come

26
00:01:27,929 --> 00:01:30,899
from the optimizing compiler so once in

27
00:01:30,899 --> 00:01:33,090
a while we decide that one function

28
00:01:33,090 --> 00:01:35,789
could run waste faster this is the time

29
00:01:35,789 --> 00:01:37,800
when the optimizing compiler kicks in

30
00:01:37,800 --> 00:01:40,140
optimizes the code and hopefully the

31
00:01:40,140 --> 00:01:42,239
code runs faster and better after that

32
00:01:42,239 --> 00:01:45,869
so these are all pauses which stop the

33
00:01:45,869 --> 00:01:49,140
chava script application where vm stuff

34
00:01:49,140 --> 00:01:51,179
happens and after that the travel script

35
00:01:51,179 --> 00:01:53,940
applications is resumed and we call it

36
00:01:53,940 --> 00:01:55,590
high latency right when these pauses

37
00:01:55,590 --> 00:01:58,679
alone and low latency when this boss is

38
00:01:58,679 --> 00:02:01,560
a short right and why do we want low

39
00:02:01,560 --> 00:02:05,039
latency because it really matters shank

40
00:02:05,039 --> 00:02:07,470
is something which is really visible to

41
00:02:07,470 --> 00:02:09,569
the user right probably people

42
00:02:09,569 --> 00:02:11,400
experience that in here right if you

43
00:02:11,400 --> 00:02:14,879
play your games or watch a video or an

44
00:02:14,879 --> 00:02:16,890
audio and they are like these chunks

45
00:02:16,890 --> 00:02:20,280
this chitters in between it's really not

46
00:02:20,280 --> 00:02:22,830
a nice user experience right and its

47
00:02:22,830 --> 00:02:24,840
really annoying right nobody likes that

48
00:02:24,840 --> 00:02:28,760
and Chang's gets worse and mobile

49
00:02:28,760 --> 00:02:30,930
platforms right mobile platforms are

50
00:02:30,930 --> 00:02:34,709
slower like the a3 of factor of three or

51
00:02:34,709 --> 00:02:36,890
four slower than a beefy lab

52
00:02:36,890 --> 00:02:39,530
top or a beefy desktop machine right so

53
00:02:39,530 --> 00:02:41,990
in mobile the change is even worse right

54
00:02:41,990 --> 00:02:47,060
so as a vm engineer we like performance

55
00:02:47,060 --> 00:02:49,220
right we'd like to tune performance but

56
00:02:49,220 --> 00:02:51,950
latency is something which is sometimes

57
00:02:51,950 --> 00:02:54,170
really on trade off with Rupert right

58
00:02:54,170 --> 00:02:56,360
you can get better latency but you pay

59
00:02:56,360 --> 00:02:58,520
in throughput for that so what we are

60
00:02:58,520 --> 00:03:01,250
trying to do is to keep this interrupt

61
00:03:01,250 --> 00:03:03,880
short but short we mean few milliseconds

62
00:03:03,880 --> 00:03:06,020
whatever that means rights basically

63
00:03:06,020 --> 00:03:09,260
around would be great to have pauses

64
00:03:09,260 --> 00:03:11,330
which are not longer than let's say five

65
00:03:11,330 --> 00:03:12,950
milliseconds of below one millisecond

66
00:03:12,950 --> 00:03:14,959
the shorter the better right the better

67
00:03:14,959 --> 00:03:17,750
user experience obviously and it would

68
00:03:17,750 --> 00:03:21,769
be nice to have predictable pauses right

69
00:03:21,769 --> 00:03:24,500
so it's like a real kind of moving in

70
00:03:24,500 --> 00:03:27,950
this soft real-time ballgame right where

71
00:03:27,950 --> 00:03:31,550
the developer may rely on some soft

72
00:03:31,550 --> 00:03:34,130
guarantees we give him right and that

73
00:03:34,130 --> 00:03:35,450
that would be nice it's not the case

74
00:03:35,450 --> 00:03:37,970
these days right but that would be the

75
00:03:37,970 --> 00:03:40,130
optimum outcome right that the vm has

76
00:03:40,130 --> 00:03:43,070
some promises for the developer and for

77
00:03:43,070 --> 00:03:44,870
example it deposit ends are never larger

78
00:03:44,870 --> 00:03:49,060
them and milliseconds or whatever so

79
00:03:49,060 --> 00:03:52,370
short overview of my talk so latency is

80
00:03:52,370 --> 00:03:55,489
an issue and we recently like start

81
00:03:55,489 --> 00:03:59,030
focusing in it and improving it and I'll

82
00:03:59,030 --> 00:04:01,400
give a shorter with you about our

83
00:04:01,400 --> 00:04:03,200
garbage collector and now optimizing

84
00:04:03,200 --> 00:04:05,720
compiler few basic how this stuff worked

85
00:04:05,720 --> 00:04:08,720
and the efforts we are taking their to

86
00:04:08,720 --> 00:04:11,570
keep Layton's low then I will show to

87
00:04:11,570 --> 00:04:14,299
latency benchmarks we're using to

88
00:04:14,299 --> 00:04:17,209
evaluate our optimizations show them in

89
00:04:17,209 --> 00:04:19,250
experiments how we improved over the

90
00:04:19,250 --> 00:04:21,919
last year and draw some conclusions and

91
00:04:21,919 --> 00:04:24,770
there should be time for Q&A maybe

92
00:04:24,770 --> 00:04:27,430
otherwise we talk offline after the talk

93
00:04:27,430 --> 00:04:30,950
so in v8 we have a generational garbage

94
00:04:30,950 --> 00:04:33,500
collector this project Lou is used by

95
00:04:33,500 --> 00:04:36,200
many bm's we have a relatively small new

96
00:04:36,200 --> 00:04:38,900
generation and they have use a semi

97
00:04:38,900 --> 00:04:41,690
space heap in a scavenger semi space

98
00:04:41,690 --> 00:04:44,090
means that we just use half of the

99
00:04:44,090 --> 00:04:46,130
memory so when we allocate stuff there

100
00:04:46,130 --> 00:04:48,830
that's okay the few objects and we run

101
00:04:48,830 --> 00:04:50,510
out of memory there now

102
00:04:50,510 --> 00:04:54,110
semi space the scavenger looks for the

103
00:04:54,110 --> 00:04:55,700
object is what the scavenger is doing

104
00:04:55,700 --> 00:04:57,590
it's basically walking throughout the

105
00:04:57,590 --> 00:04:59,120
life objects and as soon as it

106
00:04:59,120 --> 00:05:01,580
encounters the life objects it copies it

107
00:05:01,580 --> 00:05:03,950
over to the other half of the SME space

108
00:05:03,950 --> 00:05:06,290
let's say two objects survive here in

109
00:05:06,290 --> 00:05:09,170
our example there is a dead right so the

110
00:05:09,170 --> 00:05:13,640
scavenger is pretty fast then there is a

111
00:05:13,640 --> 00:05:15,920
no generation and this is typically the

112
00:05:15,920 --> 00:05:18,470
memory where objects end up dead

113
00:05:18,470 --> 00:05:21,020
survived for quite some time right and

114
00:05:21,020 --> 00:05:23,270
they ever use a mark and sweep garbage

115
00:05:23,270 --> 00:05:25,130
collect the mark and sweep means we

116
00:05:25,130 --> 00:05:27,200
calculate the transitive closure of our

117
00:05:27,200 --> 00:05:29,960
live objects go through this graph and

118
00:05:29,960 --> 00:05:33,350
mark all the objects we find that their

119
00:05:33,350 --> 00:05:36,470
life and after that when we computed the

120
00:05:36,470 --> 00:05:38,960
whole transitive closure we scan over

121
00:05:38,960 --> 00:05:42,110
the whole heap and free not marked stuff

122
00:05:42,110 --> 00:05:45,110
and for marked objects we just clear the

123
00:05:45,110 --> 00:05:47,140
markup it and go on so you see

124
00:05:47,140 --> 00:05:49,790
scavengers can be fast right the heap is

125
00:05:49,790 --> 00:05:52,100
smaller and they are fast when a lot of

126
00:05:52,100 --> 00:05:54,080
objects died in the young generation

127
00:05:54,080 --> 00:05:57,800
immediately and the old generation is

128
00:05:57,800 --> 00:06:00,260
big can be really big and garbage

129
00:06:00,260 --> 00:06:01,700
collections can take longer there right

130
00:06:01,700 --> 00:06:03,650
so let's fill up the semi space again

131
00:06:03,650 --> 00:06:06,920
and then let's say object survived well

132
00:06:06,920 --> 00:06:09,260
they eventually end up in the old

133
00:06:09,260 --> 00:06:13,970
generation okay so the main assumption

134
00:06:13,970 --> 00:06:15,500
between the generational garbage

135
00:06:15,500 --> 00:06:17,990
collector is that most of the objects

136
00:06:17,990 --> 00:06:20,870
that get allocated die young right they

137
00:06:20,870 --> 00:06:23,300
are l research out there that shows like

138
00:06:23,300 --> 00:06:25,820
for many many application that is true

139
00:06:25,820 --> 00:06:28,010
right and this already gives us some

140
00:06:28,010 --> 00:06:30,290
nice latency properties right this

141
00:06:30,290 --> 00:06:33,020
results in many short young generation

142
00:06:33,020 --> 00:06:36,980
collections which are shown you by s or

143
00:06:36,980 --> 00:06:39,620
I just saw this lights are not really IC

144
00:06:39,620 --> 00:06:43,880
shown all right now and once in a while

145
00:06:43,880 --> 00:06:47,120
when when many objects a wife they end

146
00:06:47,120 --> 00:06:49,190
up in the old generation and we have

147
00:06:49,190 --> 00:06:52,640
this long are marking three times one

148
00:06:52,640 --> 00:06:56,900
roofer should fix the slides here okay

149
00:06:56,900 --> 00:06:58,880
now that's gone I apologize for that it

150
00:06:58,880 --> 00:07:01,010
looks nice on the green screen here and

151
00:07:01,010 --> 00:07:01,640
there

152
00:07:01,640 --> 00:07:11,250
sorry yeah so we have once in a while

153
00:07:11,250 --> 00:07:13,440
this long atomic markings report well

154
00:07:13,440 --> 00:07:16,110
let's try to get again a full screen

155
00:07:16,110 --> 00:07:28,640
mode maybe it gets better

156
00:07:28,650 --> 00:07:49,430
no not really

157
00:07:49,440 --> 00:07:52,030
step better oh okay that looks better

158
00:07:52,030 --> 00:07:54,430
sorry for the interval that was high

159
00:07:54,430 --> 00:07:59,110
latency by the way so to get rid of

160
00:07:59,110 --> 00:08:02,800
these longer mark and sweet pauses what

161
00:08:02,800 --> 00:08:04,210
we implemented there is a technique

162
00:08:04,210 --> 00:08:06,850
called incremental marking that means

163
00:08:06,850 --> 00:08:09,730
when we do mark and sweep we don't do it

164
00:08:09,730 --> 00:08:12,130
all the work at once before we run out

165
00:08:12,130 --> 00:08:14,500
of memory we already do some marking

166
00:08:14,500 --> 00:08:16,450
work in advance to smaller marking steps

167
00:08:16,450 --> 00:08:18,730
until a point we decide that oh we

168
00:08:18,730 --> 00:08:21,520
probably have marked now enough and then

169
00:08:21,520 --> 00:08:24,100
do a final mark and sweep operation so

170
00:08:24,100 --> 00:08:26,170
it looks like that we smear over the

171
00:08:26,170 --> 00:08:28,540
whole JavaScript program execution

172
00:08:28,540 --> 00:08:30,550
smaller markings that which are really

173
00:08:30,550 --> 00:08:32,080
really short right they are shorter than

174
00:08:32,080 --> 00:08:34,630
a millisecond and then we do a final

175
00:08:34,630 --> 00:08:37,750
mark and sweep operation this improved

176
00:08:37,750 --> 00:08:39,820
latency significantly and user

177
00:08:39,820 --> 00:08:41,800
experience right this long marking face

178
00:08:41,800 --> 00:08:44,169
is now split up into many short ones and

179
00:08:44,169 --> 00:08:49,150
a big final marking face so how we can

180
00:08:49,150 --> 00:08:51,810
how can we do better there how we can we

181
00:08:51,810 --> 00:08:54,970
shrink the size of these final mark and

182
00:08:54,970 --> 00:08:57,880
sweep face well we can use multiple

183
00:08:57,880 --> 00:09:00,520
frets rights to for example the speed up

184
00:09:00,520 --> 00:09:02,500
the sweeping and what we implement that

185
00:09:02,500 --> 00:09:04,810
and editor we ate about half a year ago

186
00:09:04,810 --> 00:09:07,330
is a technique called parallel and

187
00:09:07,330 --> 00:09:09,790
concurrent sweeping parallel sweeping

188
00:09:09,790 --> 00:09:11,770
means that there are multiple threats

189
00:09:11,770 --> 00:09:15,010
helping out to free memory so each fret

190
00:09:15,010 --> 00:09:17,260
gets like one megabyte of the old

191
00:09:17,260 --> 00:09:19,660
generation heap and go through these

192
00:09:19,660 --> 00:09:21,760
megabyte and finds all the free objects

193
00:09:21,760 --> 00:09:23,440
and gives it back to the main

194
00:09:23,440 --> 00:09:25,330
applications so you can have multiple

195
00:09:25,330 --> 00:09:28,000
threats doing that to get a speed up by

196
00:09:28,000 --> 00:09:31,570
the number of frets and concurrent

197
00:09:31,570 --> 00:09:33,100
sweeping means that we can also run this

198
00:09:33,100 --> 00:09:35,380
sweeping phrase concurrently to the

199
00:09:35,380 --> 00:09:37,780
JavaScript application which means that

200
00:09:37,780 --> 00:09:39,730
this whipping phase can be completely

201
00:09:39,730 --> 00:09:42,670
transparent and invisible to the to the

202
00:09:42,670 --> 00:09:45,540
application so sweeping phase kind of

203
00:09:45,540 --> 00:09:48,460
goes completely away and the execution

204
00:09:48,460 --> 00:09:51,120
time of this final mark and sweep phase

205
00:09:51,120 --> 00:09:54,490
decreased even more so there is a

206
00:09:54,490 --> 00:09:56,350
trade-off right we have multiple threats

207
00:09:56,350 --> 00:09:58,060
which means higher system utilization

208
00:09:58,060 --> 00:10:01,120
and but we get wet latency out of it and

209
00:10:01,120 --> 00:10:02,620
one really as today

210
00:10:02,620 --> 00:10:04,330
earlier that right if you had too many

211
00:10:04,330 --> 00:10:06,970
frets and increased performance too much

212
00:10:06,970 --> 00:10:09,670
it can be an issue again on mobile which

213
00:10:09,670 --> 00:10:11,770
means system utilization typically

214
00:10:11,770 --> 00:10:16,530
translate into battery life right so

215
00:10:16,530 --> 00:10:20,530
then let's tackle next the scavenging

216
00:10:20,530 --> 00:10:22,630
times right as I said scavenging

217
00:10:22,630 --> 00:10:24,700
typically so the new generation

218
00:10:24,700 --> 00:10:26,800
collection doesn't take much time

219
00:10:26,800 --> 00:10:30,310
because of the main assumption that most

220
00:10:30,310 --> 00:10:32,680
of the objects die young but what if all

221
00:10:32,680 --> 00:10:36,070
the object survive or not all of most of

222
00:10:36,070 --> 00:10:38,530
the objects survive right what then well

223
00:10:38,530 --> 00:10:40,210
then scavenging is really really slow

224
00:10:40,210 --> 00:10:43,600
because as you remember when you a

225
00:10:43,600 --> 00:10:46,420
locate object and all alive you have to

226
00:10:46,420 --> 00:10:49,060
copy them all over in the semi space and

227
00:10:49,060 --> 00:10:51,520
thats get gets even worse right let's

228
00:10:51,520 --> 00:10:54,190
say they survive even longer well then

229
00:10:54,190 --> 00:10:56,110
you have to copy them over to the old

230
00:10:56,110 --> 00:10:58,240
generation so we had to do two copies in

231
00:10:58,240 --> 00:11:00,040
that case right and this is really the

232
00:11:00,040 --> 00:11:04,210
the worst case for for the generational

233
00:11:04,210 --> 00:11:06,610
garbage collector so how can we do

234
00:11:06,610 --> 00:11:09,310
better there and what we did is we

235
00:11:09,310 --> 00:11:12,070
implemented a technique called global

236
00:11:12,070 --> 00:11:15,340
preteen ring which sets kind of a global

237
00:11:15,340 --> 00:11:17,290
flag in the vm when we encounter that

238
00:11:17,290 --> 00:11:20,050
more than ninety percent of the objects

239
00:11:20,050 --> 00:11:22,630
survive and go from the new generation

240
00:11:22,630 --> 00:11:25,090
to the old generation we flip a switch

241
00:11:25,090 --> 00:11:28,240
and currently what we do is we have to

242
00:11:28,240 --> 00:11:31,470
dip demise all the generated code and

243
00:11:31,470 --> 00:11:35,160
when we generate fast code again we

244
00:11:35,160 --> 00:11:37,780
generated with a location code that

245
00:11:37,780 --> 00:11:39,820
allocates directly into the old

246
00:11:39,820 --> 00:11:42,820
generation and when we so what's going

247
00:11:42,820 --> 00:11:44,980
on is we allocate directly there we have

248
00:11:44,980 --> 00:11:47,260
white the copying and when you find out

249
00:11:47,260 --> 00:11:50,800
that ok most of the objects now die in

250
00:11:50,800 --> 00:11:52,750
the old generation for whatever reason

251
00:11:52,750 --> 00:11:54,250
right the application behavior may

252
00:11:54,250 --> 00:11:57,010
change well then we have to do p again

253
00:11:57,010 --> 00:11:59,920
and generate code again where everything

254
00:11:59,920 --> 00:12:03,760
gets allocated in a new generation it's

255
00:12:03,760 --> 00:12:05,560
kind of a global thing right now but we

256
00:12:05,560 --> 00:12:07,810
are trying to get this Pretender ring on

257
00:12:07,810 --> 00:12:10,750
a allocation side basis so on a local

258
00:12:10,750 --> 00:12:12,780
basis so that we can decide for each

259
00:12:12,780 --> 00:12:15,490
allocation call in JavaScript well

260
00:12:15,490 --> 00:12:16,329
should you

261
00:12:16,329 --> 00:12:17,980
yeah located in the new generation or

262
00:12:17,980 --> 00:12:20,110
would you better be in the old

263
00:12:20,110 --> 00:12:24,579
generation right alright so what happens

264
00:12:24,579 --> 00:12:26,499
there is for like let's say we have this

265
00:12:26,499 --> 00:12:30,489
worst-case application where a lot of

266
00:12:30,489 --> 00:12:32,679
stuff survives and we turn on the skill

267
00:12:32,679 --> 00:12:34,749
while pretending we have wet the copying

268
00:12:34,749 --> 00:12:36,610
and we can avoid these cabbages at all

269
00:12:36,610 --> 00:12:38,499
right because stuff immediately gets

270
00:12:38,499 --> 00:12:40,959
allocated in the old generation right

271
00:12:40,959 --> 00:12:42,730
that helped to improve latency

272
00:12:42,730 --> 00:12:44,670
significantly for this worst-case

273
00:12:44,670 --> 00:12:49,499
behavior applications all right so much

274
00:12:49,499 --> 00:12:52,600
for the garbage collector let's have a

275
00:12:52,600 --> 00:12:56,110
quick look at the our compiler as I

276
00:12:56,110 --> 00:12:57,670
mention in the beginning the compiler

277
00:12:57,670 --> 00:13:01,449
may also introduce some chanc right what

278
00:13:01,449 --> 00:13:05,559
we have in v8 is a we call the full code

279
00:13:05,559 --> 00:13:08,139
generator which is really really fast it

280
00:13:08,139 --> 00:13:12,129
is able to generate a good code i would

281
00:13:12,129 --> 00:13:14,319
say really fast and then there is the

282
00:13:14,319 --> 00:13:16,540
optimizing compiler and the optimizing

283
00:13:16,540 --> 00:13:18,519
compiler is used when we encounter that

284
00:13:18,519 --> 00:13:21,699
a function is executed many times then

285
00:13:21,699 --> 00:13:24,790
we say it's a hot function and we should

286
00:13:24,790 --> 00:13:27,189
generate faster code for this function

287
00:13:27,189 --> 00:13:30,309
right and when we find that out the

288
00:13:30,309 --> 00:13:31,959
travel sleep Fred next time and it

289
00:13:31,959 --> 00:13:34,089
executes the fred is doing the actual

290
00:13:34,089 --> 00:13:37,480
compilation right so it goes into vm is

291
00:13:37,480 --> 00:13:40,869
doing all the optimizations and this may

292
00:13:40,869 --> 00:13:42,730
take some time this actually depends on

293
00:13:42,730 --> 00:13:46,360
the code size right that gets compiled

294
00:13:46,360 --> 00:13:50,679
so you will have these vm passes but we

295
00:13:50,679 --> 00:13:51,610
saw before for the garbage collector

296
00:13:51,610 --> 00:13:54,850
also for a compiler and what we are

297
00:13:54,850 --> 00:13:57,459
doing there well quite similar to the

298
00:13:57,459 --> 00:13:59,970
garbage collector before we use

299
00:13:59,970 --> 00:14:02,079
mechanism called concurrent compilation

300
00:14:02,079 --> 00:14:04,929
so we use an extra fret that helps us

301
00:14:04,929 --> 00:14:07,329
out with doing compilation work right

302
00:14:07,329 --> 00:14:10,509
and we're trying to make more and more

303
00:14:10,509 --> 00:14:13,629
phases of the compiler concurrent which

304
00:14:13,629 --> 00:14:15,579
is quite tricky because the new

305
00:14:15,579 --> 00:14:17,410
compilation time you can allocate object

306
00:14:17,410 --> 00:14:19,299
or them the application itself may

307
00:14:19,299 --> 00:14:23,679
mutate objects which are kind of hooked

308
00:14:23,679 --> 00:14:25,480
up with the function we are now

309
00:14:25,480 --> 00:14:28,899
compiling so we try to get more and more

310
00:14:28,899 --> 00:14:29,840
phases

311
00:14:29,840 --> 00:14:33,500
of the optimizing compiler to make them

312
00:14:33,500 --> 00:14:36,490
concurrent and run in parallel with the

313
00:14:36,490 --> 00:14:40,100
JavaScript application the result there

314
00:14:40,100 --> 00:14:42,080
is of course we use compilation time

315
00:14:42,080 --> 00:14:43,820
right if you can stuff to do concurrent

316
00:14:43,820 --> 00:14:49,850
and a nice benefit of that is when we do

317
00:14:49,850 --> 00:14:51,500
compile stuff concurrently to the

318
00:14:51,500 --> 00:14:53,120
mutator we can actually do more

319
00:14:53,120 --> 00:14:54,710
aggressive optimizations right if

320
00:14:54,710 --> 00:14:58,130
there's time left and in the experiments

321
00:14:58,130 --> 00:15:00,860
show that there's always enough time

322
00:15:00,860 --> 00:15:04,100
left to do to do the compilation the

323
00:15:04,100 --> 00:15:05,870
extra fret we can do really heavy

324
00:15:05,870 --> 00:15:08,060
optimizations and generate even faster

325
00:15:08,060 --> 00:15:09,680
code which was not possible before

326
00:15:09,680 --> 00:15:12,200
because the post times would just be too

327
00:15:12,200 --> 00:15:14,210
long if we would do it all on the same

328
00:15:14,210 --> 00:15:19,280
JavaScript fret so all right now we

329
00:15:19,280 --> 00:15:22,750
covered quite some stuff in the vm that

330
00:15:22,750 --> 00:15:25,880
improved latency of the garbage collect

331
00:15:25,880 --> 00:15:28,220
and the optimizing compiler now let's

332
00:15:28,220 --> 00:15:31,880
see how we can measure that stuff so i'm

333
00:15:31,880 --> 00:15:34,520
now going to explain how we measure the

334
00:15:34,520 --> 00:15:36,440
stuff within the JavaScript application

335
00:15:36,440 --> 00:15:39,140
of course we could look at the vm output

336
00:15:39,140 --> 00:15:41,150
right we have in v8 we have several

337
00:15:41,150 --> 00:15:44,510
flags where we can ask DVM how long it

338
00:15:44,510 --> 00:15:46,910
took but we do we don't trust our output

339
00:15:46,910 --> 00:15:49,970
right so we really want to have the

340
00:15:49,970 --> 00:15:52,370
application tell us whether it was a

341
00:15:52,370 --> 00:15:55,340
good chunk experience or not right and

342
00:15:55,340 --> 00:15:57,350
the way we do that is we take a

343
00:15:57,350 --> 00:16:01,220
JavaScript program and add time probes

344
00:16:01,220 --> 00:16:03,590
to the application right so let's say we

345
00:16:03,590 --> 00:16:07,310
have a function that's allocating stuff

346
00:16:07,310 --> 00:16:10,220
and doing some calculations before and

347
00:16:10,220 --> 00:16:11,990
after the function we measure the time

348
00:16:11,990 --> 00:16:14,270
and we execute this function many many

349
00:16:14,270 --> 00:16:16,640
times right so in the best case this

350
00:16:16,640 --> 00:16:18,650
function should be always fast and it

351
00:16:18,650 --> 00:16:20,420
should we always take the same amount of

352
00:16:20,420 --> 00:16:23,120
time right except when DVM has to do

353
00:16:23,120 --> 00:16:25,120
some stuff like garbage collection or

354
00:16:25,120 --> 00:16:27,860
compilation then you may have outliers

355
00:16:27,860 --> 00:16:29,780
right so the samples which we measure

356
00:16:29,780 --> 00:16:32,600
may have different time spans and then

357
00:16:32,600 --> 00:16:34,730
when we run this function many many

358
00:16:34,730 --> 00:16:37,340
times we calculate the root mean square

359
00:16:37,340 --> 00:16:41,720
of this sample times and this function

360
00:16:41,720 --> 00:16:43,130
the root mean square

361
00:16:43,130 --> 00:16:46,100
heavily penalized outliers there right

362
00:16:46,100 --> 00:16:48,350
so this is what the square the formula

363
00:16:48,350 --> 00:16:50,420
is taking care of when you have a huge

364
00:16:50,420 --> 00:16:52,310
sample then you get penalized for that

365
00:16:52,310 --> 00:16:55,010
right which translates into a bad user

366
00:16:55,010 --> 00:16:57,020
experience right the first you can

367
00:16:57,020 --> 00:16:59,180
execute the samples and the more

368
00:16:59,180 --> 00:17:00,710
predictable they are the better the

369
00:17:00,710 --> 00:17:06,140
score will be so the benchmarks I'm

370
00:17:06,140 --> 00:17:08,329
using my experience here experiments

371
00:17:08,329 --> 00:17:11,000
here to benchmarks from our octane

372
00:17:11,000 --> 00:17:13,490
benchmark suit this is our JavaScript

373
00:17:13,490 --> 00:17:15,560
benchmark suit where we measure

374
00:17:15,560 --> 00:17:19,459
performance of v8 the first one is

375
00:17:19,459 --> 00:17:22,100
called splay this benchmarks creates a

376
00:17:22,100 --> 00:17:25,010
large splay tree and then modifies it

377
00:17:25,010 --> 00:17:27,860
and in this benchmark like almost all

378
00:17:27,860 --> 00:17:30,020
objects survive so this is kind of

379
00:17:30,020 --> 00:17:32,000
divorce case benchmark for a

380
00:17:32,000 --> 00:17:34,370
generational garbage collector right so

381
00:17:34,370 --> 00:17:36,440
when we want to prove this worst-case

382
00:17:36,440 --> 00:17:40,040
scenario dislike the worst-case post

383
00:17:40,040 --> 00:17:43,070
times we can observe and we're trying to

384
00:17:43,070 --> 00:17:45,560
make the better and for the compiler we

385
00:17:45,560 --> 00:17:48,350
look at them and real benchmark this one

386
00:17:48,350 --> 00:17:50,750
is also in the contain benchmarks field

387
00:17:50,750 --> 00:17:53,330
it runs the 3d bullet entry imported

388
00:17:53,330 --> 00:17:57,370
from C++ to JavaScript using module and

389
00:17:57,370 --> 00:17:59,990
the thing there is there like huge code

390
00:17:59,990 --> 00:18:03,050
chunks it's like thousands of lines of

391
00:18:03,050 --> 00:18:05,780
JavaScript code and when the optimizing

392
00:18:05,780 --> 00:18:09,770
compiler or the full kochan looks at the

393
00:18:09,770 --> 00:18:12,140
functions compilation times may be

394
00:18:12,140 --> 00:18:17,360
really really long so all right so this

395
00:18:17,360 --> 00:18:20,690
is what we did then let's look at the

396
00:18:20,690 --> 00:18:23,060
results the results both experiments run

397
00:18:23,060 --> 00:18:25,250
on a nexus 10 on a mobile device it's

398
00:18:25,250 --> 00:18:27,410
quite a beefy mobile device but I was

399
00:18:27,410 --> 00:18:31,310
lying on my desk and with Android 403

400
00:18:31,310 --> 00:18:33,410
let's look it's play first so this is

401
00:18:33,410 --> 00:18:35,840
the garbage collection benchmark on the

402
00:18:35,840 --> 00:18:38,660
x-axis you see the timeline starting in

403
00:18:38,660 --> 00:18:42,560
September last year until today and on

404
00:18:42,560 --> 00:18:44,420
the y-axis is the score so higher is

405
00:18:44,420 --> 00:18:46,160
better so we take the root-mean-square

406
00:18:46,160 --> 00:18:49,670
and just reverse to be higher this

407
00:18:49,670 --> 00:18:52,370
better and all these features i

408
00:18:52,370 --> 00:18:54,320
mentioned before except for the

409
00:18:54,320 --> 00:18:55,840
incremental marking where

410
00:18:55,840 --> 00:18:58,150
ship within the last year and one can

411
00:18:58,150 --> 00:19:00,880
see that we we doubled our our score on

412
00:19:00,880 --> 00:19:04,510
this benchmark which is quite nice if we

413
00:19:04,510 --> 00:19:07,929
look at the specific samples what we do

414
00:19:07,929 --> 00:19:10,029
here is like we do if you split three

415
00:19:10,029 --> 00:19:11,830
modifications and then we measure a time

416
00:19:11,830 --> 00:19:14,080
then go on do again like a predefined

417
00:19:14,080 --> 00:19:16,360
amount of splay tree modifications on

418
00:19:16,360 --> 00:19:18,940
average it takes 20 to 1 milliseconds

419
00:19:18,940 --> 00:19:21,669
but they're like some maximum outliers

420
00:19:21,669 --> 00:19:23,740
at the very beginning of the benchmark

421
00:19:23,740 --> 00:19:26,500
they go up to 46 milliseconds which is

422
00:19:26,500 --> 00:19:29,169
bad right so we can do better there we

423
00:19:29,169 --> 00:19:30,820
investigate and what we can do were

424
00:19:30,820 --> 00:19:34,240
there because 46 milliseconds means that

425
00:19:34,240 --> 00:19:37,270
there will be lost frames right and this

426
00:19:37,270 --> 00:19:39,789
is what you can also see when you

427
00:19:39,789 --> 00:19:42,429
started it with a visualization see a

428
00:19:42,429 --> 00:19:44,590
few chunks at the very beginning but

429
00:19:44,590 --> 00:19:46,390
then it runs smooth right is like three

430
00:19:46,390 --> 00:19:48,100
outliers we get in the beginning there

431
00:19:48,100 --> 00:19:51,130
the minimum sample time is below 1

432
00:19:51,130 --> 00:19:54,279
millisecond it's quite nice so form

433
00:19:54,279 --> 00:19:57,279
unreal we hook the measurement up with

434
00:19:57,279 --> 00:19:59,230
the frame rendering time so there's like

435
00:19:59,230 --> 00:20:01,809
this time there is like the rendering it

436
00:20:01,809 --> 00:20:04,029
sells going on and also the vm time on

437
00:20:04,029 --> 00:20:07,240
average it takes 90 milliseconds to

438
00:20:07,240 --> 00:20:09,159
render such a frame which includes like

439
00:20:09,159 --> 00:20:11,230
everything right there's also the

440
00:20:11,230 --> 00:20:12,700
computation work is also quite heavy

441
00:20:12,700 --> 00:20:15,940
there um but there's a huge outlier in

442
00:20:15,940 --> 00:20:18,490
the beginning right 249 milliseconds

443
00:20:18,490 --> 00:20:20,679
just like one outlier and then they are

444
00:20:20,679 --> 00:20:24,669
few around 40 milliseconds if I remember

445
00:20:24,669 --> 00:20:26,200
correctly and after that it runs smooth

446
00:20:26,200 --> 00:20:29,200
this is a time the first outlier is

447
00:20:29,200 --> 00:20:31,750
related to when the full coach and

448
00:20:31,750 --> 00:20:34,120
compiles the whole javascript file right

449
00:20:34,120 --> 00:20:36,460
so again you may get some chunk at the

450
00:20:36,460 --> 00:20:38,200
beginning when you execute the

451
00:20:38,200 --> 00:20:40,720
application but after that you define

452
00:20:40,720 --> 00:20:43,450
but again there's there stuff where we

453
00:20:43,450 --> 00:20:47,230
can do better all right that already

454
00:20:47,230 --> 00:20:51,070
brings me to the end of the talk low

455
00:20:51,070 --> 00:20:54,789
latency of the vm i guess is key to keep

456
00:20:54,789 --> 00:20:58,000
the user happy and make web application

457
00:20:58,000 --> 00:21:00,429
run well on all devices right not just

458
00:21:00,429 --> 00:21:03,490
on a macbook oh and a heavy desktop

459
00:21:03,490 --> 00:21:06,540
machine but also on on mobile mobile

460
00:21:06,540 --> 00:21:10,230
yeah it would be nice if javascript gems

461
00:21:10,230 --> 00:21:12,000
could provide predictable performance

462
00:21:12,000 --> 00:21:13,950
and give a promise to the application

463
00:21:13,950 --> 00:21:16,410
developers we're not quite there yet but

464
00:21:16,410 --> 00:21:19,560
I think that this is the direction we

465
00:21:19,560 --> 00:21:23,250
should head because many programmers try

466
00:21:23,250 --> 00:21:26,010
to work around to vm right like okay did

467
00:21:26,010 --> 00:21:28,170
you see make a kin so let's build our

468
00:21:28,170 --> 00:21:30,360
own memory management system within our

469
00:21:30,360 --> 00:21:32,250
JavaScript application and this is

470
00:21:32,250 --> 00:21:34,200
actually hard to do right the object

471
00:21:34,200 --> 00:21:36,090
pooling and stuff like that and the

472
00:21:36,090 --> 00:21:39,330
probability is high that one gets just

473
00:21:39,330 --> 00:21:41,910
wrong right and you end up with day with

474
00:21:41,910 --> 00:21:45,630
the same performance problems or it

475
00:21:45,630 --> 00:21:48,570
could be even worse right we already

476
00:21:48,570 --> 00:21:51,150
have some algorithms in v8 that

477
00:21:51,150 --> 00:21:53,460
eliminate allocations right so we

478
00:21:53,460 --> 00:21:55,760
recently landed an algorithm called

479
00:21:55,760 --> 00:21:58,440
which is doing escape analysis right it

480
00:21:58,440 --> 00:22:00,990
finds out whether an allocation escapes

481
00:22:00,990 --> 00:22:03,180
a function and if it's not then it's not

482
00:22:03,180 --> 00:22:05,760
actually allocated on the heap or other

483
00:22:05,760 --> 00:22:08,010
allocation folding which takes multiple

484
00:22:08,010 --> 00:22:09,990
allocations and folds them together into

485
00:22:09,990 --> 00:22:13,440
enter one right so we're trying to to a

486
00:22:13,440 --> 00:22:16,440
whitening get better there and the

487
00:22:16,440 --> 00:22:18,120
latency benchmarks are showed while

488
00:22:18,120 --> 00:22:20,100
they're quite important for us to keep

489
00:22:20,100 --> 00:22:23,040
track of performance improvement and

490
00:22:23,040 --> 00:22:26,130
regressions and they really help us to

491
00:22:26,130 --> 00:22:29,510
show where where we can do better and

492
00:22:29,510 --> 00:22:33,090
help us to evaluate new features all

493
00:22:33,090 --> 00:22:34,830
right that's the end of my talk thank

494
00:22:34,830 --> 00:22:36,780
you very much maybe there is time for

495
00:22:36,780 --> 00:22:49,020
questions I don't know how much time is

