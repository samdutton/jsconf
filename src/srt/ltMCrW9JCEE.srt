1
00:00:21,090 --> 00:00:23,290
Hi everyone.

2
00:00:23,290 --> 00:00:28,940
Today I'll tell you how to grow your
own Babel Fish.

3
00:00:28,940 --> 00:00:33,910
So it turns out that the metaphor is
not really that original, especially if you've

4
00:00:33,910 --> 00:00:41,570
seen Sam
stock from IBM Watson, but I'll briefly describe

5
00:00:41,570 --> 00:00:44,000
what
the Babel Fish is.

6
00:00:44,000 --> 00:00:48,100
Babel Fish is unfortunately
a fictional animal from Hitchhiker's Guide

7
00:00:48,100 --> 00:00:52,780
to the Galaxy
by Douglas Adams, and according to the author

8
00:00:52,780 --> 00:00:56,390
it is
a small yellow leech-like, and probably the

9
00:00:56,390 --> 00:00:57,989
oddest thing
in the universe.

10
00:00:57,989 --> 00:01:06,380
It feeds on brainwave energy and long
story short, it is a universal translator.

11
00:01:06,380 --> 00:01:08,710
The main
problem is you actually have to stick one

12
00:01:08,710 --> 00:01:12,460
into your ear
to make it work, but once you do it, you'll

13
00:01:12,460 --> 00:01:16,420
understand
anything that is said to you in any language.

14
00:01:16,420 --> 00:01:18,720
If you
haven't read the book, I really recommend

15
00:01:18,720 --> 00:01:20,950
you do.

16
00:01:20,950 --> 00:01:24,060
Okay,
so moving on.

17
00:01:24,060 --> 00:01:28,200
About two years ago I become
interested in WebRTC.

18
00:01:28,200 --> 00:01:33,840
Thomas will give a talk later
today, so I'll describe it really briefly.

19
00:01:33,840 --> 00:01:36,560
It is an API
that allows you to create peer-to-peer connections

20
00:01:36,560 --> 00:01:40,590
between devices, not necessarily just browsers,
and you

21
00:01:40,590 --> 00:01:43,100
can basically send anything you want through
such

22
00:01:43,100 --> 00:01:47,619
a connection: audio, video, or data.

23
00:01:47,619 --> 00:01:50,490
About a year ago
I get a talk about interesting applications

24
00:01:50,490 --> 00:01:54,020
of WebRTC
which was basically showing what stuff you

25
00:01:54,020 --> 00:01:58,819
can build
with it, and I was looking for some applications

26
00:01:58,819 --> 00:02:01,430
to
show, and the thing is that for data, there

27
00:02:01,430 --> 00:02:03,479
are lots of
really interesting applications.

28
00:02:03,479 --> 00:02:08,929
I mean, WebRTC
doesn't -- I mean the protocol is not based

29
00:02:08,929 --> 00:02:13,910
on TCP, but
rather UDP, so we can do very low working,

30
00:02:13,910 --> 00:02:16,240
you can do
file sharing because you have a direct connection

31
00:02:16,240 --> 00:02:20,739
between two devices, you don't have to send
a file like

32
00:02:20,739 --> 00:02:22,570
for example in Dropbox.

33
00:02:22,570 --> 00:02:26,990
Maybe some of you have even
heard my application called shadrock(?) which

34
00:02:26,990 --> 00:02:31,629
is like
airdrop, which ironically doesn't work on

35
00:02:31,629 --> 00:02:37,810
safari, but
you can actually do crazy stuff with the data

36
00:02:37,810 --> 00:02:41,599
translator, like implement protocol in JavaScript,
which

37
00:02:41,599 --> 00:02:47,469
works in browser, and that's exactly what
WebRTC does.

38
00:02:47,469 --> 00:02:51,090
But in the case of audio and video, things
are boring.

39
00:02:51,090 --> 00:02:56,299
We have Skype for actually quite some time,
and, you

40
00:02:56,299 --> 00:02:59,989
know, WebRTC does exactly the same but it
is not really

41
00:02:59,989 --> 00:03:02,750
that interesting, right, so I didn't have
anything to

42
00:03:02,750 --> 00:03:06,049
show and I decided to write my own application
with a

43
00:03:06,049 --> 00:03:09,950
small technical twist, and that's what I'd
like to show

44
00:03:09,950 --> 00:03:10,950
you today.

45
00:03:10,950 --> 00:03:11,950
Okay.

46
00:03:11,950 --> 00:03:12,950
So it is time for the demo.

47
00:03:12,950 --> 00:03:22,439
So just
sit back, relax and enjoy.

48
00:03:22,439 --> 00:04:08,540
demo played)
Okay, so

49
00:04:08,540 --> 00:04:35,200
that's it.

50
00:04:35,200 --> 00:05:01,870
[applause].

51
00:05:01,870 --> 00:05:03,250
Thank you.

52
00:05:03,250 --> 00:05:07,830
So as you can see it is basically a
universal translator as well, just like Babel

53
00:05:07,830 --> 00:05:09,199
Fish.

54
00:05:09,199 --> 00:05:11,430
The
thing is that on the left-hand side of your

55
00:05:11,430 --> 00:05:15,830
video, yeah,
left, you can select the language you're speaking.

56
00:05:15,830 --> 00:05:18,300
On
the right-hand side you can see what the language

57
00:05:18,300 --> 00:05:20,750
the
other person speaks, in and there is one picture

58
00:05:20,750 --> 00:05:23,099
that I
haven't really shown, is that if the speech

59
00:05:23,099 --> 00:05:25,650
recognition
is completely messed up you can type the message

60
00:05:25,650 --> 00:05:28,090
here
and it will also be translated and sent to

61
00:05:28,090 --> 00:05:30,660
the other
person.

62
00:05:30,660 --> 00:05:33,699
So it looks like kind of cool, right, but
it is

63
00:05:33,699 --> 00:05:34,699
actually nothing new.

64
00:05:34,699 --> 00:05:39,910
This is a screenshot from a Skype
translator by Microsoft.

65
00:05:39,910 --> 00:05:44,570
It was released in December
last year and it does exact the same thing.

66
00:05:44,570 --> 00:05:47,781
It does it
even better because in my application you

67
00:05:47,781 --> 00:05:50,440
have to press
the button to turn the speech recognition

68
00:05:50,440 --> 00:05:52,290
on.

69
00:05:52,290 --> 00:05:55,360
I later
describe why it is necessary.

70
00:05:55,360 --> 00:05:58,020
In this application you can just speak all
the time,

71
00:05:58,020 --> 00:06:03,509
and it will all the time recognise what you're
saying.

72
00:06:03,509 --> 00:06:06,320
There are some advantages over Skype.

73
00:06:06,320 --> 00:06:10,099
The most
obvious one is that it works in the browser.

74
00:06:10,099 --> 00:06:13,050
The Skype
application is obviously an application that

75
00:06:13,050 --> 00:06:16,270
works only
on windows.

76
00:06:16,270 --> 00:06:23,440
It uses open standards for WebRTC, Skype is
some proprietary protocols, but to be fair,

77
00:06:23,440 --> 00:06:25,500
Microsoft is
currently working on a version of Skype that

78
00:06:25,500 --> 00:06:28,479
will work
in the browser it will use in the next version

79
00:06:28,479 --> 00:06:32,099
of
WebRTC, I'm not sure about Skype translator,

80
00:06:32,099 --> 00:06:35,220
but at
least a normal version of Skype should work

81
00:06:35,220 --> 00:06:37,270
in the
browser.

82
00:06:37,270 --> 00:06:39,590
But the most important thing I want to say
is

83
00:06:39,590 --> 00:06:44,180
that really, the first working version of
it took one

84
00:06:44,180 --> 00:06:45,919
day to write.

85
00:06:45,919 --> 00:06:52,830
And it really shows how powerful today's
browsers are and how powerful the APIs are.

86
00:06:52,830 --> 00:06:56,169
And it is
really not that hard to write something like

87
00:06:56,169 --> 00:06:57,169
that.

88
00:06:57,169 --> 00:07:01,910
So
hopefully you're all wondering how it works.

89
00:07:01,910 --> 00:07:04,550
So I'll go
over it step by step, warning there will be

90
00:07:04,550 --> 00:07:08,340
some code
ahead, but not much fortunately so the first

91
00:07:08,340 --> 00:07:11,100
thing is
obviously send audio and video, and it is

92
00:07:11,100 --> 00:07:16,419
done via
WebRTC, so it is unfortunately not the simplest

93
00:07:16,419 --> 00:07:19,650
and
easiest API, so I am using a library to make

94
00:07:19,650 --> 00:07:21,250
it easier.

95
00:07:21,250 --> 00:07:25,370
The library at the time is called simple WebRTC,
and

96
00:07:25,370 --> 00:07:28,169
using it looks like this.

97
00:07:28,169 --> 00:07:32,610
So that's like less than ten
lines of code and that's all you actually

98
00:07:32,610 --> 00:07:37,560
need to make
your own Skype, or very basic version of Skype

99
00:07:37,560 --> 00:07:39,450
application.

100
00:07:39,450 --> 00:07:41,099
So I'll go it quickly.

101
00:07:41,099 --> 00:07:46,900
First, you need to
instantiate it, then you just pass IDs of

102
00:07:46,900 --> 00:07:51,120
DOM elements
where you want to display your video, and

103
00:07:51,120 --> 00:07:54,659
video for
other people.

104
00:07:54,659 --> 00:07:59,319
You wait for ready to call event, which
is when your browser gets access to your camera

105
00:07:59,319 --> 00:08:04,250
and
microphone, and then you have to call to join

106
00:08:04,250 --> 00:08:05,990
the room,
which will connect you to all the other peers

107
00:08:05,990 --> 00:08:06,990
in the
same room.

108
00:08:06,990 --> 00:08:09,300
That's actually the most complicated part
if

109
00:08:09,300 --> 00:08:14,240
you'll use WebRTC directly, and that's just
one call in

110
00:08:14,240 --> 00:08:16,569
simple WebRTC library.

111
00:08:16,569 --> 00:08:17,960
Yeah, that's it.

112
00:08:17,960 --> 00:08:20,759
We're actually down to the audio
and video part.

113
00:08:20,759 --> 00:08:26,500
So, moving on to a bit more interesting
part, that is speech recognition.

114
00:08:26,500 --> 00:08:31,439
It is done using web
speech recognition API, and using it looks

115
00:08:31,439 --> 00:08:33,469
very similar
to the code that we've just seen.

116
00:08:33,469 --> 00:08:37,270
Again, you have to
instantiate an object, you wait for a result

117
00:08:37,270 --> 00:08:40,550
event which
will be triggered whenever the speech recognition

118
00:08:40,550 --> 00:08:44,440
actually has some results, and finally you
go to the

119
00:08:44,440 --> 00:08:48,500
start that will ask for your microphone and
start speech

120
00:08:48,500 --> 00:08:50,530
recognition.

121
00:08:50,530 --> 00:08:57,090
Okay, so the next step is that speech
recognition gives us a transcript of what

122
00:08:57,090 --> 00:09:00,200
you just said,
so we need to send it to Google Translate

123
00:09:00,200 --> 00:09:01,200
to actually
translate it.

124
00:09:01,200 --> 00:09:03,780
It is just done using plain code.

125
00:09:03,780 --> 00:09:08,210
So no
closed here.

126
00:09:08,210 --> 00:09:11,890
Once you have the original transcript and
it is translation, you can send it to the

127
00:09:11,890 --> 00:09:13,200
other person.

128
00:09:13,200 --> 00:09:15,251
And the cool thing is that there is WebRTC
data transfer

129
00:09:15,251 --> 00:09:20,080
for that, so you don't need any servers, so
normally

130
00:09:20,080 --> 00:09:23,040
you'd send it using Ajax or web sockets or
something

131
00:09:23,040 --> 00:09:26,220
like that, but in the case of WebRTC you have
direct

132
00:09:26,220 --> 00:09:27,940
connection with the other person.

133
00:09:27,940 --> 00:09:31,530
We don't need any
server for that.

134
00:09:31,530 --> 00:09:36,130
So, using the data channels, this is
really easy.

135
00:09:36,130 --> 00:09:41,080
You have just one to send directly to all,
you specific the channel name, payload name

136
00:09:41,080 --> 00:09:43,410
which is
basically what you're sending, and the data

137
00:09:43,410 --> 00:09:44,410
itself.

138
00:09:44,410 --> 00:09:49,020
Yeah, that's it.

139
00:09:49,020 --> 00:09:54,510
So the last step is, so we already
have the original end translator transcript

140
00:09:54,510 --> 00:09:57,620
on the other
side, and now it is time to read it out loud

141
00:09:57,620 --> 00:10:01,260
and we use
again API for that.

142
00:10:01,260 --> 00:10:05,320
It is called synth API, it looks
like that.

143
00:10:05,320 --> 00:10:12,600
You have this really strange object, I just
have to instantiate, the language, the message

144
00:10:12,600 --> 00:10:18,580
itself,
and then you have this, and then you have

145
00:10:18,580 --> 00:10:22,580
this speak
method where you have to just pass this object

146
00:10:22,580 --> 00:10:28,590
to, and
that's actually, as you can see, hopefully

147
00:10:28,590 --> 00:10:33,370
the
individual steps of it are really simple,

148
00:10:33,370 --> 00:10:34,951
in the real
application there is a bit more code, for

149
00:10:34,951 --> 00:10:37,900
example, for
call handling for handling language switches

150
00:10:37,900 --> 00:10:42,080
and so on,
but yeah, that's actually the core of it.

151
00:10:42,080 --> 00:10:44,850
There is not
really that much code in the application.

152
00:10:44,850 --> 00:10:48,330
If anyone is
interested in the actual code, I'll give the

153
00:10:48,330 --> 00:10:54,011
link to the
code at the end of the talk.

154
00:10:54,011 --> 00:10:58,480
The second part, I'd like
to give you a very brief introduction into

155
00:10:58,480 --> 00:11:03,190
web speech
API, I just actually wanted to mention two

156
00:11:03,190 --> 00:11:04,780
things.

157
00:11:04,780 --> 00:11:07,170
So
the first thing is that web speech API recognition

158
00:11:07,170 --> 00:11:09,170
has
two modes, the first one is one shot, and

159
00:11:09,170 --> 00:11:11,640
the second one
is continuous.

160
00:11:11,640 --> 00:11:15,070
In the one shot mode, in the speech
recognition, the algorhythmic will stop when

161
00:11:15,070 --> 00:11:21,800
you want,
which is great for giving commands and that's

162
00:11:21,800 --> 00:11:24,640
the mode
used in my application.

163
00:11:24,640 --> 00:11:29,420
The other mode, which is more
interesting, is the continuous mode and basically

164
00:11:29,420 --> 00:11:31,430
it
will even if you make a pause, it will just

165
00:11:31,430 --> 00:11:34,240
continue to
work, at least in theory.

166
00:11:34,240 --> 00:11:38,450
So this one is great for
dictating stuff, or for my application, with

167
00:11:38,450 --> 00:11:42,570
me at
least, where users -- two users have a conversation.

168
00:11:42,570 --> 00:11:46,380
Unfortunately it doesn't really work that
well.

169
00:11:46,380 --> 00:11:49,880
And
I'll tell why really soon.

170
00:11:49,880 --> 00:11:54,690
The second thing is that
according to the spec, the API itself is like

171
00:11:54,690 --> 00:11:59,420
agnostic
of the underlying speech recognition implementation,

172
00:11:59,420 --> 00:12:01,690
so
it basically doesn't implement -- it doesn't

173
00:12:01,690 --> 00:12:04,660
specify how
the speech recognition should work.

174
00:12:04,660 --> 00:12:08,490
And you can have
speech recognition on server, or on the client.

175
00:12:08,490 --> 00:12:11,670
So in
Chrome, it uses Google services, so it is

176
00:12:11,670 --> 00:12:17,180
on the server,
but it could be done locally.

177
00:12:17,180 --> 00:12:23,000
So while the speech API
is really awesome, it has some issues.

178
00:12:23,000 --> 00:12:28,420
So if you'd like
to experiment with it, I'll just, you know,

179
00:12:28,420 --> 00:12:31,190
briefly
describe what the issues are, so at least

180
00:12:31,190 --> 00:12:34,540
you know what
to expect.

181
00:12:34,540 --> 00:12:38,670
So the first issue is that the speech
recognition will stop if you talk for too

182
00:12:38,670 --> 00:12:41,700
long, I'm not
sure what the exact limit is, but if you talk

183
00:12:41,700 --> 00:12:46,970
for a
minute or over a minute, it will stop.

184
00:12:46,970 --> 00:12:51,790
You'll get abort
event, and yeah, that's actually -- the second

185
00:12:51,790 --> 00:12:55,260
issue is
actually quite the opposite.

186
00:12:55,260 --> 00:12:58,300
It will stop if you don't
talk for as long.

187
00:12:58,300 --> 00:13:03,000
I'm talking about the continuous mode
which should handle pauses, but if you will

188
00:13:03,000 --> 00:13:05,920
make long
enough pause, it will just stop.

189
00:13:05,920 --> 00:13:08,770
Again, you'll get
abort event, and there is a work around these

190
00:13:08,770 --> 00:13:09,770
issues.

191
00:13:09,770 --> 00:13:12,040
I
mean, you can wait for the abort event and

192
00:13:12,040 --> 00:13:16,410
just call
start again, but unfortunately, in some cases,

193
00:13:16,410 --> 00:13:19,380
after you
call start, it will abort again and it will

194
00:13:19,380 --> 00:13:23,870
end up with
this, which makes the application unstable

195
00:13:23,870 --> 00:13:26,550
and actually
that's the reason why my application doesn't

196
00:13:26,550 --> 00:13:30,590
use the
continuous mode.

197
00:13:30,590 --> 00:13:32,960
So the most important issue, I think, is that
web

198
00:13:32,960 --> 00:13:35,740
speech API doesn't work with WebRTC media
streams, so

199
00:13:35,740 --> 00:13:40,050
the thing is the specification was created
probably

200
00:13:40,050 --> 00:13:43,370
before WebRTC application, so that's why it
doesn't

201
00:13:43,370 --> 00:13:46,970
mention anything about media streams, and
it means that

202
00:13:46,970 --> 00:13:50,520
the web speech API can only recognise input
from your

203
00:13:50,520 --> 00:13:54,480
own mic, nothing else, really, and that causes
a lot of

204
00:13:54,480 --> 00:13:55,480
issues.

205
00:13:55,480 --> 00:13:57,500
So the first thing is that, for example, in
my

206
00:13:57,500 --> 00:14:00,800
application, which is WebRTC, and web speech
recognition

207
00:14:00,800 --> 00:14:03,690
API, it will ask you twice for your mic, which
is really

208
00:14:03,690 --> 00:14:05,080
confusing for users.

209
00:14:05,080 --> 00:14:09,920
So once it will ask for WebRTC,
the second time with the web speech RPI, but

210
00:14:09,920 --> 00:14:12,990
the most
important thing is, like I said, it can only

211
00:14:12,990 --> 00:14:14,790
recognise
what you're saying.

212
00:14:14,790 --> 00:14:21,590
You can't get remote audio stream
from another person and just pass it into

213
00:14:21,590 --> 00:14:24,710
web speech
recognition object, right.

214
00:14:24,710 --> 00:14:29,990
So if you could do it, you
could basically just write an extension or

215
00:14:29,990 --> 00:14:34,170
plug into any
web application and perform the whole recognition

216
00:14:34,170 --> 00:14:37,680
of
whatever any other person has said on your

217
00:14:37,680 --> 00:14:39,580
computer,
which would be really cool.

218
00:14:39,580 --> 00:14:42,540
Another thing is I'm not
sure if you're aware, but using WebRTC you

219
00:14:42,540 --> 00:14:45,110
can actually
make phone calls or make or receive phone

220
00:14:45,110 --> 00:14:46,230
calls.

221
00:14:46,230 --> 00:14:51,720
You
have to use gateway service, but it is possible,

222
00:14:51,720 --> 00:14:54,690
and if
you could pass like remote audio to speech

223
00:14:54,690 --> 00:14:58,050
recognition,
you could actually, for example, make an application

224
00:14:58,050 --> 00:15:01,410
where you just call somebody, take audio from
them and

225
00:15:01,410 --> 00:15:03,830
just pass it for speech recognition locally.

226
00:15:03,830 --> 00:15:07,280
So it will
be pretty good for, for example, people with

227
00:15:07,280 --> 00:15:09,390
hearing
loss, which could basically create a transcript

228
00:15:09,390 --> 00:15:12,030
on the
phone call on the fly.

229
00:15:12,030 --> 00:15:17,740
So yeah, unfortunately it
doesn't work, but like I said, this web speech

230
00:15:17,740 --> 00:15:19,690
spec
doesn't really mention anything about it,

231
00:15:19,690 --> 00:15:24,000
but actually
it would be really easy to extend the specification

232
00:15:24,000 --> 00:15:25,890
without breaking it down.

233
00:15:25,890 --> 00:15:27,690
That's it.

234
00:15:27,690 --> 00:15:32,810
You just have to
pass audio stream to this speech recognition

235
00:15:32,810 --> 00:15:36,070
object and
it could just work.

236
00:15:36,070 --> 00:15:38,450
So that's actually what Firefox is
doing right now.

237
00:15:38,450 --> 00:15:44,180
They're extending, it has been at
least, extending this API a bit without breaking

238
00:15:44,180 --> 00:15:49,700
it
actually, I just was told that it should work

239
00:15:49,700 --> 00:15:54,680
in Firefox
2.5, so hopefully it will sync soon.

240
00:15:54,680 --> 00:15:57,400
And like the most
obvious reason why this API is not really

241
00:15:57,400 --> 00:15:59,930
more popular,
it works only in Chrome.

242
00:15:59,930 --> 00:16:05,870
And the issue is that it is
not even like all chrome, desktop and mobile.

243
00:16:05,870 --> 00:16:08,880
The issue
is that for example there is a small bug in

244
00:16:08,880 --> 00:16:11,360
mobile
version of Chrome which doesn't allow you

245
00:16:11,360 --> 00:16:15,570
to use speech
recognition if you're WebRTC at the same time.

246
00:16:15,570 --> 00:16:20,200
So
unfortunately my app won't work on android.

247
00:16:20,200 --> 00:16:23,150
But what can we do about these issues?

248
00:16:23,150 --> 00:16:26,850
I mean, the
first is we can ask them nicely to update

249
00:16:26,850 --> 00:16:31,570
the spec, ask
Google to fix the implementation, or we could

250
00:16:31,570 --> 00:16:34,220
ask
another browser mender to actually implement

251
00:16:34,220 --> 00:16:35,730
the API.

252
00:16:35,730 --> 00:16:38,310
And while the speech recognition itself is
really

253
00:16:38,310 --> 00:16:41,570
complicated problem, the technology is not
really an

254
00:16:41,570 --> 00:16:44,140
issue here.

255
00:16:44,140 --> 00:16:49,610
Personal assistance or virtual assistance
is really popular lately, and it turns other

256
00:16:49,610 --> 00:16:52,540
that every
browser actually has some implementation of

257
00:16:52,540 --> 00:16:53,600
it.

258
00:16:53,600 --> 00:16:59,190
So
Google obviously has Google Now, Apple has

259
00:16:59,190 --> 00:17:04,200
Siri, Firefox
actually doesn't have anything, but they are

260
00:17:04,200 --> 00:17:08,940
using a
library called Pocket Sphinx, which does speech

261
00:17:08,940 --> 00:17:12,990
recognition locally on the device.

262
00:17:12,990 --> 00:17:16,500
But there is also
another way.

263
00:17:16,500 --> 00:17:23,140
We don't really need to wait for a browser
vendors, so you probably remember this one.

264
00:17:23,140 --> 00:17:24,140
Right?

265
00:17:24,140 --> 00:17:26,660
So
all the time I'm talking about making implementation

266
00:17:26,660 --> 00:17:30,610
of
the API, but it is actually possible to create

267
00:17:30,610 --> 00:17:33,590
a virtual
library that will have exactly the same API

268
00:17:33,590 --> 00:17:39,039
as web
speech API and just would use some third-party

269
00:17:39,039 --> 00:17:43,029
service
to perform the speech recognition, or do it

270
00:17:43,029 --> 00:17:44,029
locally.

271
00:17:44,029 --> 00:17:45,809
So
again, like I said, the speech recognition

272
00:17:45,809 --> 00:17:49,590
is like
really complex and hard problem, but again,

273
00:17:49,590 --> 00:17:51,720
as it turns
out, there are plenty of companies that are

274
00:17:51,720 --> 00:17:54,360
actually
happy to write.

275
00:17:54,360 --> 00:18:00,769
So Facebook has recently announced
Facebook M assistant, yet another -- they

276
00:18:00,769 --> 00:18:03,779
also obviously
have technology for speech recognition.

277
00:18:03,779 --> 00:18:07,629
There is
another assistant called hound by SoundHound.

278
00:18:07,629 --> 00:18:10,410
I'm not
sure if you have heard about it but it looks

279
00:18:10,410 --> 00:18:17,279
promising,
BaiDu has announced one recently, there are

280
00:18:17,279 --> 00:18:22,879
some other
companies that do speech recognition, obviously,

281
00:18:22,879 --> 00:18:27,010
and
there is also we've just seen a demo, there

282
00:18:27,010 --> 00:18:30,200
is IBM
Watson which also has API performing speech

283
00:18:30,200 --> 00:18:31,200
recognition.

284
00:18:31,200 --> 00:18:35,629
Finally, there is an interesting project called
Pocket

285
00:18:35,629 --> 00:18:40,200
Sphinx JS which actually uses the library
that is used

286
00:18:40,200 --> 00:18:45,080
by Firefox and it compiles it using scripts
into

287
00:18:45,080 --> 00:18:48,610
JavaScript and this actually provides the
fully

288
00:18:48,610 --> 00:18:52,659
functional speech recognition library in your
browser

289
00:18:52,659 --> 00:18:55,140
and actually in any browser that supports
WebRTC.

290
00:18:55,140 --> 00:19:01,759
It
doesn't have to support speech recognition.

291
00:19:01,759 --> 00:19:05,720
So that's actually it.

292
00:19:05,720 --> 00:19:11,610
I kind of hope that you saw
that, you know, the whole implementation wasn't

293
00:19:11,610 --> 00:19:12,809
really
that hard.

294
00:19:12,809 --> 00:19:14,429
Each step was really easy.

295
00:19:14,429 --> 00:19:18,299
You can see we
have today's browsers are really powerful

296
00:19:18,299 --> 00:19:20,960
and I strongly
encourage you to maybe instead of writing

297
00:19:20,960 --> 00:19:25,750
yet another
framework just try to experiment with them.

298
00:19:25,750 --> 00:19:29,730
Also I kind
of hope that once Firefox actually releases

299
00:19:29,730 --> 00:19:33,490
a version
that of their browser that has a web speech

300
00:19:33,490 --> 00:19:42,450
implementation API, I mean web speech API
implementation, it will just become more popular.

301
00:19:42,450 --> 00:19:46,559
My
name is size a Szymon and I work with these

302
00:19:46,559 --> 00:19:47,559
companies.

303
00:19:47,559 --> 00:19:48,559
That's all.

