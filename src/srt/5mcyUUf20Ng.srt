1
00:00:24,150 --> 00:00:25,550
Thank you everyone.

2
00:00:25,550 --> 00:00:30,730
I'm glad you are still here and
have some energy left for the last two talks.

3
00:00:30,730 --> 00:00:35,280
I'm
starting this slide empty because I want to

4
00:00:35,280 --> 00:00:36,960
ask you
something first.

5
00:00:36,960 --> 00:00:42,540
The next slide will have a riddle on
it, and I want to ask you if you already know

6
00:00:42,540 --> 00:00:46,620
this one,
please don't shout the answer out loud, just

7
00:00:46,620 --> 00:00:50,510
let
everyone try and figure this out by themselves.

8
00:00:50,510 --> 00:00:53,690
So be
quiet.

9
00:00:53,690 --> 00:00:55,940
It is a bit depressing, but I hope you can
read

10
00:00:55,940 --> 00:00:56,940
it.

11
00:00:56,940 --> 00:00:59,510
A father and his son are in a car accident,
the

12
00:00:59,510 --> 00:01:03,269
father dies at the scene and the son, badly
injured, is

13
00:01:03,269 --> 00:01:04,650
rushed to the hospital.

14
00:01:04,650 --> 00:01:08,480
In the operating room the
surgeon looks at the boy and says, "I can't

15
00:01:08,480 --> 00:01:09,670
operate on
this boy.

16
00:01:09,670 --> 00:01:10,670
He is my son."

17
00:01:10,670 --> 00:01:16,170
Just read it and I'll give you ten seconds.

18
00:01:16,170 --> 00:01:19,250
I'm not
giving you the answer.

19
00:01:19,250 --> 00:01:23,040
Oh, you're super quiet.

20
00:01:23,040 --> 00:01:27,370
That's
nice.

21
00:01:27,370 --> 00:01:32,040
My name is Lieke, I'm from Amsterdam and I
notice

22
00:01:32,040 --> 00:01:35,011
there are a lot of cat pictures at this conference,
so

23
00:01:35,011 --> 00:01:40,650
I added one a black cat cycling through Amsterdam
on an

24
00:01:40,650 --> 00:01:42,380
orange bike with tulips.

25
00:01:42,380 --> 00:01:46,830
What more do you want?

26
00:01:46,830 --> 00:01:53,380
You probably know me, maybe not, maybe not.

27
00:01:53,380 --> 00:01:56,490
I'm
involved in the real skills summer of code,

28
00:01:56,490 --> 00:01:59,560
also a Dutch
ambassador for the code week, if you don't

29
00:01:59,560 --> 00:02:02,110
know, Google
it and participate, because it is awesome.

30
00:02:02,110 --> 00:02:06,740
I have my
own website called codepancake.com, where

31
00:02:06,740 --> 00:02:11,110
you can find
a lot of resources on coding and I'm proud

32
00:02:11,110 --> 00:02:15,730
of the
spotlight series, there are a lot of amazing

33
00:02:15,730 --> 00:02:19,170
interviews.

34
00:02:19,170 --> 00:02:23,560
I work at an organisation on girls and women
in

35
00:02:23,560 --> 00:02:25,590
science and technology.

36
00:02:25,590 --> 00:02:29,500
Our mission is to involve the
increasing of girls and women in science,

37
00:02:29,500 --> 00:02:32,470
mathematics,
engineering and IT.

38
00:02:32,470 --> 00:02:38,439
We do research but also organise
activities throughout the education chain,

39
00:02:38,439 --> 00:02:41,810
so primary,
secondary and higher education.

40
00:02:41,810 --> 00:02:43,799
You can see four role
models.

41
00:02:43,799 --> 00:02:51,790
We have a large database with over 2,000 female
IT professionals and students, and I think

42
00:02:51,790 --> 00:02:54,040
that is
amazing because we can always count on them

43
00:02:54,040 --> 00:02:55,379
if we have
activities.

44
00:02:55,379 --> 00:02:58,659
But the question is, why are we doing this?

45
00:02:58,659 --> 00:03:00,450
Why do we need more women in tech?

46
00:03:00,450 --> 00:03:04,269
Well I think Brenna
already mentioned it a bit, but diversity.

47
00:03:04,269 --> 00:03:08,019
Things in
more general in diversity have proved to be

48
00:03:08,019 --> 00:03:11,079
more
innovative and creative and effective in teams.

49
00:03:11,079 --> 00:03:14,909
If
you're interested in this project, go online,

50
00:03:14,909 --> 00:03:17,760
there are
lots of resources online.

51
00:03:17,760 --> 00:03:24,499
And another thing, the lack
of women entering the tech field is also a

52
00:03:24,499 --> 00:03:27,639
loss of
talent with the IT industry, and a loss of

53
00:03:27,639 --> 00:03:30,389
opportunity
for the women and entering the job market.

54
00:03:30,389 --> 00:03:36,510
So a lot of
talent is wasted, actually.

55
00:03:36,510 --> 00:03:40,971
Girls and females are underrepresented in
tech, if

56
00:03:40,971 --> 00:03:43,709
you compare them to the male counterparts,
and in the

57
00:03:43,709 --> 00:03:46,090
Netherlands it is somewhat exceptional because
we are

58
00:03:46,090 --> 00:03:50,170
last, actually Tunisia is below us, and then
the

59
00:03:50,170 --> 00:03:51,170
Netherlands.

60
00:03:51,170 --> 00:03:56,230
Less than 10 per cent of the females are
working in IT and in the Netherlands, and

61
00:03:56,230 --> 00:04:00,959
you can see
Germany is below average for the 35, and we

62
00:04:00,959 --> 00:04:04,150
have
23 per cent, and the rest of the European

63
00:04:04,150 --> 00:04:07,400
Union is 37.

64
00:04:07,400 --> 00:04:08,400
So why is this?

65
00:04:08,400 --> 00:04:12,139
Well, the choice for science and
technology by Dutch girls is far below the

66
00:04:12,139 --> 00:04:13,790
average, like
I said.

67
00:04:13,790 --> 00:04:18,109
Well, it is true that brains of boys and girls
are not the same.

68
00:04:18,109 --> 00:04:24,610
But these differences is not the
reason why there are many more boys choosing

69
00:04:24,610 --> 00:04:27,900
a career in
tech, because both of the brains are full

70
00:04:27,900 --> 00:04:30,690
with
potential, but to what extent, the potential

71
00:04:30,690 --> 00:04:35,270
is for
fields and which is highly dependent on stimulation

72
00:04:35,270 --> 00:04:37,690
that
parents and teachers offer.

73
00:04:37,690 --> 00:04:42,669
Next is also that girls think they perform
worse

74
00:04:42,669 --> 00:04:45,800
than they actually do in STEM related subjects.

75
00:04:45,800 --> 00:04:48,479
I'm not
sure what your grading system is, but in the

76
00:04:48,479 --> 00:04:52,770
Netherlands
it is from one to ten, but if you have a 5.5

77
00:04:52,770 --> 00:04:55,360
then you've
passed your exams.

78
00:04:55,360 --> 00:05:00,050
We always ask students if they think
they're good in mathematics, and boys normally

79
00:05:00,050 --> 00:05:02,590
raise
their hands if they have a six, because it's

80
00:05:02,590 --> 00:05:05,090
good they
passed our test, right?

81
00:05:05,090 --> 00:05:08,680
And girls are more like: no, I
should have eight or nine.

82
00:05:08,680 --> 00:05:12,590
So they're too hard on
themselves and it is not necessary at all.

83
00:05:12,590 --> 00:05:15,770
Secondly, environment, like I said, girls
are less

84
00:05:15,770 --> 00:05:19,330
stimulated by teachers and parents to choose
a career in

85
00:05:19,330 --> 00:05:22,540
IT, and this is a cartoon, I don't think you
can read it

86
00:05:22,540 --> 00:05:26,759
but it is a teacher, and he says: "Well, your
child is

87
00:05:26,759 --> 00:05:30,360
has very good grades in maths and science,
but she's

88
00:05:30,360 --> 00:05:31,360
a girl."

89
00:05:31,360 --> 00:05:35,199
And then the parents reply "Oh shit, that's
too bad."

90
00:05:35,199 --> 00:05:39,130
So that's not an option to consider a career
in IT.

91
00:05:39,130 --> 00:05:41,980
Last but not least, there's unfamiliarity.

92
00:05:41,980 --> 00:05:45,740
Girls have
no good image of STEM medications and professions

93
00:05:45,740 --> 00:05:49,470
and
there's a huge lack of role models.

94
00:05:49,470 --> 00:05:53,699
So I just want to
conclude this with a quote, it is from unlocking

95
00:05:53,699 --> 00:05:57,110
in the
clubhouse, very early in life computing is

96
00:05:57,110 --> 00:06:00,490
claimed as
male territory at each sustainable early childhood

97
00:06:00,490 --> 00:06:03,770
through college, computing is both claimed
as guy stuff

98
00:06:03,770 --> 00:06:08,210
by boys and men and passively ceded by girls
and women.

99
00:06:08,210 --> 00:06:10,340
Question observation shows that disinterest
and

100
00:06:10,340 --> 00:06:13,259
dissatisfaction are neither genetic nor accidental,
nor

101
00:06:13,259 --> 00:06:17,750
inherent to the field, but are the bitter
fruit of

102
00:06:17,750 --> 00:06:19,990
external influences.

103
00:06:19,990 --> 00:06:22,490
This is important because if you think there
are

104
00:06:22,490 --> 00:06:25,720
less girls in IT because of disinterest, then
you're

105
00:06:25,720 --> 00:06:28,439
wrong.

106
00:06:28,439 --> 00:06:32,139
One of the external interests is unconscious
bias.

107
00:06:32,139 --> 00:06:36,389
Evolution has taught us to mathematically
process

108
00:06:36,389 --> 00:06:39,220
information without critical thinking, so
we're

109
00:06:39,220 --> 00:06:41,990
constantly overlooking much of the world around
us and

110
00:06:41,990 --> 00:06:44,220
there's actually nothing serious about this.

111
00:06:44,220 --> 00:06:47,969
We receive
11 million bits of information every day,

112
00:06:47,969 --> 00:06:52,139
and we can
only consciously process 40 bits.

113
00:06:52,139 --> 00:06:55,790
The conclusion is
stuff gets broken while our brain is storing

114
00:06:55,790 --> 00:06:59,180
information, and our objectives are certainly
clouded

115
00:06:59,180 --> 00:07:02,129
because our brain is taking shortcuts and
relies on our

116
00:07:02,129 --> 00:07:04,349
earlier assumptions.

117
00:07:04,349 --> 00:07:09,300
So even a tiny bit of bias can
have big consequences, for example, these

118
00:07:09,300 --> 00:07:14,240
biases can
prevent introduction of diversity into workplaces.

119
00:07:14,240 --> 00:07:16,240
I think this is a great example.

120
00:07:16,240 --> 00:07:18,199
Blind auditions.

121
00:07:18,199 --> 00:07:21,050
I just add another picture of a cat behind
the curtain,

122
00:07:21,050 --> 00:07:24,550
because that's the way it is supposed to be.

123
00:07:24,550 --> 00:07:25,550
Laughter].

124
00:07:25,550 --> 00:07:29,700
Well, it is a research from Princeton, and
they

125
00:07:29,700 --> 00:07:32,759
found other that since the 1970s the number
of women in

126
00:07:32,759 --> 00:07:37,419
orchestras went up from five to 25 per cent,
and due to

127
00:07:37,419 --> 00:07:40,509
the fact that they asked them to audition
behind the

128
00:07:40,509 --> 00:07:43,789
screen or curtain so they couldn't see if
it was a male

129
00:07:43,789 --> 00:07:45,060
or female.

130
00:07:45,060 --> 00:07:50,819
So I think that is -- well, that's a lot of
bias going on right there.

131
00:07:50,819 --> 00:07:53,770
Well, most people would agree that gender
bias

132
00:07:53,770 --> 00:07:56,349
exists in others.

133
00:07:56,349 --> 00:07:58,800
All of us, myself included, are
biased.

134
00:07:58,800 --> 00:08:02,499
So you should take a look around, I think
you

135
00:08:02,499 --> 00:08:04,330
know the cartoon, maybe.

136
00:08:04,330 --> 00:08:09,409
A guy says, "Well you suck at
math" and then a girl has to answer wrong,

137
00:08:09,409 --> 00:08:13,199
and it's like
Girls suck at math."

138
00:08:13,199 --> 00:08:15,420
If you're curious about your own biases, you
can

139
00:08:15,420 --> 00:08:18,659
take the implicit associations test, it is
called

140
00:08:18,659 --> 00:08:22,350
project implicit from Harvard University,
and it

141
00:08:22,350 --> 00:08:25,620
measures actually how quickly words like math
and

142
00:08:25,620 --> 00:08:29,060
physics are associated with boys or men, you
can also

143
00:08:29,060 --> 00:08:33,940
take this test with a career and family, but
this is

144
00:08:33,940 --> 00:08:37,630
a gender science test, and I took it, and
I have to be

145
00:08:37,630 --> 00:08:41,130
honest, I worked in IT, I've been a developer,
and these

146
00:08:41,130 --> 00:08:43,290
were my results.

147
00:08:43,290 --> 00:08:47,720
So it says "Your data suggests
a strong automatic association of male with

148
00:08:47,720 --> 00:08:50,810
science, and
female with liberal arts."

149
00:08:50,810 --> 00:08:58,260
So yeah, I just added that, because it's also
something you should do, and I felt terrible

150
00:08:58,260 --> 00:09:01,600
because
I work in IT and I should have known better,

151
00:09:01,600 --> 00:09:02,600
right?

152
00:09:02,600 --> 00:09:05,820
So
this is something I should take home.

153
00:09:05,820 --> 00:09:07,910
Now it gets me
back to the riddle.

154
00:09:07,910 --> 00:09:11,500
I think you know the answer by now,
probably, I hope.

155
00:09:11,500 --> 00:09:14,030
So who is the surgeon?

156
00:09:14,030 --> 00:09:15,030
You can
shout.

157
00:09:15,030 --> 00:09:16,790
Yeah, the mother.

158
00:09:16,790 --> 00:09:17,790
It is true.

159
00:09:17,790 --> 00:09:21,390
Actually, two
days ago I learned that in German "surgeon"

160
00:09:21,390 --> 00:09:25,750
is a female
word, so it could be that most of you already

161
00:09:25,750 --> 00:09:27,230
know the
answer right away.

162
00:09:27,230 --> 00:09:30,930
I don't know, but in English and
Dutch it is different.

163
00:09:30,930 --> 00:09:36,250
In 80 per cent of the time, it
is when we ask or tell people about this riddle,

164
00:09:36,250 --> 00:09:37,250
they
don't know the answer.

165
00:09:37,250 --> 00:09:42,890
Or they are more likely to think
that it is a gay couple, or that the father

166
00:09:42,890 --> 00:09:46,190
is a step
father, or that they magically live again,

167
00:09:46,190 --> 00:09:48,630
but nobody
thinks it is the mother.

168
00:09:48,630 --> 00:09:50,020
This is true.

169
00:09:50,020 --> 00:09:53,140
I have to
admit, I had it wrong the first time.

170
00:09:53,140 --> 00:09:55,520
So don't feel bad about it.

171
00:09:55,520 --> 00:10:01,140
Don't feel bad about
being biased, because it is just the -- because

172
00:10:01,140 --> 00:10:04,720
of our
culture that we're biased.

173
00:10:04,720 --> 00:10:09,590
So, notice not about football.

174
00:10:09,590 --> 00:10:16,570
But yeah, when it
comes to implicit and explicit associations,

175
00:10:16,570 --> 00:10:20,210
the
Netherlands are highest in gender stereotyping

176
00:10:20,210 --> 00:10:23,260
because
a lot of -- I think a couple million people

177
00:10:23,260 --> 00:10:26,020
took the
test, and you could fill in your country and

178
00:10:26,020 --> 00:10:28,320
these were
the results, and I think that is shocking

179
00:10:28,320 --> 00:10:31,270
because
I always think of the Netherlands as modern

180
00:10:31,270 --> 00:10:37,290
and open
minded, and well, it turns out it is quite

181
00:10:37,290 --> 00:10:39,960
-- we are
quite old-fashioned when it comes to gender

182
00:10:39,960 --> 00:10:41,910
stereotyping.

183
00:10:41,910 --> 00:10:48,340
So I was shocked, and I decide to take
a look around, and I notice some things.

184
00:10:48,340 --> 00:10:51,530
I think you
all know the girl and boy department at toy

185
00:10:51,530 --> 00:10:52,630
stores,
yeah.

186
00:10:52,630 --> 00:11:00,470
Well on the left it is pink, it is advertisement
for girls, so you learn a bit of Dutch now,

187
00:11:00,470 --> 00:11:05,780
and on the
right it is boys, it's blue.

188
00:11:05,780 --> 00:11:10,080
This is actually for the
girls, it is a dish washing set.

189
00:11:10,080 --> 00:11:13,730
I don't know why, it
is a stupid gift anyway because nobody likes

190
00:11:13,730 --> 00:11:18,710
doing the
dishes, right?

191
00:11:18,710 --> 00:11:21,830
But yeah.

192
00:11:21,830 --> 00:11:23,390
[applause].

193
00:11:23,390 --> 00:11:24,390
It is pink.

194
00:11:24,390 --> 00:11:27,890
It has a pink sink, it has a real
working tap, it is like amazing.

195
00:11:27,890 --> 00:11:33,660
And on the right is
for boys, it is a microscope, it has 23 parts,

196
00:11:33,660 --> 00:11:36,339
it has
glasses, everything.

197
00:11:36,339 --> 00:11:40,390
So these are associations, and you
come across them when you're already 3 years

198
00:11:40,390 --> 00:11:42,680
old or
something.

199
00:11:42,680 --> 00:11:44,730
This is another one, that's also induction.

200
00:11:44,730 --> 00:11:46,390
It's from a children magazine.

201
00:11:46,390 --> 00:11:51,440
It is a test, and it
defines who you are, and on the left, it is

202
00:11:51,440 --> 00:11:56,190
for boys of
course, because it is blue, it says, "What

203
00:11:56,190 --> 00:11:57,190
are you?"

204
00:11:57,190 --> 00:12:00,010
Well, the results are: you are a tough guy,
you are

205
00:12:00,010 --> 00:12:03,680
a smart guy, or you are a funny guy, and on
the right it

206
00:12:03,680 --> 00:12:06,720
is pink, and it is for girls and it says,
"You are

207
00:12:06,720 --> 00:12:10,070
a sweet girl", and the description is horrible.

208
00:12:10,070 --> 00:12:12,690
It
says, "You think about others before you think

209
00:12:12,690 --> 00:12:13,790
about
yourself.

210
00:12:13,790 --> 00:12:15,910
That's amazing."

211
00:12:15,910 --> 00:12:17,430
Okay.

212
00:12:17,430 --> 00:12:22,950
Second -- well, you're a sporty girl, and
third you are [Dutch], which is non-translatable

213
00:12:22,950 --> 00:12:27,600
which
means cosy, fun to be around.

214
00:12:27,600 --> 00:12:35,890
I think it is a bit like
a German word, if I pronounce it right.

215
00:12:35,890 --> 00:12:37,340
Gizelle(?).

216
00:12:37,340 --> 00:12:38,800
Okay.

217
00:12:38,800 --> 00:12:40,250
Well.

218
00:12:40,250 --> 00:12:46,320
Anyway, I think this gender bias could be
a major threat, and it is a major threat,

219
00:12:46,320 --> 00:12:49,330
because it is
one of the key reasons why few girls study

220
00:12:49,330 --> 00:12:50,980
computer
science.

221
00:12:50,980 --> 00:12:53,890
The good news is bias can be weakened.

222
00:12:53,890 --> 00:12:55,980
The
bad news is you cannot do it alone.

223
00:12:55,980 --> 00:12:58,300
We should all do it
together.

224
00:12:58,300 --> 00:13:01,050
So, what should we do first?

225
00:13:01,050 --> 00:13:05,440
Well, be aware that
you're bias, because believe me, you are.

226
00:13:05,440 --> 00:13:09,620
Just take the
test when you have time, and you learn about

227
00:13:09,620 --> 00:13:12,350
your biases
and how you can address them.

228
00:13:12,350 --> 00:13:16,050
Second, I think it is important, use inclusive
language.

229
00:13:16,050 --> 00:13:20,570
Because a couple weeks ago I was at
a conference, a Dutch conference, and it was

230
00:13:20,570 --> 00:13:25,160
about the
future programmer, and someone got to the

231
00:13:25,160 --> 00:13:27,940
stage and
started talking about the future programmer

232
00:13:27,940 --> 00:13:32,930
and he said:
well, he is this and he is that, and he should

233
00:13:32,930 --> 00:13:34,910
be this
and he should be like that.

234
00:13:34,910 --> 00:13:36,720
And I felt offended, come
on.

235
00:13:36,720 --> 00:13:40,510
I am obviously not a future programmer then.

236
00:13:40,510 --> 00:13:44,011
So
I don't think he meant it that way, but it

237
00:13:44,011 --> 00:13:47,810
is good to be
aware of what you're saying.

238
00:13:47,810 --> 00:13:51,970
I think this is from
a social media platform and they removed the

239
00:13:51,970 --> 00:13:55,310
word
hacker" from their job titles, and they noticed

240
00:13:55,310 --> 00:13:57,350
the
change in who was applying.

241
00:13:57,350 --> 00:14:01,070
So they decided to write an
article about it and how gender inclusive

242
00:14:01,070 --> 00:14:06,220
they were,
they attached an e-mail they sent to the employees,

243
00:14:06,220 --> 00:14:09,200
and
the e-mail started with "Hi guys".

244
00:14:09,200 --> 00:14:12,060
It is not very
gender inclusive.

245
00:14:12,060 --> 00:14:17,810
So just for some people this is
offending, so just be aware of what you're

246
00:14:17,810 --> 00:14:19,180
saying.

247
00:14:19,180 --> 00:14:22,350
Well, this is important.

248
00:14:22,350 --> 00:14:25,920
If you're male or female,
doesn't matter, hold yourself and others accountable

249
00:14:25,920 --> 00:14:29,430
and
point it out, because by the simple act of

250
00:14:29,430 --> 00:14:32,510
talking
openly about behavioural patterns it makes

251
00:14:32,510 --> 00:14:36,040
the
subconscious conscious, so talking transforms

252
00:14:36,040 --> 00:14:38,490
minds
which transforms behaviour and communities,

253
00:14:38,490 --> 00:14:43,370
and which
can result in a better environment, for example,

254
00:14:43,370 --> 00:14:44,370
women
in tech.

255
00:14:44,370 --> 00:14:49,530
So we should all share this and talk about
it.

256
00:14:49,530 --> 00:14:54,820
Last but not least, use your imagination.

257
00:14:54,820 --> 00:14:56,730
Counter-program your brain.

258
00:14:56,730 --> 00:15:00,680
I Googled for "programmer"
and these were the results I got.

259
00:15:00,680 --> 00:15:05,040
I really like the
right one, below it says, "Do not wake up

260
00:15:05,040 --> 00:15:07,440
a programmer,
he is working, not sleeping."

261
00:15:07,440 --> 00:15:10,650
It is important,
important lesson.

262
00:15:10,650 --> 00:15:15,320
But you can only see one female and
there is a black guy and the rest of them

263
00:15:15,320 --> 00:15:16,880
are all white
males.

264
00:15:16,880 --> 00:15:20,770
And I think you should be aware of the images
you have in your head.

265
00:15:20,770 --> 00:15:25,450
If I ask you to picture an
architect or programmer, I think most of the

266
00:15:25,450 --> 00:15:28,490
time it
will be a white male with glasses or something

267
00:15:28,490 --> 00:15:29,490
like
that.

268
00:15:29,490 --> 00:15:33,050
Just be aware this is not always true ... I
think

269
00:15:33,050 --> 00:15:35,650
you probably all know this one.

270
00:15:35,650 --> 00:15:38,130
Hashtag I look like an
engineer.

271
00:15:38,130 --> 00:15:42,710
I think this a great way of bias busting.

272
00:15:42,710 --> 00:15:48,500
It
started with this girl, she works at 1 lodge

273
00:15:48,500 --> 00:15:53,110
Inn, and I
don't know if I'm supposed to say that, but

274
00:15:53,110 --> 00:15:57,701
she was an
advertising campaign for a company, and they

275
00:15:57,701 --> 00:16:00,500
were
looking for developers, and then she got a

276
00:16:00,500 --> 00:16:03,350
lot of
responses on her picture like "oh, I didn't

277
00:16:03,350 --> 00:16:04,760
know that
you were an engineer.

278
00:16:04,760 --> 00:16:07,710
You don't look like an engineer"
and so on.

279
00:16:07,710 --> 00:16:13,740
And she was a bit fed up and started the
hashtag actually, and she asked everyone to

280
00:16:13,740 --> 00:16:14,850
spread the
word.

281
00:16:14,850 --> 00:16:18,620
And I think these are all engineers, you should
remember.

282
00:16:18,620 --> 00:16:25,240
This is some great bias busting, like I said,
and it is a great way of breaking down gender

283
00:16:25,240 --> 00:16:27,230
stereotypes.

284
00:16:27,230 --> 00:16:33,480
So in short, be aware that you're bias.

285
00:16:33,480 --> 00:16:35,240
Use inclusive language.

286
00:16:35,240 --> 00:16:38,940
Hold yourself and others
accountable and use your imagination.

287
00:16:38,940 --> 00:16:41,350
So counter
programme your brain.

288
00:16:41,350 --> 00:16:46,770
I know we are at JSconf and it
should be about coding and programming, but

289
00:16:46,770 --> 00:16:49,670
the thing is
it is not about the programming, it is about

290
00:16:49,670 --> 00:16:52,750
the way
you're programmed.

291
00:16:52,750 --> 00:16:56,760
I know it has been a challenging day
today, but I'd like to ask you to take this

292
00:16:56,760 --> 00:17:01,201
home and to
look past your initial perceptions, because

293
00:17:01,201 --> 00:17:03,570
I bet you
they're probably wrong.

294
00:17:03,570 --> 00:17:04,570
So thank you.

