1
00:00:00,000 --> 00:00:01,530

hey I found out I was going to give this

2
00:00:01,530 --> 00:00:09,660
this morning so luckily I am I had some

3
00:00:09,660 --> 00:00:12,049
slides that I was working on anyway so

4
00:00:12,049 --> 00:00:14,059
so I work on an open source project

5
00:00:14,059 --> 00:00:16,260
called debt and I just wanted to kind of

6
00:00:16,260 --> 00:00:18,869
share our use case i'm really excited we

7
00:00:18,869 --> 00:00:20,369
get to use node for something other than

8
00:00:20,369 --> 00:00:23,970
building websites and i think it's a

9
00:00:23,970 --> 00:00:25,680
it's a really fun project that i am

10
00:00:25,680 --> 00:00:27,570
excited have a budget to work on also

11
00:00:27,570 --> 00:00:29,340
and it's open source so I kind of just

12
00:00:29,340 --> 00:00:31,710
want to share with you and tell you what

13
00:00:31,710 --> 00:00:33,930
we're doing and this is kind of state of

14
00:00:33,930 --> 00:00:38,010
the project as of this part of 2014 well

15
00:00:38,010 --> 00:00:42,120
I'm Max and I organized a project called

16
00:00:42,120 --> 00:00:44,850
note school and I just want to do a

17
00:00:44,850 --> 00:00:48,239
shout out about node school we have we

18
00:00:48,239 --> 00:00:49,620
came up with the idea of chapters a few

19
00:00:49,620 --> 00:00:51,870
months ago we have 60 chapters around

20
00:00:51,870 --> 00:00:54,420
the world now in like three months and I

21
00:00:54,420 --> 00:00:55,829
think that makes us the largest node

22
00:00:55,829 --> 00:00:59,309
community so if you haven't got involved

23
00:00:59,309 --> 00:01:00,359
in those school i'm going to guilt-trip

24
00:01:00,359 --> 00:01:01,890
you right here I'm looking at every one

25
00:01:01,890 --> 00:01:04,049
of you in the eyes either join your

26
00:01:04,049 --> 00:01:06,780
notes cool find the chapter or if you

27
00:01:06,780 --> 00:01:08,430
know another language translate the web

28
00:01:08,430 --> 00:01:11,220
page or translate a workshop and help it

29
00:01:11,220 --> 00:01:13,650
spread even more globally shut starting

30
00:01:13,650 --> 00:01:15,390
a chapter you just go to the website and

31
00:01:15,390 --> 00:01:17,340
follow the guide and it's really easy

32
00:01:17,340 --> 00:01:19,470
and if there's already a chapter you can

33
00:01:19,470 --> 00:01:23,250
just join it I also really love cats so

34
00:01:23,250 --> 00:01:26,340
dat dat and cat they have two out of

35
00:01:26,340 --> 00:01:29,189
three of the same letters by design that

36
00:01:29,189 --> 00:01:30,840
is for sharing data and also

37
00:01:30,840 --> 00:01:32,490
collaborating on data there's two phases

38
00:01:32,490 --> 00:01:36,420
to it it's about a year old and we are

39
00:01:36,420 --> 00:01:37,799
grant funded we're not a start-up a lot

40
00:01:37,799 --> 00:01:38,759
of people think we're started because we

41
00:01:38,759 --> 00:01:40,560
have a logo we're just a grant funded

42
00:01:40,560 --> 00:01:44,460
projects and with it basically in the US

43
00:01:44,460 --> 00:01:47,759
there's all these dead rich people that

44
00:01:47,759 --> 00:01:48,990
when they die they give all their money

45
00:01:48,990 --> 00:01:51,780
to a trust or a foundation and then they

46
00:01:51,780 --> 00:01:53,040
spend the money in the money lasts a

47
00:01:53,040 --> 00:01:54,990
long time because people be really rich

48
00:01:54,990 --> 00:01:58,290
here so we've been funded by like

49
00:01:58,290 --> 00:02:04,100
newspaper empires by automotive empires

50
00:02:04,100 --> 00:02:06,240
exploitative industries in the past that

51
00:02:06,240 --> 00:02:09,970
are now trying to offset their anyway so

52
00:02:09,970 --> 00:02:11,590
that's how we make money if anybody was

53
00:02:11,590 --> 00:02:14,590
wondering 100% precedent open source and

54
00:02:14,590 --> 00:02:17,170
we have a distributed team were working

55
00:02:17,170 --> 00:02:21,190
on some really cool stuff and we wrote

56
00:02:21,190 --> 00:02:22,180
we've been writing a lot of

57
00:02:22,180 --> 00:02:23,290
documentation about kind of how we're

58
00:02:23,290 --> 00:02:25,360
doing the project from things like I

59
00:02:25,360 --> 00:02:26,620
have a repo that's just like how our

60
00:02:26,620 --> 00:02:29,020
team is trying to work as a remote async

61
00:02:29,020 --> 00:02:31,840
team we also have been writing a ton of

62
00:02:31,840 --> 00:02:33,700
modules like I think just between me and

63
00:02:33,700 --> 00:02:37,660
Mattias we have like 500 modules and so

64
00:02:37,660 --> 00:02:39,460
we took the time to kind of like write

65
00:02:39,460 --> 00:02:41,590
down how we built the thing so you can

66
00:02:41,590 --> 00:02:45,640
go to our docs on tat and read those the

67
00:02:45,640 --> 00:02:47,440
main goal of the project the current

68
00:02:47,440 --> 00:02:50,830
funder came to us and said we see that

69
00:02:50,830 --> 00:02:52,120
you're doing stuff with data I had come

70
00:02:52,120 --> 00:02:53,410
from a government open government

71
00:02:53,410 --> 00:02:55,750
background I said we're seniors you're

72
00:02:55,750 --> 00:02:57,010
doing stuff with data have you

73
00:02:57,010 --> 00:02:58,090
considered working with scientists

74
00:02:58,090 --> 00:02:59,680
because there's this movement called

75
00:02:59,680 --> 00:03:03,610
reproducible science and we want your

76
00:03:03,610 --> 00:03:04,750
work on that if we don't pay you to work

77
00:03:04,750 --> 00:03:05,950
on that you're going to go work on

78
00:03:05,950 --> 00:03:07,780
something else so we want you to work on

79
00:03:07,780 --> 00:03:09,040
this and I was like that sounds awesome

80
00:03:09,040 --> 00:03:13,720
i love science i'm not a scientist I but

81
00:03:13,720 --> 00:03:15,700
they're like that's cool scientists need

82
00:03:15,700 --> 00:03:16,930
better data tools and better

83
00:03:16,930 --> 00:03:19,209
reproducibility tools it reminds me a

84
00:03:19,209 --> 00:03:21,280
lot basically have like Travis CI but

85
00:03:21,280 --> 00:03:22,570
for science or something like that maybe

86
00:03:22,570 --> 00:03:25,120
that's a bad metaphor but the idea is if

87
00:03:25,120 --> 00:03:26,200
you're a scientist you publish a paper

88
00:03:26,200 --> 00:03:28,239
how do you build a paper you wrote some

89
00:03:28,239 --> 00:03:30,190
code that generated some la tech maybe

90
00:03:30,190 --> 00:03:33,160
you generated some graphs based on some

91
00:03:33,160 --> 00:03:35,590
are code or some Python code and you had

92
00:03:35,590 --> 00:03:37,870
to feed some data into those pieces of

93
00:03:37,870 --> 00:03:39,519
code and you had to scrape the data from

94
00:03:39,519 --> 00:03:41,620
ftp servers there's this like this

95
00:03:41,620 --> 00:03:43,840
incredible stack of dependencies and if

96
00:03:43,840 --> 00:03:45,580
any other scientists get citron ian as a

97
00:03:45,580 --> 00:03:48,160
miracle that's the state of science

98
00:03:48,160 --> 00:03:50,500
today and there's no incentive to do it

99
00:03:50,500 --> 00:03:53,350
you get tenure by writing papers you

100
00:03:53,350 --> 00:03:55,180
don't get tenure by writing test cases

101
00:03:55,180 --> 00:03:57,280
for your unit tests for your science

102
00:03:57,280 --> 00:03:59,950
code so there's a lot of people with a

103
00:03:59,950 --> 00:04:01,690
lot of money trying to fix this problem

104
00:04:01,690 --> 00:04:04,720
and make science something where two

105
00:04:04,720 --> 00:04:06,370
scientists can actually reproduce each

106
00:04:06,370 --> 00:04:09,100
other's results because a lot of people

107
00:04:09,100 --> 00:04:12,100
think if it's a it's not science until

108
00:04:12,100 --> 00:04:13,600
two people can independently verify

109
00:04:13,600 --> 00:04:15,010
otherwise it's just you're trusting

110
00:04:15,010 --> 00:04:16,870
someone's things that they wrote done in

111
00:04:16,870 --> 00:04:21,510
a paper so what our project is about is

112
00:04:21,510 --> 00:04:23,500
kind of like get but

113
00:04:23,500 --> 00:04:28,030
data so source control before get you

114
00:04:28,030 --> 00:04:30,820
had did anybody do code before get or

115
00:04:30,820 --> 00:04:33,310
like maybe even be for subversion it's

116
00:04:33,310 --> 00:04:37,420
uh and uh we used to do that remember

117
00:04:37,420 --> 00:04:39,340
they're like you know 4050 years of

118
00:04:39,340 --> 00:04:41,920
computing resource control people did it

119
00:04:41,920 --> 00:04:44,530
and you know if you had a cool project

120
00:04:44,530 --> 00:04:46,900
you want to fix a bug how did you find

121
00:04:46,900 --> 00:04:49,090
it you got a zip file or something you

122
00:04:49,090 --> 00:04:50,920
unpacked it and you manually edited it

123
00:04:50,920 --> 00:04:53,980
and notepad.exe or whoever and you would

124
00:04:53,980 --> 00:04:55,600
email the file back to the maintainer

125
00:04:55,600 --> 00:04:56,950
and maybe you never heard back it's like

126
00:04:56,950 --> 00:04:59,440
a black hole and like the fort number

127
00:04:59,440 --> 00:05:01,450
four is actually the most important one

128
00:05:01,450 --> 00:05:02,919
like nobody knows what happens after the

129
00:05:02,919 --> 00:05:05,590
email back usually the maintainer would

130
00:05:05,590 --> 00:05:08,020
like maybe copy paste your code in and

131
00:05:08,020 --> 00:05:09,700
make a new zip and there's hope the new

132
00:05:09,700 --> 00:05:12,940
zip finds the users as totally just like

133
00:05:12,940 --> 00:05:14,950
the most insane there's just like no

134
00:05:14,950 --> 00:05:17,020
rhyme or reason and then get came along

135
00:05:17,020 --> 00:05:19,650
and get added all these cool verbs that

136
00:05:19,650 --> 00:05:22,000
tracks changes and you could actually

137
00:05:22,000 --> 00:05:24,970
diff and stuff like that but you still

138
00:05:24,970 --> 00:05:26,320
had to email that was kind of annoying

139
00:05:26,320 --> 00:05:28,150
emailing patch files but hey it was

140
00:05:28,150 --> 00:05:29,979
better than nothing and then github came

141
00:05:29,979 --> 00:05:31,870
along because get like a few years later

142
00:05:31,870 --> 00:05:34,090
and you didn't have to email anymore and

143
00:05:34,090 --> 00:05:35,950
this is actually from a collaboration

144
00:05:35,950 --> 00:05:38,110
standpoint make email inboxes aren't

145
00:05:38,110 --> 00:05:40,419
public so it's really important that if

146
00:05:40,419 --> 00:05:42,340
you're about to send a poor quest you

147
00:05:42,340 --> 00:05:43,720
can be like wait I should check if

148
00:05:43,720 --> 00:05:44,590
there's somebody else that's already

149
00:05:44,590 --> 00:05:46,270
sent this for requests that's what

150
00:05:46,270 --> 00:05:51,270
github does among other things emoji so

151
00:05:51,270 --> 00:05:53,560
with get you can just you don't have to

152
00:05:53,560 --> 00:05:55,870
do all those manual steps you can just

153
00:05:55,870 --> 00:05:59,440
get pull so our claim our kind of

154
00:05:59,440 --> 00:06:00,760
operating premise of the project is that

155
00:06:00,760 --> 00:06:02,800
data sharing I mean anyway this worked

156
00:06:02,800 --> 00:06:04,510
with data knows it's the same thing as

157
00:06:04,510 --> 00:06:07,690
the zip file source control thing it's

158
00:06:07,690 --> 00:06:09,550
like you find an ftp server you download

159
00:06:09,550 --> 00:06:11,380
a CSV if to parse the CSV into your

160
00:06:11,380 --> 00:06:13,120
database you end up writing like a stack

161
00:06:13,120 --> 00:06:14,530
of code that only does one thing and

162
00:06:14,530 --> 00:06:17,919
it's super monolithic and brittle and if

163
00:06:17,919 --> 00:06:19,360
six months later you want to switch to a

164
00:06:19,360 --> 00:06:20,680
different database you have to rewrite

165
00:06:20,680 --> 00:06:22,750
all your code or if you want to download

166
00:06:22,750 --> 00:06:25,630
the CSV again you have to like delete

167
00:06:25,630 --> 00:06:27,340
all the data you imported last time and

168
00:06:27,340 --> 00:06:28,360
then just re-import the whole thing

169
00:06:28,360 --> 00:06:29,770
again or it's just like there's no

170
00:06:29,770 --> 00:06:31,750
automation if there's no automation

171
00:06:31,750 --> 00:06:33,190
there's no reproducibility for these

172
00:06:33,190 --> 00:06:35,710
scientists so you know you have to do

173
00:06:35,710 --> 00:06:37,470
things like emailing CSV files around

174
00:06:37,470 --> 00:06:39,900
now this is a popular one if it fits in

175
00:06:39,900 --> 00:06:41,520
the in get you can just put like you

176
00:06:41,520 --> 00:06:43,860
know your sequel table or export into

177
00:06:43,860 --> 00:06:46,710
get which is you know just like a crazy

178
00:06:46,710 --> 00:06:50,250
hack and so we're just trying to do for

179
00:06:50,250 --> 00:06:52,140
data what get did for source code and

180
00:06:52,140 --> 00:06:54,510
don't take that to literally some some

181
00:06:54,510 --> 00:06:55,830
people think I like we literally have

182
00:06:55,830 --> 00:06:58,260
the same commands as good but it's a

183
00:06:58,260 --> 00:06:59,610
different problem so we have a different

184
00:06:59,610 --> 00:07:02,040
approach but we're inspired by the

185
00:07:02,040 --> 00:07:04,560
automation that get brings so dad is a

186
00:07:04,560 --> 00:07:06,450
module you can install that and we have

187
00:07:06,450 --> 00:07:08,910
a lot of dependencies the only

188
00:07:08,910 --> 00:07:11,100
dependencies and aren't JavaScript our

189
00:07:11,100 --> 00:07:18,000
node itself and level to be and there's

190
00:07:18,000 --> 00:07:19,530
a command-line API so the main idea what

191
00:07:19,530 --> 00:07:21,030
that is you you know you can do data in

192
00:07:21,030 --> 00:07:23,300
it create a new debt store in a folder

193
00:07:23,300 --> 00:07:25,860
anything that produces data you can pipe

194
00:07:25,860 --> 00:07:27,750
into a debt import and then that starts

195
00:07:27,750 --> 00:07:29,130
tracking the data in debt and then once

196
00:07:29,130 --> 00:07:31,380
it's in debt you can do things like that

197
00:07:31,380 --> 00:07:33,120
listen which starts a server that other

198
00:07:33,120 --> 00:07:34,620
people can debt pull from ORD a clone

199
00:07:34,620 --> 00:07:36,419
you can that push to another server

200
00:07:36,419 --> 00:07:38,730
we're just trying to make these

201
00:07:38,730 --> 00:07:40,320
automated data sharing pipelines easy

202
00:07:40,320 --> 00:07:42,990
and on the command line I forget I

203
00:07:42,990 --> 00:07:45,419
always forget that I highlight those so

204
00:07:45,419 --> 00:07:47,010
for instance let me just show a demo I

205
00:07:47,010 --> 00:07:50,580
have my 23 Emmy data somewhere in one of

206
00:07:50,580 --> 00:07:55,470
these so I have in the command line this

207
00:07:55,470 --> 00:07:56,700
is just like an empty folder there's

208
00:07:56,700 --> 00:07:58,560
nothing in here nothing at my sleeve and

209
00:07:58,560 --> 00:08:02,040
if I do that in it and make a new debt

210
00:08:02,040 --> 00:08:03,210
folder it kind of asked me for some

211
00:08:03,210 --> 00:08:07,350
metadata I'll say my genetic markers

212
00:08:07,350 --> 00:08:14,880
from 23 and me and I guess I am my own

213
00:08:14,880 --> 00:08:18,330
publisher of my DNA and so if i run this

214
00:08:18,330 --> 00:08:21,330
little pipeline the data comes from 23

215
00:08:21,330 --> 00:08:24,180
with me as a tsv and so it kind of looks

216
00:08:24,180 --> 00:08:26,850
like this it's basically CSV about these

217
00:08:26,850 --> 00:08:29,490
are tabs and I can parse it using the

218
00:08:29,490 --> 00:08:31,979
csb parser module that we wrote it with

219
00:08:31,979 --> 00:08:34,349
a tab separator so if I basically just

220
00:08:34,349 --> 00:08:37,979
run this little pipeline here then the

221
00:08:37,979 --> 00:08:40,979
last one is that import so I'll run that

222
00:08:40,979 --> 00:08:43,020
and it'll just sit there and start

223
00:08:43,020 --> 00:08:46,290
importing my parsing the tsv and

224
00:08:46,290 --> 00:08:50,010
importing it into debt and now I have

225
00:08:50,010 --> 00:08:52,110
I don't actually know how much data is

226
00:08:52,110 --> 00:08:54,570
and this thing's I think it's like 80

227
00:08:54,570 --> 00:08:55,980
thousand base pairs or something like

228
00:08:55,980 --> 00:08:59,400
that but this might take a little while

229
00:08:59,400 --> 00:09:00,420
because genomes are big so I'll just

230
00:09:00,420 --> 00:09:03,960
kill it I'll say dat listen and I can

231
00:09:03,960 --> 00:09:06,480
open up the server and we have a little

232
00:09:06,480 --> 00:09:11,490
admin UI and now it'll kind of oh I

233
00:09:11,490 --> 00:09:13,050
think the first time it has to do some

234
00:09:13,050 --> 00:09:17,370
indexing let me do a debt cat oh yeah

235
00:09:17,370 --> 00:09:18,840
here we go so there's like I just

236
00:09:18,840 --> 00:09:22,650
imported like 91,000 rows of my data

237
00:09:22,650 --> 00:09:25,170
into debt and what it adds is the

238
00:09:25,170 --> 00:09:28,650
version so if I go in say I'm a genetic

239
00:09:28,650 --> 00:09:32,700
mutation and I say this is now a CC and

240
00:09:32,700 --> 00:09:34,800
I update then now there's version 2 of

241
00:09:34,800 --> 00:09:37,650
this row and that means that also if i

242
00:09:37,650 --> 00:09:42,210
go into if i go to the changes feed kind

243
00:09:42,210 --> 00:09:44,940
of like CouchDB style and i get the last

244
00:09:44,940 --> 00:09:47,250
one then the last change to my genome

245
00:09:47,250 --> 00:09:50,640
was this key change from one to two and

246
00:09:50,640 --> 00:09:52,860
when you track this kind of metadata and

247
00:09:52,860 --> 00:09:54,840
the metadata we use to do replication

248
00:09:54,840 --> 00:09:58,190
and that means and we're also building

249
00:09:58,190 --> 00:10:00,660
kind of branching and merging and so you

250
00:10:00,660 --> 00:10:01,860
can kind of you pull requests on data

251
00:10:01,860 --> 00:10:04,290
and everything is streaming so the whole

252
00:10:04,290 --> 00:10:06,270
idea what this is you can track changes

253
00:10:06,270 --> 00:10:08,340
to data have versions of data push and

254
00:10:08,340 --> 00:10:10,140
pull data between remotes and we're

255
00:10:10,140 --> 00:10:11,610
building a lot of cool plugins on top of

256
00:10:11,610 --> 00:10:16,260
it and so you can clone a data set from

257
00:10:16,260 --> 00:10:21,270
a server and oh and then once you have a

258
00:10:21,270 --> 00:10:22,650
data set you just do dat pole and then

259
00:10:22,650 --> 00:10:23,850
you get the newest version and you can

260
00:10:23,850 --> 00:10:26,100
do that pole live and you this is where

261
00:10:26,100 --> 00:10:29,250
we started diverging from get with get

262
00:10:29,250 --> 00:10:32,010
you can pull like discreetly but with

263
00:10:32,010 --> 00:10:33,330
that you can be continuously pulling

264
00:10:33,330 --> 00:10:35,130
because you in such a database so you

265
00:10:35,130 --> 00:10:36,870
can have the data be like live updating

266
00:10:36,870 --> 00:10:39,900
my dropbox and we don't just do tabular

267
00:10:39,900 --> 00:10:42,540
data we also do large attachments we

268
00:10:42,540 --> 00:10:44,460
call them blobs binary large objects and

269
00:10:44,460 --> 00:10:46,920
there's a blob API for storing blobs the

270
00:10:46,920 --> 00:10:48,300
blobs are not stored in the database

271
00:10:48,300 --> 00:10:50,670
they're stored on file system but we

272
00:10:50,670 --> 00:10:52,950
store all the actual data like the key

273
00:10:52,950 --> 00:10:55,110
value data in the leveldb because it's

274
00:10:55,110 --> 00:10:56,760
good at that we saw blobs on file

275
00:10:56,760 --> 00:10:58,140
systems because file systems are good at

276
00:10:58,140 --> 00:11:00,300
files but we've abstracted the file

277
00:11:00,300 --> 00:11:02,880
system using this module called the

278
00:11:02,880 --> 00:11:03,690
abstract lobster

279
00:11:03,690 --> 00:11:05,760
or and this is like a community thing

280
00:11:05,760 --> 00:11:07,280
that we're trying to get going which is

281
00:11:07,280 --> 00:11:09,770
say you want to upload files anywhere

282
00:11:09,770 --> 00:11:12,210
don't write code that uploads to s3

283
00:11:12,210 --> 00:11:15,000
directly write a s3 blobstore or

284
00:11:15,000 --> 00:11:16,110
actually it turns out that we've already

285
00:11:16,110 --> 00:11:18,260
written one that you can use for s3 but

286
00:11:18,260 --> 00:11:20,690
if it's basically an API where you're

287
00:11:20,690 --> 00:11:22,920
streaming in and streaming out and it's

288
00:11:22,920 --> 00:11:24,990
files that's what the have struck web

289
00:11:24,990 --> 00:11:26,940
store is meant for but what's cool is

290
00:11:26,940 --> 00:11:28,680
you can use our test suite and then

291
00:11:28,680 --> 00:11:29,940
you're compatible with all the other Bob

292
00:11:29,940 --> 00:11:31,860
stores so your uploader service can

293
00:11:31,860 --> 00:11:33,660
instantly be compatible with all the

294
00:11:33,660 --> 00:11:35,940
blob stores that we support or that are

295
00:11:35,940 --> 00:11:37,950
implemented by different people so we're

296
00:11:37,950 --> 00:11:39,480
just trying to introduce nice little

297
00:11:39,480 --> 00:11:42,270
conventions so that we can not reinvent

298
00:11:42,270 --> 00:11:43,380
the wheel every time we want to do file

299
00:11:43,380 --> 00:11:45,990
upload oh yeah we also have cool ones

300
00:11:45,990 --> 00:11:47,940
like you can read a file from bittorrent

301
00:11:47,940 --> 00:11:49,860
using the same API as you use to Rio

302
00:11:49,860 --> 00:11:53,820
file from s3 or do you ftp or local FS

303
00:11:53,820 --> 00:11:56,760
or in memory so check check out the

304
00:11:56,760 --> 00:11:59,190
abstract blobstore module and we also do

305
00:11:59,190 --> 00:12:01,080
some stuff just really quickly with that

306
00:12:01,080 --> 00:12:02,820
when you import data we generate you a

307
00:12:02,820 --> 00:12:04,650
JSON schema by default but you can go in

308
00:12:04,650 --> 00:12:05,940
and we actually use protocol buffers

309
00:12:05,940 --> 00:12:08,400
from google to do the actual encoding so

310
00:12:08,400 --> 00:12:10,080
if you have a specific schema you want

311
00:12:10,080 --> 00:12:11,940
you can do your own schema we give you a

312
00:12:11,940 --> 00:12:13,560
free rest api on top of the database i

313
00:12:13,560 --> 00:12:14,610
kind of showed you the changes feed

314
00:12:14,610 --> 00:12:18,030
really quickly and everything a dad is

315
00:12:18,030 --> 00:12:19,620
totally hundred percent streaming so we

316
00:12:19,620 --> 00:12:20,880
can work with really large data sets

317
00:12:20,880 --> 00:12:25,290
without crashing debt so some demos so

318
00:12:25,290 --> 00:12:27,720
npm is a cool data set we actually have

319
00:12:27,720 --> 00:12:29,880
been importing the npm metadata into

320
00:12:29,880 --> 00:12:33,000
debt so we have up online NP mg org dot

321
00:12:33,000 --> 00:12:35,430
org isn't a real thing it's just where

322
00:12:35,430 --> 00:12:38,190
we host little deaths right now so we

323
00:12:38,190 --> 00:12:41,430
have 117,000 modules it's more than npm

324
00:12:41,430 --> 00:12:43,820
because we store the deleted ones also

325
00:12:43,820 --> 00:12:47,700
and we store the full data so npm

326
00:12:47,700 --> 00:12:50,130
actually has like a lot of data every

327
00:12:50,130 --> 00:12:54,150
version of every module is in every row

328
00:12:54,150 --> 00:12:56,790
so and then we have the read Me's so if

329
00:12:56,790 --> 00:12:58,500
i take this URL and i go to the terminal

330
00:12:58,500 --> 00:13:05,130
and do i already have a one here okay

331
00:13:05,130 --> 00:13:08,339
cool so if I debt clone this what it'll

332
00:13:08,339 --> 00:13:12,360
do is start cloning it and what it looks

333
00:13:12,360 --> 00:13:15,839
for is the blobs oh yeah so we actually

334
00:13:15,839 --> 00:13:16,860
store

335
00:13:16,860 --> 00:13:19,500
links to the tar balls and by default

336
00:13:19,500 --> 00:13:22,020
when you clone that will clone the tar

337
00:13:22,020 --> 00:13:24,150
balls so you'll have them locally so as

338
00:13:24,150 --> 00:13:25,800
you get a row it also fetches the blobs

339
00:13:25,800 --> 00:13:27,470
so you're actually I mean this will be

340
00:13:27,470 --> 00:13:30,600
140 gigs it'll eventually finish and you

341
00:13:30,600 --> 00:13:32,400
will make a full copy of NPM if you have

342
00:13:32,400 --> 00:13:34,380
a lot of bandwidth it doesn't mean it's

343
00:13:34,380 --> 00:13:36,300
doable but it's a little bit much for

344
00:13:36,300 --> 00:13:40,050
conference talks so let me start over

345
00:13:40,050 --> 00:13:41,580
I'll delete the folder and this time

346
00:13:41,580 --> 00:13:43,740
I'll do dash dash skim and what this

347
00:13:43,740 --> 00:13:46,530
means skim mode is we just skip the

348
00:13:46,530 --> 00:13:48,840
blobs we just stream the tar balls so

349
00:13:48,840 --> 00:13:50,760
then I you know it only takes about 10

350
00:13:50,760 --> 00:13:52,740
minutes to clone all of NPM just the

351
00:13:52,740 --> 00:13:55,440
metadata including all the reviews and

352
00:13:55,440 --> 00:13:56,790
so then you can actually run your own

353
00:13:56,790 --> 00:13:58,410
little local registry and would you

354
00:13:58,410 --> 00:14:00,600
actually want to get tarball it just

355
00:14:00,600 --> 00:14:02,400
lazily fetches it because we still have

356
00:14:02,400 --> 00:14:04,800
a link to the tar ball and two debt it's

357
00:14:04,800 --> 00:14:06,120
just a matter of it has it fetched it

358
00:14:06,120 --> 00:14:09,030
and cashed it yet or not so and you can

359
00:14:09,030 --> 00:14:10,500
stop it and then go back into there and

360
00:14:10,500 --> 00:14:11,940
do it that pole and it'll finish where I

361
00:14:11,940 --> 00:14:14,580
left off and then you can do pull dash

362
00:14:14,580 --> 00:14:17,820
dash alive and it'll keep in sync so

363
00:14:17,820 --> 00:14:21,780
just like a cooking show I have the full

364
00:14:21,780 --> 00:14:26,270
version already done and if I go to

365
00:14:26,270 --> 00:14:31,290
where's my slides did that one so we

366
00:14:31,290 --> 00:14:32,160
have a little demo where you can

367
00:14:32,160 --> 00:14:35,700
calculate how big npm is all we do is

368
00:14:35,700 --> 00:14:37,860
loop over every row using that the dad

369
00:14:37,860 --> 00:14:40,980
JavaScript API and we just have a

370
00:14:40,980 --> 00:14:43,110
rolling some of the all the blobs how

371
00:14:43,110 --> 00:14:44,790
many tar balls are an NPM basically so

372
00:14:44,790 --> 00:14:49,440
if i run this size of NPM basically just

373
00:14:49,440 --> 00:14:52,350
streams through debt and just calculates

374
00:14:52,350 --> 00:14:54,570
a big rolling some of the accumulated

375
00:14:54,570 --> 00:14:57,210
tarball sighs and I think it's like it

376
00:14:57,210 --> 00:14:59,610
gets around 150 gigs or something right

377
00:14:59,610 --> 00:15:02,130
now but you know it takes a minute to

378
00:15:02,130 --> 00:15:05,280
run and so that's examples like

379
00:15:05,280 --> 00:15:06,540
streaming data out of that but we also

380
00:15:06,540 --> 00:15:09,230
have doing something a little bit more

381
00:15:09,230 --> 00:15:13,140
cool you can pipe that cat which is just

382
00:15:13,140 --> 00:15:14,610
all the data and debt out into some

383
00:15:14,610 --> 00:15:16,680
transform anything UNIX for super unix

384
00:15:16,680 --> 00:15:19,740
fans and you can also send stalker is

385
00:15:19,740 --> 00:15:20,850
unix see you can do really cool stuff

386
00:15:20,850 --> 00:15:24,390
with docker so you can this is kind of

387
00:15:24,390 --> 00:15:25,890
like a really important use case in

388
00:15:25,890 --> 00:15:27,660
science because dependencies are hard so

389
00:15:27,660 --> 00:15:28,740
we've been using docker for a lot of

390
00:15:28,740 --> 00:15:30,270
these crazy science dependencies so this

391
00:15:30,270 --> 00:15:30,930
is an example

392
00:15:30,930 --> 00:15:33,960
if I wanted to generate a screenshot of

393
00:15:33,960 --> 00:15:36,660
every read me on NPM as a stream we

394
00:15:36,660 --> 00:15:38,490
wrote like Mateus wrote this cool thing

395
00:15:38,490 --> 00:15:41,399
called both marked down to PNG and I can

396
00:15:41,399 --> 00:15:44,220
just run that really quick so we have

397
00:15:44,220 --> 00:15:46,560
this little pipeline that gets all the

398
00:15:46,560 --> 00:15:48,390
modules greater than or equal to debt so

399
00:15:48,390 --> 00:15:50,430
like the debt module and then pipes them

400
00:15:50,430 --> 00:15:52,050
into Matias thing and that generates a

401
00:15:52,050 --> 00:15:59,270
tar balls so let me actually grab this

402
00:15:59,270 --> 00:16:02,670
and I'll not do 1 i'll be like 10

403
00:16:02,670 --> 00:16:08,850
actually so oops the emoji cat in my ps1

404
00:16:08,850 --> 00:16:11,459
kind of messes with stuff sometimes oh

405
00:16:11,459 --> 00:16:13,589
yeah greater than debt limit 10 so I'll

406
00:16:13,589 --> 00:16:16,230
generate 10 read Me's like the first 10

407
00:16:16,230 --> 00:16:17,700
modules that have that in the name on

408
00:16:17,700 --> 00:16:19,709
NPM so what it's doing is it's piping

409
00:16:19,709 --> 00:16:21,120
data out of debt spawning a little

410
00:16:21,120 --> 00:16:23,130
virtual linux container using docker

411
00:16:23,130 --> 00:16:24,690
generator to read me in a headless

412
00:16:24,690 --> 00:16:27,899
browser and then it generated me this

413
00:16:27,899 --> 00:16:30,029
tar ball just now and if I extract the

414
00:16:30,029 --> 00:16:32,010
tarball then I get a folder full of

415
00:16:32,010 --> 00:16:35,100
PNG's and they're basically rendered in

416
00:16:35,100 --> 00:16:37,020
phantom jas but you don't have to worry

417
00:16:37,020 --> 00:16:38,100
about that you just use the docker

418
00:16:38,100 --> 00:16:40,230
container and everything streaming so

419
00:16:40,230 --> 00:16:41,190
you can do like these really nice

420
00:16:41,190 --> 00:16:43,380
advanced data processing things on data

421
00:16:43,380 --> 00:16:45,720
sets of infinite size with like relative

422
00:16:45,720 --> 00:16:49,950
ease that's kind of the goal and we also

423
00:16:49,950 --> 00:16:51,060
made this which we're really proud of

424
00:16:51,060 --> 00:16:57,209
which is called where is it here I'll

425
00:16:57,209 --> 00:16:59,339
just go to it directly it's called get

426
00:16:59,339 --> 00:17:01,680
debt we also use docker for this it's an

427
00:17:01,680 --> 00:17:04,380
in-browser debt environment we log you

428
00:17:04,380 --> 00:17:06,660
in as root to a new linux machine on our

429
00:17:06,660 --> 00:17:10,020
server it's a virtual machine and You

430
00:17:10,020 --> 00:17:13,410
Know Who am I I am root your sandbox

431
00:17:13,410 --> 00:17:16,020
though and you can type stuff like NPM

432
00:17:16,020 --> 00:17:18,300
install the unique module from NPM and

433
00:17:18,300 --> 00:17:19,949
it'll actually get a real and p.m. it'll

434
00:17:19,949 --> 00:17:21,240
do real and Pam install on the file

435
00:17:21,240 --> 00:17:23,280
system but in order to make it a little

436
00:17:23,280 --> 00:17:25,230
bit easier to learn we sync the file

437
00:17:25,230 --> 00:17:26,459
system to this little editor that we

438
00:17:26,459 --> 00:17:28,650
wrote so you can actually go in there

439
00:17:28,650 --> 00:17:31,200
and inspect it and you can say oh I want

440
00:17:31,200 --> 00:17:32,250
to go edit the source code of this

441
00:17:32,250 --> 00:17:35,670
module and say console.log hello Jess

442
00:17:35,670 --> 00:17:38,280
fists and then if you go up here and you

443
00:17:38,280 --> 00:17:39,780
go into the node ripple and require

444
00:17:39,780 --> 00:17:42,510
unique then it'll type hello jazz fest

445
00:17:42,510 --> 00:17:44,070
so we sync the filesystem back and forth

446
00:17:44,070 --> 00:17:44,590
from the brow

447
00:17:44,590 --> 00:17:47,049
to the server so the point of this is

448
00:17:47,049 --> 00:17:48,309
you have this tutorial over here that

449
00:17:48,309 --> 00:17:50,500
teaches you the fundamentals of debt you

450
00:17:50,500 --> 00:17:53,260
can also do stuff like require an HTTP

451
00:17:53,260 --> 00:18:03,590
server um and a live coding with latency

452
00:18:03,600 --> 00:18:06,190
so if I write a little HTTP server in

453
00:18:06,190 --> 00:18:08,260
here we wanted it to be real so it's

454
00:18:08,260 --> 00:18:11,860
like you could actually do real stuff on

455
00:18:11,860 --> 00:18:13,480
a real server and not do like a fake

456
00:18:13,480 --> 00:18:14,799
environment like what like Codecademy

457
00:18:14,799 --> 00:18:17,020
does or something like that so if I just

458
00:18:17,020 --> 00:18:23,320
say res and high then i go to my welcome

459
00:18:23,320 --> 00:18:25,809
text it has a when you load the page it

460
00:18:25,809 --> 00:18:28,630
gives you a unique coast and anything

461
00:18:28,630 --> 00:18:30,490
that listens on port 80 is a real server

462
00:18:30,490 --> 00:18:32,919
on the real internet so I have high here

463
00:18:32,919 --> 00:18:36,159
so what's cool is when you close the tab

464
00:18:36,159 --> 00:18:39,460
we close the process so you can't it's

465
00:18:39,460 --> 00:18:41,230
kind of like our anti abuse policy so

466
00:18:41,230 --> 00:18:43,960
now I can't find it but we do store the

467
00:18:43,960 --> 00:18:46,570
data so in the tutorial you actually use

468
00:18:46,570 --> 00:18:48,220
a real debt you start a real debt server

469
00:18:48,220 --> 00:18:49,990
and you actually actually clone your own

470
00:18:49,990 --> 00:18:51,580
data from the server down to your laptop

471
00:18:51,580 --> 00:18:53,350
so you install that locally on like step

472
00:18:53,350 --> 00:18:55,600
5 so we want to do something real and

473
00:18:55,600 --> 00:18:58,090
this is actually a preview of we're

474
00:18:58,090 --> 00:18:59,950
working on adding this functionality to

475
00:18:59,950 --> 00:19:02,110
all node schools so that anyone can get

476
00:19:02,110 --> 00:19:04,330
notes cool in one click and not have to

477
00:19:04,330 --> 00:19:06,220
install everything locally if they don't

478
00:19:06,220 --> 00:19:11,950
want to so we're also working on a bunch

479
00:19:11,950 --> 00:19:13,330
of DNA processing one of people on our

480
00:19:13,330 --> 00:19:14,919
team is doing his PhD on this stuff and

481
00:19:14,919 --> 00:19:16,240
we have a project called bio node and

482
00:19:16,240 --> 00:19:17,830
what that has taught us is that we need

483
00:19:17,830 --> 00:19:19,929
to set you handle these advanced data

484
00:19:19,929 --> 00:19:21,520
pipelines with lots of crazy

485
00:19:21,520 --> 00:19:22,809
dependencies and everything needs to be

486
00:19:22,809 --> 00:19:24,510
streaming because the data sets are huge

487
00:19:24,510 --> 00:19:27,309
and we're using a lot of docker for

488
00:19:27,309 --> 00:19:28,779
automating these really difficult to

489
00:19:28,779 --> 00:19:31,870
install bioinformatics tools we've been

490
00:19:31,870 --> 00:19:32,890
playing around with this thing called

491
00:19:32,890 --> 00:19:34,149
gasket if you were at node comp this

492
00:19:34,149 --> 00:19:35,740
summer we had a very early version of

493
00:19:35,740 --> 00:19:37,480
that which is like trying to build these

494
00:19:37,480 --> 00:19:39,190
cross-platform data pipelines we're

495
00:19:39,190 --> 00:19:41,110
basically it's a really cool use case

496
00:19:41,110 --> 00:19:42,970
for note I think because it's note isn't

497
00:19:42,970 --> 00:19:44,950
doing is the computation notice spawning

498
00:19:44,950 --> 00:19:46,299
the processes and moving the data

499
00:19:46,299 --> 00:19:48,399
between them so it's just I mean that's

500
00:19:48,399 --> 00:19:50,830
kind of I mean it's io literally it's

501
00:19:50,830 --> 00:19:53,950
what note is built for and not

502
00:19:53,950 --> 00:19:56,020
everything is 11 min monolithic note app

503
00:19:56,020 --> 00:19:58,360
it's an invented pipeline where data is

504
00:19:58,360 --> 00:19:58,840
getting pipe

505
00:19:58,840 --> 00:20:00,250
in and out of different places and

506
00:20:00,250 --> 00:20:01,620
coming from different network services

507
00:20:01,620 --> 00:20:03,760
so I think it's a really cool use case

508
00:20:03,760 --> 00:20:05,620
and gasket looks like this it's JSON and

509
00:20:05,620 --> 00:20:07,780
you can declare pipelines but it's

510
00:20:07,780 --> 00:20:09,220
little bit too linear for us so we've

511
00:20:09,220 --> 00:20:10,930
been experimenting with a single debt

512
00:20:10,930 --> 00:20:14,170
script super experimental which you can

513
00:20:14,170 --> 00:20:16,810
define pipelines that do stuff like you

514
00:20:16,810 --> 00:20:19,120
know this is a reads pipeline it will

515
00:20:19,120 --> 00:20:21,730
run a search that outputs JSON and then

516
00:20:21,730 --> 00:20:24,730
fork for pipelines and pipe the search

517
00:20:24,730 --> 00:20:26,950
output into all four and then aggregate

518
00:20:26,950 --> 00:20:30,040
those for pipelines into a debt so you

519
00:20:30,040 --> 00:20:31,710
can do really powerful stuff in a really

520
00:20:31,710 --> 00:20:34,810
simple expressive syntax this is like

521
00:20:34,810 --> 00:20:36,610
super alpha but where this is the kind

522
00:20:36,610 --> 00:20:38,920
of stuff that were interested in stuff

523
00:20:38,920 --> 00:20:40,300
that's the future of the project we want

524
00:20:40,300 --> 00:20:42,310
to be able to check out a data set to a

525
00:20:42,310 --> 00:20:44,260
specific point in time just like it so

526
00:20:44,260 --> 00:20:45,700
you can say get me the data as it was

527
00:20:45,700 --> 00:20:47,800
two weeks ago so that when you publish

528
00:20:47,800 --> 00:20:49,540
your paper you can have the hash of the

529
00:20:49,540 --> 00:20:50,770
data of the paper so people can

530
00:20:50,770 --> 00:20:53,350
reproduce it we're doing a lot of really

531
00:20:53,350 --> 00:20:54,400
cool stuff for multi-master replication

532
00:20:54,400 --> 00:20:56,950
and merging that Matthias will talk

533
00:20:56,950 --> 00:20:59,500
about and we want to sync to a bunch of

534
00:20:59,500 --> 00:21:01,480
different databases so that you can have

535
00:21:01,480 --> 00:21:04,060
stuff like I wrote a thing the other day

536
00:21:04,060 --> 00:21:06,010
that takes any data from debt and puts

537
00:21:06,010 --> 00:21:07,600
it into a full text index you can just

538
00:21:07,600 --> 00:21:08,860
do that automatically and not have to

539
00:21:08,860 --> 00:21:10,930
install a database it uses sequel light

540
00:21:10,930 --> 00:21:12,280
under the hood but we can stopped up for

541
00:21:12,280 --> 00:21:14,530
like elasticsearch doing these like data

542
00:21:14,530 --> 00:21:17,320
automation tasks and then finally we're

543
00:21:17,320 --> 00:21:18,490
building a registry which is like what

544
00:21:18,490 --> 00:21:21,670
I'm super excited about the registry is

545
00:21:21,670 --> 00:21:23,200
like a super sneak preview first time

546
00:21:23,200 --> 00:21:24,940
we've shown it publicly at all it's not

547
00:21:24,940 --> 00:21:26,650
even like Alpha yet we're still working

548
00:21:26,650 --> 00:21:28,240
on the first version but you'll be able

549
00:21:28,240 --> 00:21:29,980
to publish data sets to the registry and

550
00:21:29,980 --> 00:21:31,810
find them and clone them and get

551
00:21:31,810 --> 00:21:33,850
information about them and actually view

552
00:21:33,850 --> 00:21:36,280
the data in line and this is super super

553
00:21:36,280 --> 00:21:38,650
early like i said but we think that

554
00:21:38,650 --> 00:21:40,510
putting all the data online and making

555
00:21:40,510 --> 00:21:42,670
it easy to find is going to help a lot

556
00:21:42,670 --> 00:21:45,840
of these build these ecosystems out so

557
00:21:45,840 --> 00:21:49,090
with that this is like how to find us

558
00:21:49,090 --> 00:21:51,610
dad on freenode we have a gator and or

559
00:21:51,610 --> 00:21:53,470
also the debt repo and i'll hand it off

560
00:21:53,470 --> 00:21:59,600
to Matthias now thank you

