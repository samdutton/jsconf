1
00:00:03,100 --> 00:00:05,303
Emily Gorcenski-The Ethical Things of the Internet 

2
00:00:29,890 --> 00:00:30,900
Emily: Thank you very much.

3
00:00:30,900 --> 00:00:34,789
I'm going to talk about the ethics of this issue and I promise I won't go

4
00:00:34,789 --> 00:00:37,960
Too much lecture.

5
00:00:37,960 --> 00:00:38,960
This is Emily Gorcenski.

6
00:00:38,960 --> 00:00:40,309
I'm on Twitter.

7
00:00:40,309 --> 00:00:43,440
I sometimes say something.

8
00:00:43,440 --> 00:00:50,859
I like the Internet of Things and the landscape it is creating.

9
00:00:50,859 --> 00:00:53,859
So why should I talk about the Internet? Things at a JavaScript conference?

10
00:00:53,859 --> 00:00:56,839
Why should I use JavaScript to talk about ethics meetings?

11
00:00:56,839 --> 00:01:02,020
This is because I have given this speech many times and I joked that every time I give it, I can

12
00:01:02,020 --> 00:01:06,979
Give a whole new conversation, because there are so many problems, there are many failures, and

13
00:01:06,979 --> 00:01:11,659
Errors, and security issues that occur strangely if I only focus on the case

14
00:01:11,659 --> 00:01:15,509
Research, this will be a whole new conversation time.

15
00:01:15,509 --> 00:01:19,170
This time, I decided I didn't want to talk about this topic anymore.

16
00:01:19,170 --> 00:01:23,630
I hope you guys can say that, so I want to talk more

17
00:01:23,630 --> 00:01:28,430
About why ethical issues are important and why you should do this is a JavaScript developer and

18
00:01:28,430 --> 00:01:38,070
How can we integrate technology into a variety of abnormal equipment and services

19
00:01:38,070 --> 00:01:40,200
belong.

20
00:01:40,200 --> 00:01:43,790
Doing moral talk is almost impossible without heavy things.

21
00:01:43,790 --> 00:01:45,369
There are warnings about the conversation.

22
00:01:45,369 --> 00:01:51,770
We will talk frankly about some incidents that resulted in injuries and deaths.

23
00:01:51,770 --> 00:01:56,520
Discussion about specific examples of sex and the image of raw meat.

24
00:01:56,520 --> 00:02:04,020
If this scares you, then maybe that's about ten minutes of conversation.

25
00:02:04,020 --> 00:02:05,020
who am I?

26
00:02:05,020 --> 00:02:07,100
I'm a bit frank.

27
00:02:07,100 --> 00:02:08,789
I'm a little offended here.

28
00:02:08,789 --> 00:02:16,800
I am not a JavaScript developer.

29
00:02:16,800 --> 00:02:21,950
I am a data scientist and I am trained as a mathematician and engineer.

30
00:02:21,950 --> 00:02:27,140
I went to aeronautical engineering school and mechanical engineering, I worked

31
00:02:27,140 --> 00:02:33,490
In my aerospace, biotechnology and career I now work in the financial industry.

32
00:02:33,490 --> 00:02:39,190
What these industries have in common is that they are all heavily regulated and most of them are regulated

33
00:02:39,190 --> 00:02:44,410
People who work among them subscribe through professional ethics

34
00:02:44,410 --> 00:02:53,620
An independent society or other organization that guides the meaning of ethical behavior.

35
00:02:53,620 --> 00:02:57,180
I'm here, I'm in defense, health, and banking, and I want to talk to you

36
00:02:57,180 --> 00:03:02,120
About Morality-Deduction!

37
00:03:02,120 --> 00:03:04,880
What do I mean when I talk about the Internet of Things?

38
00:03:04,880 --> 00:03:06,510
This is a conservative definition of warranty.

39
00:03:06,510 --> 00:03:11,540
We might consider smart refrigerators or smart refrigerator cars, that kind of thing.

40
00:03:11,540 --> 00:03:15,210
I like to think of it as a place where the Internet usually doesn't belong.

41
00:03:15,210 --> 00:03:17,700
So it may be a smart appliance.

42
00:03:17,700 --> 00:03:24,540
I think Uber is an IoT taxi.

43
00:03:24,540 --> 00:03:30,400
When we look at this ethics, we have to do this to see the entire range of what we are doing

44
00:03:30,400 --> 00:03:32,640
With our technology and our contact information?

45
00:03:32,640 --> 00:03:38,760
The difference is that these devices and products or services are not yet computerized,

46
00:03:38,760 --> 00:03:44,150
We're letting consumers have the connection exactly what's happening.

47
00:03:44,150 --> 00:03:49,540
So if you are a JavaScript developer, maybe you want a bread maker so you can hack

48
00:03:49,540 --> 00:03:52,060
Your code while baking bread.

49
00:03:52,060 --> 00:03:58,450
This is important because IoT products are the next level of convenience and optimization.

50
00:03:58,450 --> 00:04:03,390
We have been optimizing our products for the past 30 years and for convenience, nothing more

51
00:04:03,390 --> 00:04:07,090
You can get the competitive advantage of the refrigerator now.

52
00:04:07,090 --> 00:04:11,280
So if you don't have a competitive advantage using unconnected devices, you have to go

53
00:04:11,280 --> 00:04:12,280
connected.

54
00:04:12,280 --> 00:04:17,950
It is also important for those who have a livelihood to be affected by disability.

55
00:04:17,950 --> 00:04:24,250
You might worry about the ability to monitor IoT devices or terrible

56
00:04:24,250 --> 00:04:29,940
Uber is accused of doing things, but if you ca nâ€™t bypass or cannot bypass

57
00:04:29,940 --> 00:04:35,400
Live in a relaxing taxi service where you have other needs, something

58
00:04:35,400 --> 00:04:38,470
Like Uber is a life saver and changes your life.

59
00:04:38,470 --> 00:04:49,540
We can't write it as ridiculous that the internet of things is frivolous, we have internet twitter

60
00:04:49,540 --> 00:04:54,630
Account, there are a lot of misses and there are a lot of good things

61
00:04:54,630 --> 00:04:57,180
Also from the Internet of Things.

62
00:04:57,180 --> 00:05:06,310
When I talk about morality, what do I mean when I say that word?

63
00:05:06,310 --> 00:05:07,870
You may have seen this picture.

64
00:05:07,870 --> 00:05:12,440
The frame is with one Nobel Prize winner tied to five sets of tracks and another tied

65
00:05:12,440 --> 00:05:13,990
to another one.

66
00:05:13,990 --> 00:05:18,630
Somehow you have been put in this position by the lever.

67
00:05:18,630 --> 00:05:22,880
This is a very popular question on the internet now because it feels like something

68
00:05:22,880 --> 00:05:26,900
If we just, we can use category theory to solve it, two, it really

69
00:05:26,900 --> 00:05:30,470
For some wet memes.

70
00:05:30,470 --> 00:05:35,720
The thing about the trolley problem is that the trolley problem is not the trolley problem,

71
00:05:35,720 --> 00:05:39,290
So why are there problems with self-driving cars?

72
00:05:39,290 --> 00:05:47,670
We like to solve things as problems-this is our nature as developers and engineers.

73
00:05:47,670 --> 00:05:52,150
In the field of technology, we don't actually face moral dilemmas that often.

74
00:05:52,150 --> 00:05:56,480
When there are two, moral dilemma occurs

75
00:05:56,480 --> 00:06:02,120
What you take must not violate at least one of them.

76
00:06:02,120 --> 00:06:06,330
I think JavaScript is attractive and the JavaScript community is in charge

77
00:06:06,330 --> 00:06:12,169
Because I think the most fascinating real moral dilemma in a decade of technology.

78
00:06:12,169 --> 00:06:14,110
I will tell you later.

79
00:06:14,110 --> 00:06:17,790
Some people may already know what I'm talking about.

80
00:06:17,790 --> 00:06:22,500
Technical issues usually mean we don't follow ethics.

81
00:06:22,500 --> 00:06:28,020
I am not saying that this is an indictment that you are a bad person, immoral, immoral person.

82
00:06:28,020 --> 00:06:33,960
There are companies out there that will get a side look now, but I mean in our

83
00:06:33,960 --> 00:06:36,950
Industry, we have no professional code.

84
00:06:36,950 --> 00:06:40,480
Some clubs can join, but if you are one of them, please raise your hand

85
00:06:40,480 --> 00:06:42,639
Like ACM or I888.

86
00:06:42,639 --> 00:06:49,210
There are some, but it is not the majority.

87
00:06:49,210 --> 00:06:54,450
In practice, morals are about things: they are analysis of harm and mitigation

88
00:06:54,450 --> 00:06:55,450
risk.

89
00:06:55,450 --> 00:07:01,501
So when we talk about moral behavior, especially when it comes to research ethics, what are we

90
00:07:01,501 --> 00:07:05,990
What we are doing is not to eliminate the possibility of injury,

91
00:07:05,990 --> 00:07:11,820
But we are trying to understand all these ways our technology can hurt someone,

92
00:07:11,820 --> 00:07:17,120
We are looking for actions we can take to reduce the chance of this happening,

93
00:07:17,120 --> 00:07:23,400
Reduce its severity when it happens, and provide remedies when it inevitably

94
00:07:23,400 --> 00:07:24,400
indeed.

95
00:07:24,400 --> 00:07:28,730
So this is the ethical standard we need to raise when we develop technology

96
00:07:28,730 --> 00:07:31,650
Especially the Internet of Things.

97
00:07:31,650 --> 00:07:34,639
Therefore, injuries can occur in three ways: first, through malfeasance.

98
00:07:34,639 --> 00:07:37,940
This is the most common theme in the Internet of Things.

99
00:07:37,940 --> 00:07:38,940
It's safe.

100
00:07:38,940 --> 00:07:42,090
This is where people talk about hacking.

101
00:07:42,090 --> 00:07:48,730
This was the biggest problem when the last DS attack on the Miri botnet occurred.

102
00:07:48,730 --> 00:07:54,270
Witnessing the occurrence of a DOS attack event IoT devices are insecure, you know

103
00:07:54,270 --> 00:07:57,620
IoT security is in a very bad state right away.

104
00:07:57,620 --> 00:08:04,050
When this happened, its timing and the way it was structured gave many people

105
00:08:04,050 --> 00:08:09,360
Many people worry that this is a precursor to a US presidential election,

106
00:08:09,360 --> 00:08:14,729
This will be an influential attempt and it turns out that the result of this election,

107
00:08:14,729 --> 00:08:19,180
Fear is unfounded-we managed to mess up that one on our own-because I didn't

108
00:08:19,180 --> 00:08:21,310
I want to talk about the security issues in this conversation.

109
00:08:21,310 --> 00:08:30,621
First of all, I ca nâ€™t cover everything; it succeeds, if we do this it may cause other harm

110
00:08:30,621 --> 00:08:35,329
To solve these problems, we solved the security problem.

111
00:08:35,329 --> 00:08:37,779
Failures are errors and software failures.

112
00:08:37,779 --> 00:08:43,430
What happened while the equipment was running was normal business environment, but it got

113
00:08:43,430 --> 00:08:46,670
Stuck in a situation we didn't predict as a developer.

114
00:08:46,670 --> 00:08:55,639
Sometimes it's worth mentioning that we like to stay ... except

115
00:08:55,639 --> 00:08:57,649
Semantics.

116
00:08:57,649 --> 00:09:00,550
This is a good example, this is on Twitter.

117
00:09:00,550 --> 00:09:09,160
The poor gentleman, Andrew, has an IoT water cooler and his TLS certificate has expired, of which

118
00:09:09,160 --> 00:09:13,420
Cause some blocking code, which means hardware interlock fails, he has water

119
00:09:13,420 --> 00:09:15,939
His house is everywhere.

120
00:09:15,939 --> 00:09:17,470
This is a real question, right?

121
00:09:17,470 --> 00:09:19,339
This is a problem.

122
00:09:19,339 --> 00:09:26,050
If a TLS certificate expires in a web service, just like in cyberspace, we forget TLS

123
00:09:26,050 --> 00:09:32,240
Certificate, we have a blocking code and we have someone who intends to deal with this.

124
00:09:32,240 --> 00:09:38,879
We cannot treat IoT devices more like cows, we must treat them like pets

125
00:09:38,879 --> 00:09:43,190
Living in people's homes, very, very angry when they are not being fed.

126
00:09:43,190 --> 00:09:53,610
One day, if we are not careful, we will put JavaScript in. I do nâ€™t know, an IoT

127
00:09:53,610 --> 00:09:58,050
Kettle and lit someone's house on fire because "undefined" is not a feature.

128
00:09:58,050 --> 00:10:06,390
I stole this joke, and it was such a return on investment because he didn't save me MPM socks!

129
00:10:06,390 --> 00:10:08,800
This is what I have done, and I am proud of it.

130
00:10:08,800 --> 00:10:13,410
A few years ago, I did this. If I can, I can do it full screen.

131
00:10:13,410 --> 00:10:19,899
Where is my mouse-we are gone.

132
00:10:19,899 --> 00:10:25,499
This is a Microsoft band-I am not picking here at Microsoft-this is a piece

133
00:10:25,499 --> 00:10:27,870
Raw chicken.

134
00:10:27,870 --> 00:10:32,480
I â€™m not as scary as a chicken. There is a zombie chicken. This is a piece.

135
00:10:32,480 --> 00:10:35,459
The meat I bought from the grocery store score.

136
00:10:35,459 --> 00:10:40,819
Its heart rate is 120 minutes per time!

137
00:10:40,819 --> 00:10:43,179
[laughter].

138
00:10:43,179 --> 00:10:48,699
In the real world, sensors are messy, they are noisy, they are not perfect.

139
00:10:48,699 --> 00:10:51,990
So when we design for the Internet of Things, we have to take this into account.

140
00:10:51,990 --> 00:10:57,279
You can read a piece of chicken breast whose heart rate is ridiculous, but this has

141
00:10:57,279 --> 00:11:01,760
It has a profound and profound impact on many things.

142
00:11:01,760 --> 00:11:06,829
First, there are universities that require students to wear FitBits.

143
00:11:06,829 --> 00:11:13,189
There are employers out there that have health insurance incentive plans.

144
00:11:13,189 --> 00:11:18,980
If you're following what's happening in health care in the United States, now we have this question

145
00:11:18,980 --> 00:11:23,730
We have monitoring equipment where we monitor our health and report existing conditions

146
00:11:23,730 --> 00:11:24,839
condition.

147
00:11:24,839 --> 00:11:30,540
This is not actually a hypothesis, this is what really happened.

148
00:11:30,540 --> 00:11:42,970
If possible, let me get out of full screen mode.

149
00:11:42,970 --> 00:11:49,230
In 2015, a lady was visiting a colleague.

150
00:11:49,230 --> 00:11:52,970
She pulled police to report sexual assault.

151
00:11:52,970 --> 00:11:57,209
When the police investigated them and found that she was a bit fit, and with her permission, they conducted an analysis

152
00:11:57,209 --> 00:11:59,930
data.

153
00:11:59,930 --> 00:12:06,389
When they analyzed the data, not only that they gave up the investigation of her claims,

154
00:12:06,389 --> 00:12:10,459
But they turned around, and they accused her of making false statements to the police,

155
00:12:10,459 --> 00:12:19,579
Last year, she was charged with convictions and sentenced to probation.

156
00:12:19,579 --> 00:12:25,490
Prosecutors said FitBit data blocked the transaction.

157
00:12:25,490 --> 00:12:30,139
I can pull 120 beats per minute raw chicken and woman's life is ruined

158
00:12:30,139 --> 00:12:35,959
Because no one at FitBit stood up and said no, our device was inaccurate.

159
00:12:35,959 --> 00:12:41,630
You cannot do this.

160
00:12:41,630 --> 00:12:45,319
Our equipment testifies to us falsely, and they can.

161
00:12:45,319 --> 00:12:51,699
The problem is that without regulation, how do we structure their guarantees or standards.

162
00:12:51,699 --> 00:12:53,089
We just send the code.

163
00:12:53,089 --> 00:12:54,410
We ship hardware.

164
00:12:54,410 --> 00:12:58,100
We are innovative, fast, fast, fast.

165
00:12:58,100 --> 00:13:03,850
We don't ask ourselves what kind of harm would happen when this goes wrong?

166
00:13:03,850 --> 00:13:06,680
This is happening more and more frequently.

167
00:13:06,680 --> 00:13:10,519
These devices are used in criminal and criminal civil investigations.

168
00:13:10,519 --> 00:13:15,379
Just last week, CNN reported a man charged with murder of his wife

169
00:13:15,379 --> 00:13:22,019
Data about FitBit says she has traveled a certain distance without

170
00:13:22,019 --> 00:13:25,420
Associated with his story.

171
00:13:25,420 --> 00:13:30,249
For example, anyone who wears knitwear will know that it will be recorded

172
00:13:30,249 --> 00:13:34,449
You sit on the steps on the sofa.

173
00:13:34,449 --> 00:13:37,959
How can we make this happen?

174
00:13:37,959 --> 00:13:42,209
How can we make this information impact people's lives?

175
00:13:42,209 --> 00:13:46,649
Another thing: Smart water meters were used in last year's murder investigation.

176
00:13:46,649 --> 00:13:53,189
In the same survey, they also submitted a guarantee for Amazon Echo data.

177
00:13:53,189 --> 00:13:55,649
The question is: who is going to jail?

178
00:13:55,649 --> 00:14:00,139
When will someone try a probation device to make a false statement to the police?

179
00:14:00,139 --> 00:14:04,610
And, say something, rest, someone is injured or someone is injured

180
00:14:04,610 --> 00:14:05,610
Killed?

181
00:14:05,610 --> 00:14:09,350
Who will be held responsible for an accident if the equipment causes it?

182
00:14:09,350 --> 00:14:12,639
Is it the owner?

183
00:14:12,639 --> 00:14:14,860
Is it a developer?

184
00:14:14,860 --> 00:14:17,069
The company that made it?

185
00:14:17,069 --> 00:14:21,269
This may seem like a problem, but it is not.

186
00:14:21,269 --> 00:14:23,329
This has happened.

187
00:14:23,329 --> 00:14:27,730
In this frame you will see the vehicle, the white vehicle on the right is a Google self-driving car

188
00:14:27,730 --> 00:14:33,269
Car, this image is a still image-this is a screenshot taken from a video

189
00:14:33,269 --> 00:14:42,499
Dashcam of a mountain municipal bus view California.

190
00:14:42,499 --> 00:14:47,240
Google SUV is about to exit the bus and an accident occurs.

191
00:14:47,240 --> 00:14:49,439
Thankfully, no one was injured.

192
00:14:49,439 --> 00:14:52,239
No injuries, but the fenders were bent.

193
00:14:52,239 --> 00:14:57,379
This is the first time a self-driving car has been found responsible for the damage

194
00:14:57,379 --> 00:15:00,809
An accident.

195
00:15:00,809 --> 00:15:01,899
Google expressed its dissatisfaction.

196
00:15:01,899 --> 00:15:03,589
They said, "You know what?

197
00:15:03,589 --> 00:15:05,449
We are not good

198
00:15:05,449 --> 00:15:08,550
We will be responsible for compensation. "

199
00:15:08,550 --> 00:15:14,180
They investigated what happened and they concluded that the car had predicted

200
00:15:14,180 --> 00:15:18,550
Because we are ahead, the bus will affect us.

201
00:15:18,550 --> 00:15:23,699
Okay, now Google is in a good position where they want to transport self-driving cars,

202
00:15:23,699 --> 00:15:27,660
So of course they take responsibility because they don't want to test it

203
00:15:27,660 --> 00:15:29,470
in the court.

204
00:15:29,470 --> 00:15:32,109
But as we enter, we cannot rely on it for the future of the Internet of Things.

205
00:15:32,109 --> 00:15:39,179
We cannot rely on benevolent companies to take responsibility once they emerge on a large scale.

206
00:15:39,179 --> 00:15:43,160
By the way, even if Google is right, it will be a historic moment

207
00:15:43,160 --> 00:15:47,720
Because if the bus has succumbed to the vehicle, it will be the first municipal

208
00:15:47,720 --> 00:15:54,839
The bus has yielded!

209
00:15:54,839 --> 00:15:59,480
A few years ago, there was a judge in San Francisco as part of a research project, not

210
00:15:59,480 --> 00:16:06,069
In part of the case he investigated whether the autonomous system belongs to an existing system

211
00:16:06,069 --> 00:16:07,649
Responsibility theory.

212
00:16:07,649 --> 00:16:15,449
In the investigation, he found the vehicle to make our own decision, something to use

213
00:16:15,449 --> 00:16:23,519
Control the system like an adaptive self-tuning neural network, intelligent system, if you want,

214
00:16:23,519 --> 00:16:28,639
Designing your own means to accomplish tasks may not take any responsibility

215
00:16:28,639 --> 00:16:31,749
Existing tort theory.

216
00:16:31,749 --> 00:16:35,850
This is of great significance.

217
00:16:35,850 --> 00:16:42,850
Because if you buy an ordinary refrigerator and it breaks, you can say, "Hey, the manufacturer,

218
00:16:42,850 --> 00:16:44,019
You should be responsible for this rest. "

219
00:16:44,019 --> 00:16:49,250
If you buy a coffee machine, it will burn your house because the defective unit you

220
00:16:49,250 --> 00:16:55,689
Can go out safely and then you recover the loss from the company and your insurance company

221
00:16:55,689 --> 00:16:57,009
Take care of it.

222
00:16:57,009 --> 00:17:00,990
This is a complete ethical framework around this.

223
00:17:00,990 --> 00:17:03,910
There is a legal structure there as well.

224
00:17:03,910 --> 00:17:07,809
Obviously, self-driving cars will happen safer and they will save lives.

225
00:17:07,809 --> 00:17:09,620
This is a very important thing.

226
00:17:09,620 --> 00:17:10,699
We want to save lives.

227
00:17:10,699 --> 00:17:12,900
We hope the road is better.

228
00:17:12,900 --> 00:17:17,829
But the number of lives saved is not the only term in our moral calculations.

229
00:17:17,829 --> 00:17:21,079
We have to see what happened to people?

230
00:17:21,079 --> 00:17:22,689
How did they take care?

231
00:17:22,689 --> 00:17:27,720
How can they pay for medical care or go back to work or miss work biochemistry

232
00:17:27,720 --> 00:17:34,470
They are recovering, but still able to pay rent and affordable food?

233
00:17:34,470 --> 00:17:41,070
So this question is what we mean as developers?

234
00:17:41,070 --> 00:17:44,970
For example, will this give us a free pass?

235
00:17:44,970 --> 00:17:47,519
We take no responsibility for IoT devices.

236
00:17:47,519 --> 00:17:48,860
Does that mean we can ship it?

237
00:17:48,860 --> 00:17:49,880
We can do anything.

238
00:17:49,880 --> 00:17:57,370
Let's innovate from everything until there is a problem, and there is a precedent,

239
00:17:57,370 --> 00:17:58,370
Correct?

240
00:17:58,370 --> 00:18:00,750
Is this really the legacy we want? Forgotten?

241
00:18:00,750 --> 00:18:05,090
Do we want to leave the legacy behind us because we can and we don't give

242
00:18:05,090 --> 00:18:07,760
Damn who we hurt?

243
00:18:07,760 --> 00:18:12,940
Some companies are doing this.

244
00:18:12,940 --> 00:18:18,330
Some companies are actually working in spaces where they want to innovate,

245
00:18:18,330 --> 00:18:22,850
They just want to build things and then ship things and they will deal with the consequences

246
00:18:22,850 --> 00:18:23,850
later.

247
00:18:23,850 --> 00:18:27,169
But you have to ask yourself: I want to take responsibility for this?

248
00:18:27,169 --> 00:18:28,750
This is the whole point of morality.

249
00:18:28,750 --> 00:18:34,200
Now, I'm talking about one of the most fascinating ethical events in the JavaScript community

250
00:18:34,200 --> 00:18:41,130
In a period of technology, this is the mat event on the left.

251
00:18:41,130 --> 00:18:44,790
I don't know why it's pink, but anyway.

252
00:18:44,790 --> 00:18:49,040
Ashley talked about the left footpad last night and it was really fascinating because she focused

253
00:18:49,040 --> 00:18:51,220
Rally in a lot.

254
00:18:51,220 --> 00:18:53,990
She said that when the left foot pad went online, the Internet exploded.

255
00:18:53,990 --> 00:18:59,889
People are angry about many things. The JavaScript community has developed a small module.

256
00:18:59,889 --> 00:19:04,190
The system may be wrong, or it may be wrong

257
00:19:04,190 --> 00:19:10,120
And friendship is lost or damaged in this matter.

258
00:19:10,120 --> 00:19:17,299
What people don't realize is that all these angry and sharp reasons

259
00:19:17,299 --> 00:19:22,130
Because it â€™s actually the left-hander incident that reveals what a real moral dilemma is, and

260
00:19:22,130 --> 00:19:29,830
We just don't see the trees in the forest at the moment because the left footpad has two competitors

261
00:19:29,830 --> 00:19:33,050
Ethical decision.

262
00:19:33,050 --> 00:19:39,230
The first is that the ethics of hacking culture is the most important thing.

263
00:19:39,230 --> 00:19:46,549
Openness is a virtue, and that ability to control your code is tantamount to existence

264
00:19:46,549 --> 00:19:48,990
A hanger to become an open source developer.

265
00:19:48,990 --> 00:19:52,740
Of course, others can fork, but you have to choose to tout it.

266
00:19:52,740 --> 00:19:57,260
When there are all his modules on the left and broke a bunch of things on the Internet,

267
00:19:57,260 --> 00:20:03,059
MPM has a competitive framework they own who is responsible for who uses them

268
00:20:03,059 --> 00:20:04,059
product.

269
00:20:04,059 --> 00:20:06,659
They have the responsibility of engineers.

270
00:20:06,659 --> 00:20:08,169
They also value openness.

271
00:20:08,169 --> 00:20:13,139
They are an open source community.

272
00:20:13,139 --> 00:20:19,870
So this is a very difficult decision and that is why there are so many heads and ass

273
00:20:19,870 --> 00:20:22,470
Decision made.

274
00:20:22,470 --> 00:20:26,909
Can you imagine what would happen? If this did not happen in 2016

275
00:20:26,909 --> 00:20:28,559
Affect the network?

276
00:20:28,559 --> 00:20:35,409
But on the contrary, I do nâ€™t know. In 2018 and 2020, MPM is running people â€™s cars, people â€™s cars.

277
00:20:35,409 --> 00:20:37,399
refrigerator?

278
00:20:37,399 --> 00:20:42,110
Someone pulled down a module and now, all of a sudden you drive on the highway

279
00:20:42,110 --> 00:20:46,720
At 70 miles per hour, your car shuts down and something goes wrong.

280
00:20:46,720 --> 00:20:49,620
You think this is impossible.

281
00:20:49,620 --> 00:20:55,039
No one will actually do real-time deployments running on cars running IoT devices

282
00:20:55,039 --> 00:20:56,039
field.

283
00:20:56,039 --> 00:20:59,919
Please do the system as we do in production now.

284
00:20:59,919 --> 00:21:02,390
IoT security is a mess.

285
00:21:02,390 --> 00:21:04,750
We are innovating fast.

286
00:21:04,750 --> 00:21:07,030
Of course there will be problems.

287
00:21:07,030 --> 00:21:11,500
And you don't want to be that person responsible for someone's refrigerator

288
00:21:11,500 --> 00:21:14,669
They have lost all their food or maybe they have lost the important medication they need

289
00:21:14,669 --> 00:21:15,669
Refrigeration.

290
00:21:15,669 --> 00:21:19,539
You don't want to take responsibility for it-maybe you do it.

291
00:21:19,539 --> 00:21:23,121
Maybe you think the virtue of openness is more important morality, that is a

292
00:21:23,121 --> 00:21:25,769
Real moral dilemma.

293
00:21:25,769 --> 00:21:28,190
So what do we do as a mockery?

294
00:21:28,190 --> 00:21:32,220
What can we gain from talking about morality?

295
00:21:32,220 --> 00:21:39,370
That's why I don't want to speak this way, instead of allowing you to

296
00:21:39,370 --> 00:21:48,110
This speech, because there are things that we can do as engineers, as developers,

297
00:21:48,110 --> 00:21:51,120
Make our work space better and act with more ethics.

298
00:21:51,120 --> 00:21:54,640
The first is to set expectations for your boss.

299
00:21:54,640 --> 00:22:00,770
If you know what pressure your boss has, if they ask you to do something you do nâ€™t

300
00:22:00,770 --> 00:22:08,010
Feel comfortable, you need to know, i can go to my boss, i feel uncomfortable

301
00:22:08,010 --> 00:22:09,010
Got this?

302
00:22:09,010 --> 00:22:13,059
You can go to your manager and say, "I have concerns about this?"

303
00:22:13,059 --> 00:22:16,120
Do you know what will happen if you do?

304
00:22:16,120 --> 00:22:18,250
This is a very important thing.

305
00:22:18,250 --> 00:22:23,009
You must also be prepared to refuse.

306
00:22:23,009 --> 00:22:31,809
If someone comes to you and says, "Hey, I need you to build this method of sending

307
00:22:31,809 --> 00:22:38,460
Tracking someone's heart rate data to our server in real time, "Do you feel comfortable?

308
00:22:38,460 --> 00:22:39,760
this way?

309
00:22:39,760 --> 00:22:41,180
Maybe you are not.

310
00:22:41,180 --> 00:22:45,149
But do you know how to reject an order?

311
00:22:45,149 --> 00:22:51,049
Would you like to decline the order? If you do, your career will be at risk

312
00:22:51,049 --> 00:22:54,580
Against what you believe?

313
00:22:54,580 --> 00:22:59,139
You also need to be able to have frank discussions with your colleagues about what it means.

314
00:22:59,139 --> 00:23:04,520
I work in the financial world and I'm a data scientist, so we have a lot of data about people,

315
00:23:04,520 --> 00:23:10,799
And a lot of ability to work with that data, so we often talk about my

316
00:23:10,799 --> 00:23:17,640
Team, about what we mean when we record customer data

317
00:23:17,640 --> 00:23:19,710
Record information about its financial situation.

318
00:23:19,710 --> 00:23:23,240
We keep talking about it, as if you weren't happy to do it?

319
00:23:23,240 --> 00:23:25,850
What do we have a legal obligation to do?

320
00:23:25,850 --> 00:23:32,529
Financially, we have a legal obligation to report fraud and view

321
00:23:32,529 --> 00:23:34,549
For example, money laundering.

322
00:23:34,549 --> 00:23:36,490
So we have to talk about these things with each other.

323
00:23:36,490 --> 00:23:41,140
And, as an engineer, you should be able to speak frankly, like your colleagues, "I didn't

324
00:23:41,140 --> 00:23:43,230
like this. "

325
00:23:43,230 --> 00:23:45,440
How do we make sure we don't go there?

326
00:23:45,440 --> 00:23:50,350
How do we ensure that it stays safe and not dangerous?

327
00:23:50,350 --> 00:23:54,809
The most important thing is to know your limits.

328
00:23:54,809 --> 00:23:58,700
Know when you are willing to speak.

329
00:23:58,700 --> 00:24:02,809
Because technology is really profitable and we have a lot of privileges.

330
00:24:02,809 --> 00:24:04,980
We have many privileges in technology.

331
00:24:04,980 --> 00:24:09,910
Even if just looking around the space we are in, this is an extraordinary meeting

332
00:24:09,910 --> 00:24:15,799
Space, and various facilities and various facilities are decadent here

333
00:24:15,799 --> 00:24:18,769
This is not the case in all industries.

334
00:24:18,769 --> 00:24:24,289
What restrictions are you willing to accept? Saying, "I can't have a conscience anymore

335
00:24:24,289 --> 00:24:27,090
Keep doing this? "

336
00:24:27,090 --> 00:24:29,179
Then do something different.

337
00:24:29,179 --> 00:24:33,850
If you don't know what this limit is, then you just won't find out that you are over

338
00:24:33,850 --> 00:24:36,299
Until it's too late.

339
00:24:36,299 --> 00:24:37,299
That's all I have.

340
00:24:37,299 --> 00:24:38,299
thank you very much.

341
00:24:38,299 --> 00:24:39,299
[applause].

342
00:24:39,299 --> 00:24:40,299
[cheer].

343
00:24:40,299 --> 00:24:41,299
>> Thank you, Emily.

344
00:24:41,299 --> 00:24:42,299
It was a wonderful speech.

345
00:24:42,299 --> 00:24:43,299
I think you've touched on many very important things, the gist of our current ethical issue

346
00:24:43,299 --> 00:24:44,299
Should ask.

347
00:24:44,299 --> 00:24:45,299
We will work with the next speaker.

