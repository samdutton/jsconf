1
00:00:06,580 --> 00:00:08,889

I'm Emily and in today's talk we're

2
00:00:08,889 --> 00:00:09,720
going to talk about

3
00:00:09,720 --> 00:00:12,120
is an unconscious bias and how it

4
00:00:12,120 --> 00:00:13,440
affects folks like us and tech

5
00:00:13,440 --> 00:00:15,770
workspaces and what we can do about it

6
00:00:15,770 --> 00:00:20,340
great so just a little bit more about me

7
00:00:20,340 --> 00:00:23,430
my pronouns are she her hers and I'm a

8
00:00:23,430 --> 00:00:25,740
PhD candidate at UC Berkeley

9
00:00:25,740 --> 00:00:27,689
I'm studying chocolate and how humans

10
00:00:27,689 --> 00:00:29,310
can save it so I know very important

11
00:00:29,310 --> 00:00:31,890
work but well my work doesn't have me

12
00:00:31,890 --> 00:00:33,300
playing around in JavaScript all that

13
00:00:33,300 --> 00:00:35,190
much I do do a lot of data science and

14
00:00:35,190 --> 00:00:38,010
RN Python but today my day job doesn't

15
00:00:38,010 --> 00:00:40,649
matter instead we're going to talk about

16
00:00:40,649 --> 00:00:42,300
something that affects everyone in this

17
00:00:42,300 --> 00:00:44,280
room because we are all humans and

18
00:00:44,280 --> 00:00:47,940
that's unconscious bias so who here has

19
00:00:47,940 --> 00:00:49,470
gone to some sort of diversity training

20
00:00:49,470 --> 00:00:51,980
that they felt was a big waste of time

21
00:00:51,980 --> 00:00:55,410
yeah I definitely have and as it turns

22
00:00:55,410 --> 00:00:57,210
out studies have shown that a bad

23
00:00:57,210 --> 00:00:59,610
training can actually have a negative

24
00:00:59,610 --> 00:01:02,280
effect on people so they can come out of

25
00:01:02,280 --> 00:01:04,259
a bad training with more hostile

26
00:01:04,259 --> 00:01:05,939
attitudes towards people who are

27
00:01:05,939 --> 00:01:07,860
different from them and so at the

28
00:01:07,860 --> 00:01:10,259
unconscious bias project where most of

29
00:01:10,259 --> 00:01:12,030
us are either in grad school or have

30
00:01:12,030 --> 00:01:14,220
PhDs in STEM fields we've always been

31
00:01:14,220 --> 00:01:15,600
really careful to only promote

32
00:01:15,600 --> 00:01:18,300
evidence-based approaches we also want

33
00:01:18,300 --> 00:01:20,040
the information we provide to empower

34
00:01:20,040 --> 00:01:22,710
you instead of leading you even more

35
00:01:22,710 --> 00:01:24,240
deflated about the current state of the

36
00:01:24,240 --> 00:01:26,190
world that's why our motto is a hundred

37
00:01:26,190 --> 00:01:29,490
percent oops yeah a hundred percent

38
00:01:29,490 --> 00:01:31,770
empowerment zero percent guilt trip and

39
00:01:31,770 --> 00:01:33,810
today I'm going to teach you all tools

40
00:01:33,810 --> 00:01:35,940
to reduce the effect of unconscious bias

41
00:01:35,940 --> 00:01:38,180
and I want you to leave this conference

42
00:01:38,180 --> 00:01:41,220
as unconscious bias warriors our

43
00:01:41,220 --> 00:01:43,049
ambassadors that are dedicated to making

44
00:01:43,049 --> 00:01:45,479
a positive impact in your circle so that

45
00:01:45,479 --> 00:01:47,220
tech becomes more and more diverse

46
00:01:47,220 --> 00:01:50,729
inclusive and equitable so I'll be going

47
00:01:50,729 --> 00:01:52,619
over a lot of tools and resources today

48
00:01:52,619 --> 00:01:55,020
in my talk but don't worry if you miss

49
00:01:55,020 --> 00:01:57,180
something you'll be able to access all

50
00:01:57,180 --> 00:01:58,770
of the peer reviewed literature and all

51
00:01:58,770 --> 00:02:01,140
of the other resources I mentioned on

52
00:02:01,140 --> 00:02:04,200
our website so finally before we jump

53
00:02:04,200 --> 00:02:05,939
into the good stuff I want to mention

54
00:02:05,939 --> 00:02:07,950
that I'll be touching on issues that are

55
00:02:07,950 --> 00:02:10,739
deeply personal and some of the examples

56
00:02:10,739 --> 00:02:12,330
that I may present could be triggering

57
00:02:12,330 --> 00:02:14,280
to some of you in the audience but I

58
00:02:14,280 --> 00:02:15,900
present them in the hopes of having a

59
00:02:15,900 --> 00:02:18,209
frank conversation about why they are

60
00:02:18,209 --> 00:02:21,030
harmful and what we can take what steps

61
00:02:21,030 --> 00:02:23,520
we can take as individuals and by Stan

62
00:02:23,520 --> 00:02:25,700
to make the world a better place so

63
00:02:25,700 --> 00:02:27,720
unfortunately I won't really have any

64
00:02:27,720 --> 00:02:29,790
time for questions from the audience up

65
00:02:29,790 --> 00:02:31,500
here on stage but I encourage you to

66
00:02:31,500 --> 00:02:32,790
come find me if you have questions

67
00:02:32,790 --> 00:02:34,170
concerns or comments about the

68
00:02:34,170 --> 00:02:36,330
presentation and so with that let's

69
00:02:36,330 --> 00:02:39,510
start with a definition so we need to be

70
00:02:39,510 --> 00:02:41,490
on the same page about what bias is and

71
00:02:41,490 --> 00:02:43,950
biases prejudice in favor of or against

72
00:02:43,950 --> 00:02:46,680
one group person or thing compared with

73
00:02:46,680 --> 00:02:49,410
another and usually a way considered to

74
00:02:49,410 --> 00:02:51,960
be unfair so unconscious or implicit

75
00:02:51,960 --> 00:02:54,180
bias is bias that you have even though

76
00:02:54,180 --> 00:02:55,920
you don't consciously agree with it or

77
00:02:55,920 --> 00:02:59,310
even know that it's there and most

78
00:02:59,310 --> 00:03:01,290
people don't want to be racist or sexist

79
00:03:01,290 --> 00:03:03,240
but a lot of us are unconsciously biased

80
00:03:03,240 --> 00:03:05,190
against certain groups anyway and this

81
00:03:05,190 --> 00:03:06,780
starts with negative stereotypes from

82
00:03:06,780 --> 00:03:08,940
the world around us what we see in TV

83
00:03:08,940 --> 00:03:11,820
shows in movies here on the radio read

84
00:03:11,820 --> 00:03:13,860
in books see on the internet and even

85
00:03:13,860 --> 00:03:15,690
pick up from our friends and family and

86
00:03:15,690 --> 00:03:18,030
so even if we don't unconsciously agree

87
00:03:18,030 --> 00:03:19,950
with these negative stereotypes they can

88
00:03:19,950 --> 00:03:21,330
still worm their way into our

89
00:03:21,330 --> 00:03:23,490
unconscious and influence how we think

90
00:03:23,490 --> 00:03:25,950
and act so I'd like to call attention to

91
00:03:25,950 --> 00:03:28,140
a few examples of negative stereotypes

92
00:03:28,140 --> 00:03:30,270
from American culture to show you what

93
00:03:30,270 --> 00:03:32,790
I'm talking about men are seen as

94
00:03:32,790 --> 00:03:34,680
breadwinners and women as caregivers

95
00:03:34,680 --> 00:03:36,959
women can be beautiful or intelligent

96
00:03:36,959 --> 00:03:39,390
but not both African Americans are

97
00:03:39,390 --> 00:03:41,820
commonly portrayed as loud and biased

98
00:03:41,820 --> 00:03:43,709
can even come from simple associations

99
00:03:43,709 --> 00:03:45,930
of patterns that we notice unconsciously

100
00:03:45,930 --> 00:03:48,209
like that custodians are often racial

101
00:03:48,209 --> 00:03:51,360
minorities and before stereotypes even

102
00:03:51,360 --> 00:03:53,580
lead to bias stereotypes themselves are

103
00:03:53,580 --> 00:03:55,650
damaging being stereotyped has been

104
00:03:55,650 --> 00:03:57,330
found to lead to short-term aggression

105
00:03:57,330 --> 00:03:59,760
inability to focus which results in

106
00:03:59,760 --> 00:04:01,920
decrease performance and overeating

107
00:04:01,920 --> 00:04:04,350
people who regularly encounter the

108
00:04:04,350 --> 00:04:06,930
threat of being judged by a negative

109
00:04:06,930 --> 00:04:08,820
stereotype are more likely to have

110
00:04:08,820 --> 00:04:11,850
hypertension be depressed and to rate

111
00:04:11,850 --> 00:04:14,070
their own health more poorly and being

112
00:04:14,070 --> 00:04:16,020
on the receiving end of positive bias or

113
00:04:16,020 --> 00:04:18,359
positive stereotypes hurts too implying

114
00:04:18,359 --> 00:04:20,100
someone is good at math because they are

115
00:04:20,100 --> 00:04:22,049
Asian might make the receiver of the

116
00:04:22,049 --> 00:04:23,940
comment dislike the speaker and invoke

117
00:04:23,940 --> 00:04:26,910
feelings of anger but when stereotypes

118
00:04:26,910 --> 00:04:29,250
lead to unconscious bias about who can

119
00:04:29,250 --> 00:04:31,470
thrive in science and tech this can be

120
00:04:31,470 --> 00:04:33,780
even this can have even more damaging

121
00:04:33,780 --> 00:04:36,630
effects by creating barriers to entering

122
00:04:36,630 --> 00:04:38,340
STEM fields and this is the so-called

123
00:04:38,340 --> 00:04:41,310
pipeline problem and folks start being

124
00:04:41,310 --> 00:04:43,260
affected by this bias at a very early

125
00:04:43,260 --> 00:04:45,600
age and it happens around the world so

126
00:04:45,600 --> 00:04:47,430
this paper is actually from Germany and

127
00:04:47,430 --> 00:04:49,890
I'll be presenting a few more papers and

128
00:04:49,890 --> 00:04:52,020
then in this section here the

129
00:04:52,020 --> 00:04:53,490
researchers showed that teachers have

130
00:04:53,490 --> 00:04:54,720
both conscious and unconscious

131
00:04:54,720 --> 00:04:56,850
stereotypes about boys being better at

132
00:04:56,850 --> 00:04:58,770
math and girls being better at languages

133
00:04:58,770 --> 00:05:00,810
and those stereotypes influence whether

134
00:05:00,810 --> 00:05:02,520
the teachers recommend boys and girls

135
00:05:02,520 --> 00:05:04,500
for science and technology schools

136
00:05:04,500 --> 00:05:06,720
versus language oriented schools and

137
00:05:06,720 --> 00:05:08,940
this bias continues from grade school

138
00:05:08,940 --> 00:05:12,510
through graduate school but the pipeline

139
00:05:12,510 --> 00:05:14,360
is only the beginning of the problem

140
00:05:14,360 --> 00:05:16,770
once marginalized people get their foot

141
00:05:16,770 --> 00:05:18,660
in the door unconscious bias leads to

142
00:05:18,660 --> 00:05:22,530
barriers to rising in stem so study

143
00:05:22,530 --> 00:05:24,480
after study has found evidence of bias

144
00:05:24,480 --> 00:05:26,460
against almost any marginalized group

145
00:05:26,460 --> 00:05:28,170
you can think of in terms of who gets

146
00:05:28,170 --> 00:05:29,970
called back for an interview and as we

147
00:05:29,970 --> 00:05:32,700
all know jobs are pretty important and

148
00:05:32,700 --> 00:05:34,500
this type of bias occurs even when that

149
00:05:34,500 --> 00:05:36,720
discrimination is illegal in that area

150
00:05:36,720 --> 00:05:38,700
so in the United States there's bias

151
00:05:38,700 --> 00:05:41,280
against openly gay men and bias against

152
00:05:41,280 --> 00:05:43,230
people with disabilities even with the

153
00:05:43,230 --> 00:05:45,180
disability would not have affected the

154
00:05:45,180 --> 00:05:48,240
person's ability to perform the job in

155
00:05:48,240 --> 00:05:50,670
the United States and China being a

156
00:05:50,670 --> 00:05:52,860
parent hurt job prospects for women but

157
00:05:52,860 --> 00:05:54,420
it was actually the opposite for men

158
00:05:54,420 --> 00:05:56,820
being a father helped job prospects and

159
00:05:56,820 --> 00:05:59,250
one last example bias was found against

160
00:05:59,250 --> 00:06:01,440
people with Arabic sounding names upon

161
00:06:01,440 --> 00:06:04,140
both among both American and Dutch

162
00:06:04,140 --> 00:06:06,750
reviewers so imagine experiencing these

163
00:06:06,750 --> 00:06:09,090
biases every time you apply to a job or

164
00:06:09,090 --> 00:06:10,920
when you want to move up within your own

165
00:06:10,920 --> 00:06:12,950
company or switch to a different team

166
00:06:12,950 --> 00:06:16,020
and imagine how much harder that might

167
00:06:16,020 --> 00:06:18,150
be or how much worse that bias might be

168
00:06:18,150 --> 00:06:20,400
if you're an Arabic lesbian mother in a

169
00:06:20,400 --> 00:06:22,890
wheelchair and studies have actually

170
00:06:22,890 --> 00:06:24,660
looked specifically at this type of

171
00:06:24,660 --> 00:06:27,180
intersection for instance between gender

172
00:06:27,180 --> 00:06:29,220
and race so women and racial minorities

173
00:06:29,220 --> 00:06:31,470
are both less likely to be given the

174
00:06:31,470 --> 00:06:33,690
benefit of the doubt and the problem is

175
00:06:33,690 --> 00:06:35,640
only worse if you're both a woman and a

176
00:06:35,640 --> 00:06:37,980
racial minority so when women of color

177
00:06:37,980 --> 00:06:39,810
demonstrate leadership and competence

178
00:06:39,810 --> 00:06:41,580
there are certain is's perceived

179
00:06:41,580 --> 00:06:43,290
negatively and the data shows that they

180
00:06:43,290 --> 00:06:46,200
get pushed back for it so even when

181
00:06:46,200 --> 00:06:48,150
you're successful unsupportive work

182
00:06:48,150 --> 00:06:49,200
environments are

183
00:06:49,200 --> 00:06:51,600
reason for leading stem in the next few

184
00:06:51,600 --> 00:06:54,330
slides I'll be focusing on women but

185
00:06:54,330 --> 00:06:55,980
these trends are representative of most

186
00:06:55,980 --> 00:07:00,420
underrepresented groups in tech so let

187
00:07:00,420 --> 00:07:02,490
me be frank women do leave stem at

188
00:07:02,490 --> 00:07:04,050
greater rates than they leave the non

189
00:07:04,050 --> 00:07:06,390
stem workforce the orange line that you

190
00:07:06,390 --> 00:07:07,980
see on this screen is women being

191
00:07:07,980 --> 00:07:09,600
retained in non STEM fields

192
00:07:09,600 --> 00:07:12,420
now let's look at stem as you can see

193
00:07:12,420 --> 00:07:14,760
the retention is much much worse and

194
00:07:14,760 --> 00:07:17,520
less than 20% of the women who leave

195
00:07:17,520 --> 00:07:19,200
STEM fields are also leaving the

196
00:07:19,200 --> 00:07:21,540
workforce for instance like taking care

197
00:07:21,540 --> 00:07:24,750
of family more than 80% of the women

198
00:07:24,750 --> 00:07:26,820
leaving the stem workforce are just

199
00:07:26,820 --> 00:07:28,590
getting jobs in other fields they're

200
00:07:28,590 --> 00:07:30,960
going to non STEM fields

201
00:07:30,960 --> 00:07:32,790
so let's look at a different control and

202
00:07:32,790 --> 00:07:35,490
focus in on just the tech industry when

203
00:07:35,490 --> 00:07:38,130
you compare the attrition rates of of

204
00:07:38,130 --> 00:07:40,260
women to men in tech it's more than

205
00:07:40,260 --> 00:07:43,110
twice as high 41% of women leave tech

206
00:07:43,110 --> 00:07:47,880
versus just 17% of men and on top of the

207
00:07:47,880 --> 00:07:49,860
well-documented pay gap that exists

208
00:07:49,860 --> 00:07:51,240
there are many cultural cues that a

209
00:07:51,240 --> 00:07:53,460
company can give to signal that an

210
00:07:53,460 --> 00:07:55,710
employee is not valued which in turn

211
00:07:55,710 --> 00:07:58,470
encourages that employee to leave one is

212
00:07:58,470 --> 00:08:00,870
feedback in performance reviews in this

213
00:08:00,870 --> 00:08:02,670
study the majority of women received

214
00:08:02,670 --> 00:08:04,410
reviews with a negative unhelpful

215
00:08:04,410 --> 00:08:06,450
feedback while the majority of men

216
00:08:06,450 --> 00:08:08,430
received only constructive feedback and

217
00:08:08,430 --> 00:08:10,410
the negative feedback for women was

218
00:08:10,410 --> 00:08:12,000
about personality in ways that reflect

219
00:08:12,000 --> 00:08:14,610
common unconscious biases like telling

220
00:08:14,610 --> 00:08:17,570
women to pipe down or be less abrasive

221
00:08:17,570 --> 00:08:20,400
so together these barriers keep some

222
00:08:20,400 --> 00:08:22,590
groups underrepresented in stem and tech

223
00:08:22,590 --> 00:08:24,770
and reinforce pre-existing stereotypes

224
00:08:24,770 --> 00:08:27,150
creating what we've dubbed the positive

225
00:08:27,150 --> 00:08:29,160
feedback loop of negative stereotypes

226
00:08:29,160 --> 00:08:32,220
and there's so so much more

227
00:08:32,220 --> 00:08:33,750
peer-reviewed literature out there that

228
00:08:33,750 --> 00:08:35,640
shows that unconscious bias is real and

229
00:08:35,640 --> 00:08:38,490
it is impacting diversity if you'd like

230
00:08:38,490 --> 00:08:40,800
to explore more please head over to our

231
00:08:40,800 --> 00:08:42,240
website where you can find all of the

232
00:08:42,240 --> 00:08:43,800
papers that I mentioned in the stock as

233
00:08:43,800 --> 00:08:46,800
well as hundreds others about almost

234
00:08:46,800 --> 00:08:49,260
every aspect of bias its effect and what

235
00:08:49,260 --> 00:08:52,680
we can do about it ok so let's take a

236
00:08:52,680 --> 00:08:55,350
breath because that was a lot of bad

237
00:08:55,350 --> 00:08:57,300
news and I know some of you might be

238
00:08:57,300 --> 00:08:58,890
feeling a little low at this point in

239
00:08:58,890 --> 00:09:01,020
the talk but the good news is that we're

240
00:09:01,020 --> 00:09:02,390
only about half way through

241
00:09:02,390 --> 00:09:03,890
that means we have a lot of time to talk

242
00:09:03,890 --> 00:09:06,020
about solutions so even though

243
00:09:06,020 --> 00:09:07,460
stereotypes come from our environment

244
00:09:07,460 --> 00:09:10,310
that doesn't make us powerless happily

245
00:09:10,310 --> 00:09:11,840
there's enough studies out there that we

246
00:09:11,840 --> 00:09:13,760
know how to reduce negative effects of

247
00:09:13,760 --> 00:09:16,600
bias and reliably reduce our biases too

248
00:09:16,600 --> 00:09:19,070
so thoughts are actually our first line

249
00:09:19,070 --> 00:09:21,050
of defense when confronting bias so I'm

250
00:09:21,050 --> 00:09:22,880
going to teach you two techniques that

251
00:09:22,880 --> 00:09:24,200
can help you decrease your own

252
00:09:24,200 --> 00:09:26,600
unconscious bias but before we start

253
00:09:26,600 --> 00:09:28,160
training we need to cover some

254
00:09:28,160 --> 00:09:30,680
prerequisites in order for these

255
00:09:30,680 --> 00:09:33,590
techniques to be effective for you you

256
00:09:33,590 --> 00:09:35,450
need to be motivated to overcome your

257
00:09:35,450 --> 00:09:37,670
unconscious bias hopefully the positive

258
00:09:37,670 --> 00:09:39,860
loop of negative stereo bat stereotypes

259
00:09:39,860 --> 00:09:43,070
helps to give you that motivation you

260
00:09:43,070 --> 00:09:44,810
then need to take steps to become aware

261
00:09:44,810 --> 00:09:46,850
of your bias and why it exists and we'll

262
00:09:46,850 --> 00:09:48,380
explore how to recognize your own bias

263
00:09:48,380 --> 00:09:49,760
and the first technique that I teach you

264
00:09:49,760 --> 00:09:51,710
but you can also do it through more

265
00:09:51,710 --> 00:09:54,170
technical ways by taking an implicit

266
00:09:54,170 --> 00:09:56,450
association test at harvard's project

267
00:09:56,450 --> 00:09:59,330
implicit website you'll need to learn

268
00:09:59,330 --> 00:10:01,880
the to detect the subtle influence of

269
00:10:01,880 --> 00:10:03,440
bias and your everyday surroundings

270
00:10:03,440 --> 00:10:05,270
which helps you know when to act and

271
00:10:05,270 --> 00:10:06,860
then you have to practice these

272
00:10:06,860 --> 00:10:09,620
strategies to reduce your bias that you

273
00:10:09,620 --> 00:10:11,270
know what to do in that moment when you

274
00:10:11,270 --> 00:10:13,430
recognize it and yes you do have to

275
00:10:13,430 --> 00:10:15,350
practice without practice there will be

276
00:10:15,350 --> 00:10:16,070
no change

277
00:10:16,070 --> 00:10:18,290
okay so let's learn our first strategy I

278
00:10:18,290 --> 00:10:20,900
want you to take a look at this picture

279
00:10:20,900 --> 00:10:22,550
and think to yourself what is your first

280
00:10:22,550 --> 00:10:24,500
reaction to it if I asked you to

281
00:10:24,500 --> 00:10:26,150
describe who these people are and what

282
00:10:26,150 --> 00:10:27,230
they were doing what would you think

283
00:10:27,230 --> 00:10:30,140
what would you say did you assume that

284
00:10:30,140 --> 00:10:32,030
each of these people are engineers or

285
00:10:32,030 --> 00:10:34,550
designers beginners or experienced

286
00:10:34,550 --> 00:10:37,310
straight or queer disabled or not

287
00:10:37,310 --> 00:10:40,220
communicating well or poorly when I

288
00:10:40,220 --> 00:10:42,470
first saw this picture I thought young

289
00:10:42,470 --> 00:10:44,930
and experienced interns and that's my

290
00:10:44,930 --> 00:10:45,500
bias

291
00:10:45,500 --> 00:10:47,570
so once you've acknowledged that your

292
00:10:47,570 --> 00:10:49,700
own biases are informing the way you see

293
00:10:49,700 --> 00:10:51,470
people you can choose to replace any

294
00:10:51,470 --> 00:10:53,710
biased aspect of your thoughts or your

295
00:10:53,710 --> 00:10:55,700
reaction with a different less

296
00:10:55,700 --> 00:10:59,690
stereotypical scenario and this is what

297
00:10:59,690 --> 00:11:02,150
we call breaking down the stereotype the

298
00:11:02,150 --> 00:11:03,680
idea here is to consciously recognize

299
00:11:03,680 --> 00:11:06,470
that I thought that you had came from a

300
00:11:06,470 --> 00:11:08,270
stereotype and instead of leaving that

301
00:11:08,270 --> 00:11:10,730
bias buried in your subconscious bring

302
00:11:10,730 --> 00:11:12,840
it to the surface and then replace it

303
00:11:12,840 --> 00:11:14,430
so in this case I can choose to

304
00:11:14,430 --> 00:11:16,860
recognize my biased reaction and think

305
00:11:16,860 --> 00:11:18,870
CEOs entrepreneurs

306
00:11:18,870 --> 00:11:22,320
leaders experienced okay so let's move

307
00:11:22,320 --> 00:11:24,180
on to our second strategy which is

308
00:11:24,180 --> 00:11:26,490
increasing opportunities for contact and

309
00:11:26,490 --> 00:11:28,500
it is probably one of the most important

310
00:11:28,500 --> 00:11:31,290
strategies that you'll find because it

311
00:11:31,290 --> 00:11:33,089
tends to help with all aspects of

312
00:11:33,089 --> 00:11:35,310
breaking down our biases it can make us

313
00:11:35,310 --> 00:11:37,020
confront our basic assumptions

314
00:11:37,020 --> 00:11:39,000
it helps us empathize with individuals

315
00:11:39,000 --> 00:11:40,710
who are different from us and it can

316
00:11:40,710 --> 00:11:43,320
give us non-stereotypical x' that we can

317
00:11:43,320 --> 00:11:45,779
continuously use to fight our own bias

318
00:11:45,779 --> 00:11:49,410
and there's many ways to do this even if

319
00:11:49,410 --> 00:11:51,720
you aren't a social butterfly so you can

320
00:11:51,720 --> 00:11:53,370
try going to a diverse conference like

321
00:11:53,370 --> 00:11:55,980
this one you could follow folks like the

322
00:11:55,980 --> 00:11:57,690
other amazing speakers that you've heard

323
00:11:57,690 --> 00:12:00,330
on Twitter you can contribute to an open

324
00:12:00,330 --> 00:12:01,980
source project with diverse maintained

325
00:12:01,980 --> 00:12:03,900
errs or you could mentor someone new or

326
00:12:03,900 --> 00:12:06,120
find a mentor for yourself whatever

327
00:12:06,120 --> 00:12:07,830
feels like a good step for you that's

328
00:12:07,830 --> 00:12:09,839
enough and I'm not going to lie

329
00:12:09,839 --> 00:12:12,270
increasing opportunities for contact can

330
00:12:12,270 --> 00:12:14,760
be scary sometimes too so if you

331
00:12:14,760 --> 00:12:16,500
struggle with this like I tend to do

332
00:12:16,500 --> 00:12:18,750
it's always a good move to listen and be

333
00:12:18,750 --> 00:12:21,360
genuinely curious Twitter and blogs can

334
00:12:21,360 --> 00:12:23,220
be an awesome start because you can

335
00:12:23,220 --> 00:12:24,960
follow along and get a sense of what

336
00:12:24,960 --> 00:12:26,460
someone might or might not want to talk

337
00:12:26,460 --> 00:12:29,400
about it also helps to be open to

338
00:12:29,400 --> 00:12:31,500
hearing things you didn't expect reading

339
00:12:31,500 --> 00:12:33,810
body language cues and believing what

340
00:12:33,810 --> 00:12:36,089
people say about their experiences even

341
00:12:36,089 --> 00:12:38,490
if it seems contradictory to what -

342
00:12:38,490 --> 00:12:40,080
something that you thought you knew and

343
00:12:40,080 --> 00:12:42,720
if someone gives you some hard feedback

344
00:12:42,720 --> 00:12:44,700
or it just doesn't have time to talk

345
00:12:44,700 --> 00:12:46,920
try and stay humble other people are the

346
00:12:46,920 --> 00:12:49,830
main character in their own story so as

347
00:12:49,830 --> 00:12:50,550
a group

348
00:12:50,550 --> 00:12:52,589
let's send an intention to use the rest

349
00:12:52,589 --> 00:12:54,000
of this conference to meet new people

350
00:12:54,000 --> 00:12:56,100
and help increase our opportunities for

351
00:12:56,100 --> 00:12:57,720
contact with groups that are

352
00:12:57,720 --> 00:12:59,880
underrepresented in our workplaces and

353
00:12:59,880 --> 00:13:01,800
let's try and maintain those connections

354
00:13:01,800 --> 00:13:04,350
after this conference even if it is just

355
00:13:04,350 --> 00:13:07,709
following a new person on Twitter so to

356
00:13:07,709 --> 00:13:09,720
wrap up this section unconsciously

357
00:13:09,720 --> 00:13:11,610
biased thoughts are a habit and like any

358
00:13:11,610 --> 00:13:15,060
habit they can be broken mostly you can

359
00:13:15,060 --> 00:13:16,860
never be quite cured of unconscious bias

360
00:13:16,860 --> 00:13:18,900
but over time the strategies to reduce

361
00:13:18,900 --> 00:13:21,420
bias become habits themselves and it

362
00:13:21,420 --> 00:13:23,100
will feel easier to quash unconscious

363
00:13:23,100 --> 00:13:23,870
bias when it

364
00:13:23,870 --> 00:13:26,480
sup again so let's shift away from our

365
00:13:26,480 --> 00:13:28,220
internal struggles and dive into how we

366
00:13:28,220 --> 00:13:30,170
can respond to bias in our environments

367
00:13:30,170 --> 00:13:31,790
we've been working with an amazing

368
00:13:31,790 --> 00:13:34,760
cartoonist - RIA Theresa Oh born to

369
00:13:34,760 --> 00:13:36,529
illustrate real scenarios that come up

370
00:13:36,529 --> 00:13:38,870
and how we can react when they do so we

371
00:13:38,870 --> 00:13:40,400
have cartoon showing bias against white

372
00:13:40,400 --> 00:13:42,350
women people of color bias against

373
00:13:42,350 --> 00:13:44,120
people with disabilities against queer

374
00:13:44,120 --> 00:13:46,640
folks and so on but for this talk I

375
00:13:46,640 --> 00:13:48,380
picked just a couple of cartoons that I

376
00:13:48,380 --> 00:13:51,140
hope are less emotionally charged than

377
00:13:51,140 --> 00:13:53,450
others but still useful for practicing

378
00:13:53,450 --> 00:13:56,270
these techniques so who's heard this one

379
00:13:56,270 --> 00:13:58,850
before she's really smart but I wish she

380
00:13:58,850 --> 00:14:02,930
wasn't so bossy so I have and I may have

381
00:14:02,930 --> 00:14:04,880
thought it one or two times to remember

382
00:14:04,880 --> 00:14:06,980
that we are all biased and we can even

383
00:14:06,980 --> 00:14:10,130
be biased against our own group so let's

384
00:14:10,130 --> 00:14:12,320
identify this bias this cartoon

385
00:14:12,320 --> 00:14:14,390
showcases a common bias against women in

386
00:14:14,390 --> 00:14:16,190
leadership roles so what are your

387
00:14:16,190 --> 00:14:18,250
options when someone says this to you

388
00:14:18,250 --> 00:14:20,690
instead of labeling the person who made

389
00:14:20,690 --> 00:14:23,360
the comment as racist or sexist inciting

390
00:14:23,360 --> 00:14:24,800
a reflection on the comment through four

391
00:14:24,800 --> 00:14:26,839
simple strategies will help to highlight

392
00:14:26,839 --> 00:14:28,940
the problematic nature of it and reflect

393
00:14:28,940 --> 00:14:31,430
and encourage a change in behavior

394
00:14:31,430 --> 00:14:34,339
rather than an argument so our first

395
00:14:34,339 --> 00:14:36,140
strategy is just to observe the problem

396
00:14:36,140 --> 00:14:38,510
instead of saying staying silent or

397
00:14:38,510 --> 00:14:39,380
moving on

398
00:14:39,380 --> 00:14:40,640
observe that the comment was

399
00:14:40,640 --> 00:14:42,770
stereotypical and hurtful something like

400
00:14:42,770 --> 00:14:46,190
ouch that's a bit harsh or you could

401
00:14:46,190 --> 00:14:47,779
counter a microaggression with a micro

402
00:14:47,779 --> 00:14:50,540
affection she's not bossy she's a leader

403
00:14:50,540 --> 00:14:52,790
a positive comment like this can help

404
00:14:52,790 --> 00:14:54,529
weaken the negative stereotype for those

405
00:14:54,529 --> 00:14:56,510
around you and without being too

406
00:14:56,510 --> 00:14:58,070
confrontational or derail in the

407
00:14:58,070 --> 00:15:00,800
conversation our bystander could also

408
00:15:00,800 --> 00:15:03,110
reflect the comment back which would

409
00:15:03,110 --> 00:15:04,490
make the speaker do the work of

410
00:15:04,490 --> 00:15:05,839
challenging their own assumptions

411
00:15:05,839 --> 00:15:08,150
something like how come you never called

412
00:15:08,150 --> 00:15:11,330
guys bossy and finally he could simply

413
00:15:11,330 --> 00:15:13,700
label the stereotype that's in play hey

414
00:15:13,700 --> 00:15:15,860
I think that's reinforcing a hurtful

415
00:15:15,860 --> 00:15:18,500
gender stereotype or as a friend said to

416
00:15:18,500 --> 00:15:20,510
me when I was practice talk a few days

417
00:15:20,510 --> 00:15:23,500
ago bias much bro

418
00:15:23,500 --> 00:15:26,510
okay so let's practice in this cartoon a

419
00:15:26,510 --> 00:15:28,130
manager is just finishing up a meeting

420
00:15:28,130 --> 00:15:30,080
and asking one of his newer team members

421
00:15:30,080 --> 00:15:32,360
does your wife take care care of the

422
00:15:32,360 --> 00:15:35,150
kids or do you do daycare sorry let me

423
00:15:35,150 --> 00:15:36,920
repeat that does your wife take care of

424
00:15:36,920 --> 00:15:37,410
the kids

425
00:15:37,410 --> 00:15:40,350
or do you do daycare as an opportunity

426
00:15:40,350 --> 00:15:42,779
to meet people right now I would like

427
00:15:42,779 --> 00:15:44,430
you to form groups of two or three make

428
00:15:44,430 --> 00:15:47,129
sure no one is left out and identify the

429
00:15:47,129 --> 00:15:49,470
stereotype or types in this cartoon and

430
00:15:49,470 --> 00:15:51,600
then brainstorm a few responses of your

431
00:15:51,600 --> 00:15:53,910
own I'll give you a little bit more than

432
00:15:53,910 --> 00:15:55,230
a minute to do that and I'm actually

433
00:15:55,230 --> 00:15:58,800
going to ask visnu to come up and play

434
00:15:58,800 --> 00:16:00,930
some lilting ukulele tunes to keep you

435
00:16:00,930 --> 00:16:03,810
call while you are brainstorming and

436
00:16:03,810 --> 00:16:06,870
talking so when you hear his music stop

437
00:16:06,870 --> 00:16:09,209
please turn your attention back up on

438
00:16:09,209 --> 00:16:10,720
stage to me

439
00:16:10,720 --> 00:16:20,670
[Music]

440
00:16:20,680 --> 00:16:54,090


441
00:16:54,100 --> 00:18:12,060
[Music]

442
00:18:12,070 --> 00:18:16,700
great wow that was a lot of talking and

443
00:18:16,700 --> 00:18:18,980
I'm so glad that all of you have so much

444
00:18:18,980 --> 00:18:23,510
to say about this so let's review what

445
00:18:23,510 --> 00:18:26,690
we just went over and had great ideas

446
00:18:26,690 --> 00:18:30,590
about so there are actually two biases

447
00:18:30,590 --> 00:18:32,390
in this situation and I hope some of you

448
00:18:32,390 --> 00:18:34,250
picked up on that so the first is that

449
00:18:34,250 --> 00:18:35,780
dads don't play an active role in

450
00:18:35,780 --> 00:18:38,090
parenting and the second is that dads

451
00:18:38,090 --> 00:18:41,470
must be in a heterosexual relationship

452
00:18:41,470 --> 00:18:46,270
yeah

453
00:18:46,280 --> 00:18:50,420
i I couldn't Oh monogamy

454
00:18:50,420 --> 00:18:52,820
yeah you know could be in a polyamorous

455
00:18:52,820 --> 00:18:55,700
relationship too okay so let's review

456
00:18:55,700 --> 00:18:57,680
some of the possible responses that our

457
00:18:57,680 --> 00:19:00,350
dad could use in this situation the team

458
00:19:00,350 --> 00:19:01,940
member could observe the problem by

459
00:19:01,940 --> 00:19:03,920
saying actually my husband and I share

460
00:19:03,920 --> 00:19:06,470
parenting responsibilities equally he

461
00:19:06,470 --> 00:19:08,270
could also use positive redirection I

462
00:19:08,270 --> 00:19:09,680
take pride in being the primary

463
00:19:09,680 --> 00:19:12,620
caretaker of my children he could also

464
00:19:12,620 --> 00:19:13,760
transfer the work

465
00:19:13,760 --> 00:19:16,670
why are those the only options and

466
00:19:16,670 --> 00:19:18,980
finally he could label the stereotype as

467
00:19:18,980 --> 00:19:21,230
a gay father who manages our family's

468
00:19:21,230 --> 00:19:23,330
childcare my situation is only unusual

469
00:19:23,330 --> 00:19:25,460
for those who hold unfair gender

470
00:19:25,460 --> 00:19:31,880
binaries so that one might be a little

471
00:19:31,880 --> 00:19:33,650
wordy but I'm sure you can come up with

472
00:19:33,650 --> 00:19:35,390
some that you would use in that

473
00:19:35,390 --> 00:19:38,390
situation too so we want all of you to

474
00:19:38,390 --> 00:19:40,430
feel represented by our cartoons so feel

475
00:19:40,430 --> 00:19:42,560
free to share your own real experiences

476
00:19:42,560 --> 00:19:44,870
of bias with us through our website and

477
00:19:44,870 --> 00:19:46,550
maybe they will be made into a cartoon

478
00:19:46,550 --> 00:19:48,380
to teach others to be more empathetic

479
00:19:48,380 --> 00:19:52,010
humans and to born the great is actually

480
00:19:52,010 --> 00:19:54,460
the cartoonists Instagram and Twitter

481
00:19:54,460 --> 00:19:57,050
handle so if you want to tweet at her

482
00:19:57,050 --> 00:20:00,410
feel free so in parallel with personal

483
00:20:00,410 --> 00:20:02,060
adjustments that you can make to your

484
00:20:02,060 --> 00:20:03,140
words and thoughts there's also

485
00:20:03,140 --> 00:20:04,880
important work that can be done at an

486
00:20:04,880 --> 00:20:06,740
organizational level through putting

487
00:20:06,740 --> 00:20:09,050
good policies in place now I bet some of

488
00:20:09,050 --> 00:20:10,990
you might be resisting this idea of it

489
00:20:10,990 --> 00:20:13,990
after all Tech is a meritocracy right

490
00:20:13,990 --> 00:20:17,600
well nope exactly the opposite the fact

491
00:20:17,600 --> 00:20:19,490
that the tech sector celebrates the idea

492
00:20:19,490 --> 00:20:21,410
of innate talent is actually evidence

493
00:20:21,410 --> 00:20:23,930
that tech is not a meritocracy this

494
00:20:23,930 --> 00:20:25,370
study shows that when you focus on just

495
00:20:25,370 --> 00:20:27,980
coding or engineering instead of

496
00:20:27,980 --> 00:20:29,900
including holistic values like teamwork

497
00:20:29,900 --> 00:20:31,880
transparency and consistency in

498
00:20:31,880 --> 00:20:34,250
performance reviews then manager level

499
00:20:34,250 --> 00:20:36,200
folks with unconscious bias are more

500
00:20:36,200 --> 00:20:37,940
likely to review their team members with

501
00:20:37,940 --> 00:20:40,340
that bias regardless of how awesome a

502
00:20:40,340 --> 00:20:43,700
person's code is so you can think of

503
00:20:43,700 --> 00:20:46,130
good policies as the bumpers in bowling

504
00:20:46,130 --> 00:20:47,720
without them people with more

505
00:20:47,720 --> 00:20:49,220
unconscious bias will throw more gutter

506
00:20:49,220 --> 00:20:51,620
balls by unintentionally acting with

507
00:20:51,620 --> 00:20:53,870
their biases on the other hand good

508
00:20:53,870 --> 00:20:55,850
policies can guide people's behavior in

509
00:20:55,850 --> 00:20:57,710
the direction that you want it to go if

510
00:20:57,710 --> 00:20:59,179
you consciously ask them

511
00:20:59,179 --> 00:21:01,219
towards an environment that includes and

512
00:21:01,219 --> 00:21:04,460
recognizes everyone's unique talent so

513
00:21:04,460 --> 00:21:06,080
many conversations about policies

514
00:21:06,080 --> 00:21:08,149
usually start with hiring how to get

515
00:21:08,149 --> 00:21:09,619
more diverse candidates into the

516
00:21:09,619 --> 00:21:11,419
recruiting pipeline how to get more

517
00:21:11,419 --> 00:21:13,219
through the interview process and how to

518
00:21:13,219 --> 00:21:15,320
have more have more of them accept

519
00:21:15,320 --> 00:21:18,169
offers and hiring really is an important

520
00:21:18,169 --> 00:21:20,029
place to start but it's only one small

521
00:21:20,029 --> 00:21:22,369
piece of the puzzle so once you have

522
00:21:22,369 --> 00:21:24,589
employees on staff there's a wide range

523
00:21:24,589 --> 00:21:28,249
of of ways you can provide inclusive

524
00:21:28,249 --> 00:21:29,929
support that meet the different needs

525
00:21:29,929 --> 00:21:32,049
and working styles of different people

526
00:21:32,049 --> 00:21:34,940
might have and when people on your team

527
00:21:34,940 --> 00:21:36,769
get into the habit of valuing each other

528
00:21:36,769 --> 00:21:38,929
as unique individuals that internal

529
00:21:38,929 --> 00:21:40,549
culture can actually translate into

530
00:21:40,549 --> 00:21:42,379
building more equitable less biased

531
00:21:42,379 --> 00:21:45,979
products for your users so we like to

532
00:21:45,979 --> 00:21:48,080
say that policies should come from

533
00:21:48,080 --> 00:21:49,789
overarching principles and one of the

534
00:21:49,789 --> 00:21:51,499
most important is aiming for inclusion

535
00:21:51,499 --> 00:21:54,259
and equity not just diversity we haven't

536
00:21:54,259 --> 00:21:55,940
really defined those terms yet so here's

537
00:21:55,940 --> 00:21:57,649
how I think about it I think if

538
00:21:57,649 --> 00:22:00,379
diversity is about being about headcount

539
00:22:00,379 --> 00:22:02,869
and representation as an example do you

540
00:22:02,869 --> 00:22:04,460
do user research with people with

541
00:22:04,460 --> 00:22:07,369
disabilities equity is about resources

542
00:22:07,369 --> 00:22:09,529
and opportunity do you compensate people

543
00:22:09,529 --> 00:22:11,269
with disabilities at a fair and just

544
00:22:11,269 --> 00:22:12,799
level considering their unique

545
00:22:12,799 --> 00:22:14,839
perspective their investment and giving

546
00:22:14,839 --> 00:22:16,700
you that feedback and the extra value

547
00:22:16,700 --> 00:22:18,229
that they're adding to your product an

548
00:22:18,229 --> 00:22:20,479
inclusion is the hardest to measure it's

549
00:22:20,479 --> 00:22:22,279
about how marginalized people feel about

550
00:22:22,279 --> 00:22:24,799
your culture do people with disabilities

551
00:22:24,799 --> 00:22:26,899
come away feeling like valued partners

552
00:22:26,899 --> 00:22:29,119
do you create an environment where they

553
00:22:29,119 --> 00:22:30,919
can provide honest feedback and you

554
00:22:30,919 --> 00:22:34,539
listen and act on their feedback so

555
00:22:34,539 --> 00:22:37,609
let's give you some examples to improve

556
00:22:37,609 --> 00:22:39,279
the inclusion and equity within your

557
00:22:39,279 --> 00:22:41,330
organization you can work at many levels

558
00:22:41,330 --> 00:22:43,549
so you could write more inclusive job

559
00:22:43,549 --> 00:22:46,759
descriptions a cool - cool tool here is

560
00:22:46,759 --> 00:22:48,289
text EO which I think one of our

561
00:22:48,289 --> 00:22:49,759
previous speakers actually works at

562
00:22:49,759 --> 00:22:52,009
right now which points out masculine and

563
00:22:52,009 --> 00:22:53,479
feminine word ease and helps you shift

564
00:22:53,479 --> 00:22:55,429
the balance in your job description so

565
00:22:55,429 --> 00:22:57,440
that everyone feels comfortable applying

566
00:22:57,440 --> 00:22:59,809
you could also expand benefits beyond

567
00:22:59,809 --> 00:23:02,299
just young single employees one example

568
00:23:02,299 --> 00:23:04,249
is allowing flexible hours for a child

569
00:23:04,249 --> 00:23:07,190
or elder care or you can make make

570
00:23:07,190 --> 00:23:09,489
remote work an option for everyone

571
00:23:09,489 --> 00:23:11,629
you should also account for power

572
00:23:11,629 --> 00:23:12,559
dynamics

573
00:23:12,559 --> 00:23:14,870
during conflict resolution this can

574
00:23:14,870 --> 00:23:16,429
relieve some pressure from marginalized

575
00:23:16,429 --> 00:23:18,409
people who may feel trapped or powerless

576
00:23:18,409 --> 00:23:21,409
if there's a dispute so focusing on

577
00:23:21,409 --> 00:23:23,539
diversity without also working on

578
00:23:23,539 --> 00:23:25,190
inclusion and equity can create a

579
00:23:25,190 --> 00:23:27,110
situation where you get diverse people

580
00:23:27,110 --> 00:23:29,480
in the door take your metrics up but

581
00:23:29,480 --> 00:23:30,679
they encounter high levels of

582
00:23:30,679 --> 00:23:32,269
unconscious bias on their team and

583
00:23:32,269 --> 00:23:34,580
they'll be quicker to leave if you make

584
00:23:34,580 --> 00:23:36,440
inclusion equity and bias reduction

585
00:23:36,440 --> 00:23:38,840
priorities to marginalized people may be

586
00:23:38,840 --> 00:23:40,429
more likely to stay on your team and

587
00:23:40,429 --> 00:23:43,370
recruit others to join and with that

588
00:23:43,370 --> 00:23:46,429
let's wrap up so yes unconscious bias is

589
00:23:46,429 --> 00:23:48,799
a real problem and yes ways to reduce

590
00:23:48,799 --> 00:23:51,200
unconscious bias are real - and no

591
00:23:51,200 --> 00:23:52,820
matter what role we play in different

592
00:23:52,820 --> 00:23:55,190
situations or different organizations we

593
00:23:55,190 --> 00:23:57,049
each have options for what to say in the

594
00:23:57,049 --> 00:23:59,269
moment when we see or experience bias

595
00:23:59,269 --> 00:24:01,309
and what policies we can advocate for

596
00:24:01,309 --> 00:24:03,350
that can guide everyone towards less

597
00:24:03,350 --> 00:24:06,080
bias and even though most of us here

598
00:24:06,080 --> 00:24:08,450
have unconscious bias those biases don't

599
00:24:08,450 --> 00:24:11,119
own us with dedication we can practice

600
00:24:11,119 --> 00:24:13,639
breaking that bias habit and becoming

601
00:24:13,639 --> 00:24:15,559
closer the people around us and closer

602
00:24:15,559 --> 00:24:18,440
to the people that we want to be so if

603
00:24:18,440 --> 00:24:20,240
you like more info please come talk to

604
00:24:20,240 --> 00:24:22,429
me or contact us online or go to our

605
00:24:22,429 --> 00:24:25,970
website you BP is always super excited

606
00:24:25,970 --> 00:24:27,049
to speak to new people and new

607
00:24:27,049 --> 00:24:29,029
organizations about what they can do to

608
00:24:29,029 --> 00:24:31,009
improve their culture and decrease the

609
00:24:31,009 --> 00:24:33,320
affective bias in their workplaces we

610
00:24:33,320 --> 00:24:35,299
are a volunteer driven nonprofit so if

611
00:24:35,299 --> 00:24:37,249
you'd like to get involved you can visit

612
00:24:37,249 --> 00:24:39,590
our website or chat with me again and

613
00:24:39,590 --> 00:24:41,179
you can also just jump right in by

614
00:24:41,179 --> 00:24:43,009
joining our weekly virtual meetings

615
00:24:43,009 --> 00:24:45,619
using the join ubp link so thank you so

616
00:24:45,619 --> 00:24:47,480
much and let's work together to weed out

617
00:24:47,480 --> 00:24:49,070
unconscious bias in all of our

618
00:24:49,070 --> 00:24:53,760
workplaces

619
00:24:53,770 --> 00:24:55,050
you

