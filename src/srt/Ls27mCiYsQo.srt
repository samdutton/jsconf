1
00:00:01,640 --> 00:00:21,829
Hello, everyone, thanks for the introduction.

2
00:00:21,829 --> 00:00:28,480
I will be talking about Parallel JavaScript,
before I tell you more about it, why Parallel

3
00:00:28,480 --> 00:00:29,480
JavaScript?

4
00:00:29,480 --> 00:00:35,370
Well, simple the hardware you have is getting
more and more parallel, you have more cores,

5
00:00:35,370 --> 00:00:43,140
you is wider vector units, your GPUs are increasing
in performance since 2006 there's been a 75

6
00:00:43,140 --> 00:00:53,240
X increase of par are legal use\h‑‑ at
the same time, the kinds of applications would

7
00:00:53,240 --> 00:00:55,680
you write with JavaScript has changed.

8
00:00:55,680 --> 00:01:02,239
Now you have physics and fluid simulations
you have path tracing, real‑time game engines

9
00:01:02,239 --> 00:01:06,799
at the same time JavaScript programs have
access to device sensors, on board sensors,

10
00:01:06,799 --> 00:01:11,490
multimedia streams, so the kind of applications
are changing, many sophisticated more computing

11
00:01:11,490 --> 00:01:14,310
intensive.

12
00:01:14,310 --> 00:01:17,090
But there's no support for parallel computing.

13
00:01:17,090 --> 00:01:20,750
Why do we wanton vent a new parallel programming
model

14
00:01:20,750 --> 00:01:26,430
's so many, so many people have invented so
many, why not take it and bolt it on top of

15
00:01:26,430 --> 00:01:27,549
JavaScript, right.

16
00:01:27,549 --> 00:01:37,700
Turns out the web has specific requirements
that make this approach undesirable.

17
00:01:37,700 --> 00:01:41,430
First thing we want to preserve is ease of
use, whatever programming, whatever parallel

18
00:01:41,430 --> 00:01:45,800
programming model we use should fit in well
with JavaScript.

19
00:01:45,800 --> 00:01:52,939
So the programmerrer should haven't to two
different mental model, one for the sequential

20
00:01:52,939 --> 00:01:57,350
parts of the program and another for the parallel
part of the program, it should all just be

21
00:01:57,350 --> 00:01:58,350
JavaScript.

22
00:01:58,350 --> 00:02:03,880
We want to have a very high level API, we
don't want the programmer to worry about and

23
00:02:03,880 --> 00:02:12,410
exchange primitives, or even threads, we want
a very high level API fitting with the Ethos

24
00:02:12,410 --> 00:02:13,739
of JavaScript.

25
00:02:13,739 --> 00:02:18,550
We want to be deterministic where possible,
this is hard for parallel programming, but

26
00:02:18,550 --> 00:02:22,310
the determinism is awesome for debugging and
deployment.

27
00:02:22,310 --> 00:02:28,769
We also want to be platform indent, we want
to run on all kind of platforms, from single

28
00:02:28,769 --> 00:02:35,730
core, mobile phones to the most powerful desktop
computer, we want the leverage different kinds

29
00:02:35,730 --> 00:02:45,970
of particularism, could be multicore or GPU,
not just that, we want the program to write

30
00:02:45,970 --> 00:02:51,110
the same code for all of these different kinds
of parallelism, we don't want the programmer

31
00:02:51,110 --> 00:02:58,349
to write one kind of code and another for
GPUs, e we want that to be abstract away,

32
00:02:58,349 --> 00:03:04,860
our goal is to extract reasonable performance
from parallel hardware.

33
00:03:04,860 --> 00:03:11,769
it's a secondary goal and this should not
be completely to the developer, the browser

34
00:03:11,769 --> 00:03:17,209
Runtime should share a lot of this burden
of extracting performance, we want to make

35
00:03:17,209 --> 00:03:24,019
the expression of parallelism easy but not
exactly how you implement it .

36
00:03:24,019 --> 00:03:29,069
Finally we want to make it suitable for the
open web, we want to\h‑‑ we don't want

37
00:03:29,069 --> 00:03:35,370
to increase the attack service for JavaScript,
we don't want native execution of native code

38
00:03:35,370 --> 00:03:42,450
for example, we still want to preserve the
sand boxed execution environment of JavaScript,

39
00:03:42,450 --> 00:03:43,569
right.

40
00:03:43,569 --> 00:03:49,480
So, this is a brief overview of the Parallel
JavaScript API, it's built on 3 pillars, the

41
00:03:49,480 --> 00:03:56,060
first is collections, these collections in
JavaScript are just JavaScript arrays, and,

42
00:03:56,060 --> 00:04:01,189
typed object, which are going to be in an
ES 7.

43
00:04:01,189 --> 00:04:06,409
Then you have high level data par are legal
methods that operate on these collections,

44
00:04:06,409 --> 00:04:13,799
some of these may already be familiar, map,
reduce, filter, scatter, the PAR suffix simply

45
00:04:13,799 --> 00:04:19,970
conveys the programmers intention that he
would like this method called to operate in

46
00:04:19,970 --> 00:04:21,250
parallel.

47
00:04:21,250 --> 00:04:24,910
It may not, as we will see, but he would like
it to be.

48
00:04:24,910 --> 00:04:26,390
So very high level.

49
00:04:26,390 --> 00:04:28,450
No threads.

50
00:04:28,450 --> 00:04:34,290
The third component is elemental functions,
these are just regular JavaScript functions

51
00:04:34,290 --> 00:04:40,430
that are the things that are execute in the
par are legal across cores, GPUs and so on,

52
00:04:40,430 --> 00:04:44,820
there's one important caffuate, that have
to be side effect free, they cannot alter

53
00:04:44,820 --> 00:04:46,170
global state.

54
00:04:46,170 --> 00:04:52,780
So global state may for example be a global
variable, right, in a parallel region, you

55
00:04:52,780 --> 00:04:58,100
not, in an elemental function, you cannot
change global state, right, you can read it,

56
00:04:58,100 --> 00:04:59,910
but you cannot write to it.

57
00:04:59,910 --> 00:05:04,670
To there are also more subtle kind of side
effects in JavaScript program which is are

58
00:05:04,670 --> 00:05:11,150
engine level side effects, so, for example
just\h‑‑ two strings, though it appears

59
00:05:11,150 --> 00:05:17,950
at a surface level has engine level side effects,
but it's not up to the programmer to worry

60
00:05:17,950 --> 00:05:22,970
about those things, it's about the browser
Runtime were tory about that.

61
00:05:22,970 --> 00:05:29,690
So let's look at some code example (To worry
about that) this is a very simple example

62
00:05:29,690 --> 00:05:35,000
of how you would use the MapPar, you have
a tie any array, one, two, three, four, you

63
00:05:35,000 --> 00:05:42,660
give mapPar an elemental function, that fat
arrow function over there.

64
00:05:42,660 --> 00:05:49,160
And that increments each element in the input
array, this looks exactly like map, if you

65
00:05:49,160 --> 00:05:52,380
load this using map it would look exactly
the same.

66
00:05:52,380 --> 00:05:57,660
The different here the elemental function
is execute in the parallel for each element

67
00:05:57,660 --> 00:05:58,660
in this array.

68
00:05:58,660 --> 00:06:02,550
If you\h‑‑ of course in this case you
don't need parallelism, it's only four elements

69
00:06:02,550 --> 00:06:06,930
if you is a million elements and your elemental
function is doing something more sophisticated

70
00:06:06,930 --> 00:06:10,120
you imagine how this would be useful.

71
00:06:10,120 --> 00:06:17,000
Reduce looks similar, you give it, again,
an elemental function that takes two values

72
00:06:17,000 --> 00:06:22,870
and returns one value so you're reducing A
and B to produce B, when applied to this array

73
00:06:22,870 --> 00:06:29,210
produce ten this is exactly the semantics
that normal Java can produce as well.

74
00:06:29,210 --> 00:06:37,010
By adding Par you're marking it parallel
's an important caveat here, determinism,

75
00:06:37,010 --> 00:06:43,690
when you do reduce, determinism is guaranteed
only when the elemental function is associated,

76
00:06:43,690 --> 00:06:50,560
right, if the elemental function is not associative,
then, you'll get, you'll transparently get

77
00:06:50,560 --> 00:06:53,850
sequential execution, it will be equal in
to just reduce.

78
00:06:53,850 --> 00:07:01,250
In other words, the programmer does the best
job he can to write code that is paraism friendly,

79
00:07:01,250 --> 00:07:07,870
if not, if it's not paraelism friendly he'll
get sequential execution, without changing

80
00:07:07,870 --> 00:07:09,520
any code.

81
00:07:09,520 --> 00:07:16,340
So let's say you have an input image which
is a 3 dimensional width times height times

82
00:07:16,340 --> 00:07:22,660
four, right, four is RGB alpha, this is how
you would convert it into gray scale, you

83
00:07:22,660 --> 00:07:29,750
do a mapPar, ignore 2 over there and give
it an elemental function that averages the

84
00:07:29,750 --> 00:07:37,600
first three channels, the RGB values and returns
an array with this so this is a simple averageaing

85
00:07:37,600 --> 00:07:40,650
gray scale operation.

86
00:07:40,650 --> 00:07:47,380
The two here is the depth at which you want
the mapPar to operate, if you have a 3 dimensional

87
00:07:47,380 --> 00:07:53,850
collection, sometimes would you like to operate
on rows of an image, sometimes would you like

88
00:07:53,850 --> 00:08:00,130
the operate on fix els, sometimes would you
like to operate on individual color channels

89
00:08:00,130 --> 00:08:06,690
in an image, the depth parameter would allow
you to select what slice you want to do this

90
00:08:06,690 --> 00:08:10,530
parallel operation on, in an input collection,
okay.

91
00:08:10,530 --> 00:08:15,510
You don't have to worry about all the details,
theu just to give you an idea of what the

92
00:08:15,510 --> 00:08:22,660
APL looks like, so reversing an array, simple
exact million you create an ray for build

93
00:08:22,660 --> 00:08:32,740
par\h‑‑ in parallel it constructs an array
that just contains 0123 and scatter Par takes

94
00:08:32,740 --> 00:08:40,610
a function that maps elements in the input
array to the out put away.

95
00:08:40,610 --> 00:08:48,890
ScatterPar is taking the first element putting
it in the last index and so on.

96
00:08:48,890 --> 00:08:57,440
Filter works exactly the same way as\h‑‑
filterPar works exactly the same way as\h‑‑

97
00:08:57,440 --> 00:09:03,160
just give it an elemental function this returns
the Boolean, it determines whether it stay

98
00:09:03,160 --> 00:09:06,970
in the out put array or not.

99
00:09:06,970 --> 00:09:11,670
Typed objects are a new proposal.

100
00:09:11,670 --> 00:09:17,840
I believe there are\h‑‑ there's a preliminary
implementation in fire fox.

101
00:09:17,840 --> 00:09:24,280
The API just works on typed objects I dentically
to JavaScript objects, the same gray scale

102
00:09:24,280 --> 00:09:30,120
operation I showed you earlier, it's very
similar, except it's returning the JavaScript

103
00:09:30,120 --> 00:09:33,210
array, you're returning a new typed object.

104
00:09:33,210 --> 00:09:38,190
So, this is what I mean by feeding in with
the rest of JavaScript, you don't want to

105
00:09:38,190 --> 00:09:45,610
introduce new types or concepts, you want
to kind of build upon what is already there.

106
00:09:45,610 --> 00:09:51,720
Of Parallel JavaScript is an ES 7 proposal
at this point.

107
00:09:51,720 --> 00:09:57,510
We're discussing the API in the committee
and with the browser\h‑‑ there's a specification

108
00:09:57,510 --> 00:09:58,720
you can look at.

109
00:09:58,720 --> 00:10:07,240
And a preliminary implementation is in fire
fox nightly the API I just showed you is the

110
00:10:07,240 --> 00:10:14,680
result of a collaboration between my team
and the Mozilla team that is implements JPS

111
00:10:14,680 --> 00:10:17,010
in Nightly.

112
00:10:17,010 --> 00:10:24,550
So in order to get parallel execution, they
had to solve several challenges I'll briefly

113
00:10:24,550 --> 00:10:30,580
describe them, first was JIT support, so you
want for parallelism you want to go in IC

114
00:10:30,580 --> 00:10:38,840
side effect freedom, how do you make sure
they Donahue state global state T way they

115
00:10:38,840 --> 00:10:44,260
do this is theyv a static safety analysis
that inspects your JavaScript and figures

116
00:10:44,260 --> 00:10:49,540
out whether there's a violation of the side
effect freedom requirement.

117
00:10:49,540 --> 00:10:54,580
They also have right guard which is are dynamic
checks that check whether you're following

118
00:10:54,580 --> 00:10:55,930
the rule.

119
00:10:55,930 --> 00:11:02,490
They have a pretty cool work stealing scheduler
that tries to schedule work on hardware threads

120
00:11:02,490 --> 00:11:10,660
in the way that reduces imbalances on threads,
if you have hypothreading, you have many hardware

121
00:11:10,660 --> 00:11:14,770
threats, it tries to keep all of them busy.

122
00:11:14,770 --> 00:11:20,040
And there's a new garbage collection scheme
just for Parallel JavaScript that is kind

123
00:11:20,040 --> 00:11:26,430
of integrated with the spider monkey, fire
foxes generational garbage collector, and

124
00:11:26,430 --> 00:11:32,440
cool thing about this is it actually exploits
the programming model, the property of the

125
00:11:32,440 --> 00:11:36,500
programming model, which is that you can't
have side effects, and this fact makes garbage

126
00:11:36,500 --> 00:11:40,160
collection fast, it turns out, which is cool.

127
00:11:40,160 --> 00:11:45,440
Finally there's a lot of work on providing
retail reasons for why something did not execute

128
00:11:45,440 --> 00:11:46,440
in parallel.

129
00:11:46,440 --> 00:11:57,320
For example in the mapPar, if I were to insert
some Dom, which is not side effect free, then

130
00:11:57,320 --> 00:12:01,120
the program will bail out of parallel execution
and you'll just get sequential execution,

131
00:12:01,120 --> 00:12:06,350
but your console will tell you that this is
the operation at this line number in your

132
00:12:06,350 --> 00:12:11,020
program, that is the reason for you not getting
parallel speed ups, right.

133
00:12:11,020 --> 00:12:17,210
They also provide complete stack frames at
bailout points so that you can actually debug

134
00:12:17,210 --> 00:12:23,230
why things didn't flan parallel this is really
important\h‑‑ didn't run in parallel.

135
00:12:23,230 --> 00:12:31,960
I would like to take a few minutes to talk
about parallel JavaScript on GPUs, hugely

136
00:12:31,960 --> 00:12:39,050
important resource that I think we should
use more in JavaScript programs in the non‑graphical

137
00:12:39,050 --> 00:12:46,650
portions of JavaScript programs, I mean GPGPU,
general programming context.

138
00:12:46,650 --> 00:12:53,930
But, compiling Parallel JavaScript to GPUs
is not easy, it's challenging,I I'llout line

139
00:12:53,930 --> 00:13:05,560
a few reasons why, GPUs have a separate address
space than CPU,\h‑‑ how do you make that

140
00:13:05,560 --> 00:13:09,560
available to a JavaScript program running
on the GPU?

141
00:13:09,560 --> 00:13:16,790
You have copy it, which would be hugely expensive
otherwise you would have to map it somehow.

142
00:13:16,790 --> 00:13:20,060
So this is a non‑trivial problem.

143
00:13:20,060 --> 00:13:28,000
Even though in some new chips, physical memory
is shared, virtual memory is not shared, this

144
00:13:28,000 --> 00:13:36,730
makes the problem of mapping non‑contiguous
parts of the heap on your device very challenging.

145
00:13:36,730 --> 00:13:45,100
Then there are a number of paths to the actual
GPU hardware, there's directX,there's openCL

146
00:13:45,100 --> 00:13:47,839
Kernels and there are tons more.

147
00:13:47,839 --> 00:13:54,200
Each of these have slightly different semantics,
each of these is\h‑‑ allows a different

148
00:13:54,200 --> 00:14:00,080
subset of JavaScript, so I'm talking about
compiling JavaScript to one of these, right.

149
00:14:00,080 --> 00:14:04,430
So, as you can imagine, you can compile a
different subset of JavaScript, so you have

150
00:14:04,430 --> 00:14:09,580
to reconcile these together in your browser
Runtime.

151
00:14:09,580 --> 00:14:16,790
So, some things that would be possible to
express in, let's say openCL would not be

152
00:14:16,790 --> 00:14:22,490
possible to express in OpenGL, right, just
because of the programming model.

153
00:14:22,490 --> 00:14:26,850
You have to keep that in mind when you're
compiling.

154
00:14:26,850 --> 00:14:32,900
Then there's no dynamic location on northwest
GPUs there, is no notion overheap, as such,

155
00:14:32,900 --> 00:14:39,589
it's just scratch pad memory, the star there
indicates that on kind of more recent GPUs

156
00:14:39,589 --> 00:14:48,930
there is actually, I think Kuda, 2.0 has Mallac,
it's not wide limply meanted on all GPUs,

157
00:14:48,930 --> 00:14:56,180
there's no general notion of a stack either,
rite on a GPU, so most interesting JavaScript

158
00:14:56,180 --> 00:15:01,480
programs would like to do heap allocations,
I presume, this would be hard to support,

159
00:15:01,480 --> 00:15:08,620
you would have to build a dynamic memory managerrer,
and a collector that runs completely on the

160
00:15:08,620 --> 00:15:09,620
GPU.

161
00:15:09,620 --> 00:15:15,770
There are also no function pointers in most
GPU programming models, this makes things

162
00:15:15,770 --> 00:15:20,850
like dynamic dispatch difficult to impresent.

163
00:15:20,850 --> 00:15:24,030
And polymorphism is also difficult oimpresent.

164
00:15:24,030 --> 00:15:29,870
And getting final performance is not easy,
the developer has to keep track of different

165
00:15:29,870 --> 00:15:41,670
families of GPUs, this is a fourth generation
GPU with Open CL, and this is a Coda, so it's

166
00:15:41,670 --> 00:15:46,649
very, it's an intractable problem and unreasonable
to expect the programmer to keep track of

167
00:15:46,649 --> 00:15:47,649
all of these things.

168
00:15:47,649 --> 00:15:53,290
So our approach is to basically take this
high level specification in PGS and let the

169
00:15:53,290 --> 00:15:58,270
browser Runtime\h‑‑ the browser Runtime
has perfect knowledge of what the platform

170
00:15:58,270 --> 00:16:05,080
is, it can query GP capabilities, it can do
all of that, the idea is to threat browser

171
00:16:05,080 --> 00:16:08,670
run time do it, transparently.

172
00:16:08,670 --> 00:16:16,020
We've implemented a back end that takes your
JavaScript and compiles it to run on GPU,

173
00:16:16,020 --> 00:16:19,339
we have impresented it within fire fox.

174
00:16:19,339 --> 00:16:24,060
So, this is an overview of what it looks like,
the blue parts are what would happen with

175
00:16:24,060 --> 00:16:29,830
normal parallel Java execution, CPU, would
you run for a little while with an at the

176
00:16:29,830 --> 00:16:36,260
present timer, baseline JIT, once your code
has been hit a few times, it's hot, you know

177
00:16:36,260 --> 00:16:41,880
all the type information, then you would go
to the Ion monkey JIT that would spend more

178
00:16:41,880 --> 00:16:48,940
time optimizing, it's that the point doing
parallel safety analysis, checking to see

179
00:16:48,940 --> 00:16:52,230
if it's safe to run in parallel or not.

180
00:16:52,230 --> 00:16:56,640
Then lower it a few times then you get just
native code,right.

181
00:16:56,640 --> 00:17:04,850
In the GPU case, we take MIR, which is the
mid‑level intermediary rep evennation in

182
00:17:04,850 --> 00:17:12,459
JavaScript in Ion monkey, we do GPU specific
safety analysis, this checke checks whether

183
00:17:12,459 --> 00:17:19,880
all the byte codes can be supported on the
GPU, for example, if you're compiling to GSLL,

184
00:17:19,880 --> 00:17:24,270
then maybe some particular byte codes are
not supported

185
00:17:24,270 --> 00:17:28,280
are no equal lens in GLL for some Java code.

186
00:17:28,280 --> 00:17:35,210
Then we do some simple type inference, this
is basically taking the JIT, ION monkey git

187
00:17:35,210 --> 00:17:41,670
information and translating it to type that
open CL understands, and we do some optimizations

188
00:17:41,670 --> 00:17:48,660
for exploiting local memory, which is very
fast on GPs, and then finally we generate

189
00:17:48,660 --> 00:18:00,060
open CL, and then Open CL is built into a
CPU binary and we reuse that binary, as long

190
00:18:00,060 --> 00:18:05,500
as type information doesn't change, the types
flowing your you JavaScript program doesn't

191
00:18:05,500 --> 00:18:10,750
change, the Kernel is valid and we keep reusing
it.

192
00:18:10,750 --> 00:18:13,679
So what does all of this buy us, right?

193
00:18:13,679 --> 00:18:19,150
Let's look at the energy and\h‑‑ let's
look at performance first, verse parallel

194
00:18:19,150 --> 00:18:27,660
execution on a CPU, remember, the baseline
here is Parallel JavaScript execution on a

195
00:18:27,660 --> 00:18:32,830
CPU, which is already depending on mow cores
you is, which is already faster than regular

196
00:18:32,830 --> 00:18:35,350
sequential JavaScript, right.

197
00:18:35,350 --> 00:18:46,280
So the blue access, the blue white access
performance relative to CPU, so these are\h‑‑

198
00:18:46,280 --> 00:18:48,650
these are some simple benchmarks that I picked.

199
00:18:48,650 --> 00:18:54,870
We have more, I can show them to you later,
so this MM is dense matrix, it does 8 times

200
00:18:54,870 --> 00:19:02,980
better than on the GPU than on parallel CPU,
this is written in JavaScript using PGS, and

201
00:19:02,980 --> 00:19:16,990
2D‑Conv which you use for image sharpening,
finding contours or optical flow, it's a fairly

202
00:19:16,990 --> 00:19:25,120
important ‑‑ it does 2.8 times faster
on GPU than parallel CP, you all know the

203
00:19:25,120 --> 00:19:33,250
Mandel set, it does 6.2 times better in terms
of performance than CPU.

204
00:19:33,250 --> 00:19:34,750
What about energy?

205
00:19:34,750 --> 00:19:44,270
So for dense matrix multiply, GPU execution
the 7.7 times lower in energy than CPU execution,

206
00:19:44,270 --> 00:19:51,420
than parallel CPU execution, it's 3 times
and Mandel is 7.7 times lower, not only are

207
00:19:51,420 --> 00:19:59,300
you getting huge increases from par are legal
execution on GPU, over what is already fast

208
00:19:59,300 --> 00:20:10,930
parallel CP um, you're also getting huge decrees
in total energy consumption, this is big.

209
00:20:10,930 --> 00:20:20,110
The matrix multiplier programs runs\h‑‑
runs 22 times faster.

210
00:20:20,110 --> 00:20:25,580
So this 8 X is over that.

211
00:20:25,580 --> 00:20:36,110
Ever 6\h‑‑ this would be 2.8 times faster
than that.

212
00:20:36,110 --> 00:20:37,110
Okay.

213
00:20:37,110 --> 00:20:59,559
Let me quickly show you a quick demo\h‑‑
this is a simple program of so there are a

214
00:20:59,559 --> 00:21:06,549
burner bunch of bugs the goal of the game
is to move them using my hand to the frog,

215
00:21:06,549 --> 00:21:09,620
feed them to the frog or take them to their
house, right.

216
00:21:09,620 --> 00:21:16,130
So, I probably should be standing closer,
so, it's using\h‑‑ it's doing optical

217
00:21:16,130 --> 00:21:21,750
flow on the image stream captured by the camera
to basically figure out which way I'm moving

218
00:21:21,750 --> 00:21:30,020
so you can see tall bugs settling on my face
(Laughing) so it basically detects changes

219
00:21:30,020 --> 00:21:31,990
in energy in the image.

220
00:21:31,990 --> 00:21:36,280
So if I were to just run the sequential version
of this today.

221
00:21:36,280 --> 00:21:39,680
It would run like this.

222
00:21:39,680 --> 00:21:43,910
Right this is pretty slow.

223
00:21:43,910 --> 00:21:52,380
Let's go back to the parallel version.

224
00:21:52,380 --> 00:22:05,240
(Applause) so, let me go back to my presentation,
I can show you this in more detail, if you're

225
00:22:05,240 --> 00:22:06,260
interested.

226
00:22:06,260 --> 00:22:11,770
Okay, so the implementation is in fire fox
Nightly there's a ECMA script proposal, we

227
00:22:11,770 --> 00:22:17,520
would like the feedback, this is the time
to influence when the API looks like and how

228
00:22:17,520 --> 00:22:24,540
it's specified, if you is comments write to
me or participate in the discussion on ECMAS

229
00:22:24,540 --> 00:22:26,020
discuss.

230
00:22:26,020 --> 00:22:27,370
Try it out and let us know what you think.

